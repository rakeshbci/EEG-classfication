{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#====================================================================\n",
    "#-- Project      : EE239 Final Project\n",
    "#-- File Name    : CNN_For_EEG.ipynb\n",
    "#-- Description  : EEG signal processing by CNN\n",
    "#-- Date         : 03/13/2018\n",
    "#-- Author       : Xingyi Chen\n",
    "#===================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subject 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(288, 25, 1000)\n",
      "(288,)\n",
      "(288, 22, 1000)\n",
      "(288, 1000, 22)\n",
      "(288, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np \n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "A01T = h5py.File('A01T_slice.mat', 'r') \n",
    "X = np.copy(A01T['image']) \n",
    "y = np.copy(A01T['type']) \n",
    "y = y[0,0:X.shape[0]:1] \n",
    "y = np.asarray(y, dtype=np.int32)\n",
    "\n",
    "# 288 trials per session\n",
    "# 25 channels (the Ô¨Årst 22 are EEG and the last 3 are EOG signals)\n",
    "# The data is sampled at 250Hz, and so 1000 corresponds to 4s of data for each trial.\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# get rid of the EOG signal\n",
    "X = X[:,:22,:]\n",
    "print(X.shape)\n",
    "\n",
    "# get rif of NaN\n",
    "y = np.asarray(y, dtype=np.float32)-769\n",
    "y = y[:,None]\n",
    "X = X.transpose(0,2,1)\n",
    "where_are_NaNs = np.isnan(X)\n",
    "X[where_are_NaNs] = 0\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(X, y, num_test = 50, num_train = 238, num_val = 50, \n",
    "                   subtract_mean=True):\n",
    "   \n",
    "    mask = list(range(num_train))\n",
    "    X_train = X[mask]\n",
    "    y_train = y[mask]\n",
    "    mask = list(range(num_train,num_train+num_test))\n",
    "    X_test = X[mask]\n",
    "    y_test = y[mask]\n",
    "    \n",
    "    #mask = list(range(num_train-num_val))\n",
    "    #X_train = X_train0[mask]\n",
    "    #y_train = y_train0[mask]\n",
    "    #mask = list(range(num_train-num_val,num_train))\n",
    "    #X_val = X_train0[mask]\n",
    "    #y_val = y_train0[mask]\n",
    "\n",
    "#     if subtract_mean:\n",
    "#         mean_image = np.mean(X_train, axis=0)\n",
    "#         X_train -= mean_image\n",
    "#         X_val -= mean_image\n",
    "#         X_test -= mean_image\n",
    "        \n",
    "    return {\n",
    "      'X_train': X_train, 'y_train': y_train,\n",
    "      'X_test': X_test, 'y_test': y_test,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (238, 1000, 22) \n",
      "y_train: (238, 1) \n",
      "X_test: (50, 1000, 22) \n",
      "y_test: (50, 1) \n"
     ]
    }
   ],
   "source": [
    "# get train, validation, test data\n",
    "data = get_train_test(X,y)\n",
    "for k in data.keys():\n",
    "    print('{}: {} '.format(k, data[k].shape))\n",
    "# X[n_trials, n_steps, n_inputs]\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(238, 4)\n",
      "(50, 4)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, Conv2D, Conv3D, MaxPooling1D,MaxPooling2D, AveragePooling2D, Reshape, BatchNormalization\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import regularizers\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# There are 4 classes  \n",
    "num_classes = 4  \n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "#y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(y_train.shape)\n",
    "#print(y_val.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "class TimeHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.times = [0]\n",
    "\n",
    "    def on_epoch_begin(self, batch, logs={}):\n",
    "        self.epoch_time_start = time.time()\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.times.append(self.times[-1]+time.time() - self.epoch_time_start)\n",
    "        \n",
    "class AccuracyHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.testaccuracy = []\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.testaccuracy.append(logs.get('val_acc'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shallow ConvNet from the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(238, 1000, 22, 1)\n",
      "(50, 1000, 22, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train1 = np.expand_dims(X_train,-1)\n",
    "#X_val1 = np.expand_dims(X_val,-1)\n",
    "X_test1 = np.expand_dims(X_test,-1)\n",
    "\n",
    "print(X_train1.shape)\n",
    "#print(X_train1.shape[1:])\n",
    "print(X_test1.shape)\n",
    "#print(X_val1.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_15 (Conv2D)           (None, 976, 22, 40)       1040      \n",
      "_________________________________________________________________\n",
      "reshape_15 (Reshape)         (None, 22, 40, 976)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 1, 40, 976)        20957648  \n",
      "_________________________________________________________________\n",
      "reshape_16 (Reshape)         (None, 976, 40, 1)        0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d_7 (Average (None, 61, 40, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten_15 (Flatten)         (None, 2440)              0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 4)                 9764      \n",
      "=================================================================\n",
      "Total params: 20,968,452\n",
      "Trainable params: 20,968,452\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 238 samples, validate on 50 samples\n",
      "Epoch 1/20\n",
      "238/238 [==============================] - 42s 178ms/step - loss: 5.6932 - acc: 0.2773 - val_loss: 10.0729 - val_acc: 0.2400\n",
      "Epoch 2/20\n",
      "238/238 [==============================] - 40s 169ms/step - loss: 9.9069 - acc: 0.2521 - val_loss: 9.6595 - val_acc: 0.2400\n",
      "Epoch 3/20\n",
      "238/238 [==============================] - 46s 192ms/step - loss: 9.5682 - acc: 0.2353 - val_loss: 8.4595 - val_acc: 0.2600\n",
      "Epoch 4/20\n",
      "238/238 [==============================] - 46s 192ms/step - loss: 9.4615 - acc: 0.2185 - val_loss: 8.4365 - val_acc: 0.2400\n",
      "Epoch 5/20\n",
      "238/238 [==============================] - 48s 201ms/step - loss: 8.8069 - acc: 0.2857 - val_loss: 8.4225 - val_acc: 0.3000\n",
      "Epoch 6/20\n",
      "238/238 [==============================] - 47s 197ms/step - loss: 8.6263 - acc: 0.2437 - val_loss: 8.4935 - val_acc: 0.2600\n",
      "Epoch 7/20\n",
      "238/238 [==============================] - 49s 208ms/step - loss: 8.4298 - acc: 0.2773 - val_loss: 8.5852 - val_acc: 0.2400\n",
      "Epoch 8/20\n",
      "238/238 [==============================] - 46s 192ms/step - loss: 8.4037 - acc: 0.3025 - val_loss: 8.4425 - val_acc: 0.2600\n",
      "Epoch 9/20\n",
      "238/238 [==============================] - 51s 215ms/step - loss: 8.3906 - acc: 0.2857 - val_loss: 8.4159 - val_acc: 0.2200\n",
      "Epoch 10/20\n",
      "238/238 [==============================] - 48s 202ms/step - loss: 8.3543 - acc: 0.3151 - val_loss: 8.4520 - val_acc: 0.2400\n",
      "Epoch 11/20\n",
      "238/238 [==============================] - 45s 189ms/step - loss: 8.3047 - acc: 0.4076 - val_loss: 8.3948 - val_acc: 0.3000\n",
      "Epoch 12/20\n",
      "238/238 [==============================] - 50s 209ms/step - loss: 8.2975 - acc: 0.3782 - val_loss: 8.3864 - val_acc: 0.2600\n",
      "Epoch 13/20\n",
      "238/238 [==============================] - 44s 186ms/step - loss: 8.2536 - acc: 0.4832 - val_loss: 8.4195 - val_acc: 0.2800\n",
      "Epoch 14/20\n",
      "238/238 [==============================] - 48s 200ms/step - loss: 8.2240 - acc: 0.4664 - val_loss: 8.3716 - val_acc: 0.3000\n",
      "Epoch 15/20\n",
      "238/238 [==============================] - 47s 198ms/step - loss: 8.1817 - acc: 0.4916 - val_loss: 8.3692 - val_acc: 0.3000\n",
      "Epoch 16/20\n",
      "238/238 [==============================] - 45s 187ms/step - loss: 8.1540 - acc: 0.4958 - val_loss: 8.3771 - val_acc: 0.3000\n",
      "Epoch 17/20\n",
      "238/238 [==============================] - 46s 195ms/step - loss: 8.1263 - acc: 0.5000 - val_loss: 8.3605 - val_acc: 0.3400\n",
      "Epoch 18/20\n",
      "238/238 [==============================] - 46s 195ms/step - loss: 8.1031 - acc: 0.5000 - val_loss: 8.3647 - val_acc: 0.3400\n",
      "Epoch 19/20\n",
      "238/238 [==============================] - 47s 199ms/step - loss: 8.0897 - acc: 0.5000 - val_loss: 8.3682 - val_acc: 0.3200\n",
      "Epoch 20/20\n",
      "238/238 [==============================] - 46s 193ms/step - loss: 8.0790 - acc: 0.5000 - val_loss: 8.3706 - val_acc: 0.3200\n",
      "50/50 [==============================] - 2s 36ms/step\n",
      "Test loss: 8.37064064025879\n",
      "Test accuracy: 0.3200000047683716\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 80\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 1st cnn block:\n",
    "\n",
    "# 1000*22*1\n",
    "model.add(Conv2D(filters=40, kernel_size=(25,1), strides=(1,1),activation='relu', padding='valid',kernel_initializer='uniform',\n",
    "                 input_shape=(1000,22,1)))\n",
    "# 991*22*25\n",
    "model.add(Reshape((22,40,976), input_shape=(976,22,40)))\n",
    "# 22*25*991\n",
    "model.add(Conv2D(filters=976, kernel_size=(22,1), strides=1,activation='relu',padding='valid',kernel_initializer='uniform'))\n",
    "# 1*25*991\n",
    "model.add(Reshape((976,40,1), input_shape=(1,40,976)))\n",
    "# 991*25*1\n",
    "model.add(AveragePooling2D(pool_size=(75,1),strides=(15,1)))\n",
    "\n",
    "model.add(Flatten()) \n",
    "model.add(Dense(num_classes,activation='softmax'))  \n",
    "\n",
    "opt = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.0005, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])  \n",
    "model.summary()  \n",
    "\n",
    "model.fit(X_train1, y_train, batch_size=batch_size,validation_data=(X_test1,y_test),epochs=epochs,verbose=1)\n",
    "scores = model.evaluate(X_test1, y_test, verbose=1)\n",
    "\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep ConvNet from the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188, 1000, 22, 1)\n",
      "(50, 1000, 22, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train1 = np.expand_dims(X_train,-1)\n",
    "X_val1 = np.expand_dims(X_val,-1)\n",
    "X_test1 = np.expand_dims(X_test,-1)\n",
    "\n",
    "print(X_train1.shape)\n",
    "#print(X_train1.shape[1:])\n",
    "print(X_val1.shape)\n",
    "#print(X_val1.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_112 (Conv2D)          (None, 991, 22, 25)       275       \n",
      "_________________________________________________________________\n",
      "reshape_98 (Reshape)         (None, 22, 25, 991)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_113 (Conv2D)          (None, 1, 25, 991)        21606773  \n",
      "_________________________________________________________________\n",
      "reshape_99 (Reshape)         (None, 991, 25, 1)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_63 (MaxPooling (None, 330, 25, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_114 (Conv2D)          (None, 321, 1, 50)        12550     \n",
      "_________________________________________________________________\n",
      "reshape_100 (Reshape)        (None, 321, 50, 1)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_64 (MaxPooling (None, 107, 50, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_115 (Conv2D)          (None, 98, 1, 100)        50100     \n",
      "_________________________________________________________________\n",
      "reshape_101 (Reshape)        (None, 98, 100, 1)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_65 (MaxPooling (None, 32, 100, 1)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_116 (Conv2D)          (None, 23, 1, 200)        200200    \n",
      "_________________________________________________________________\n",
      "reshape_102 (Reshape)        (None, 23, 200, 1)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_66 (MaxPooling (None, 7, 200, 1)         0         \n",
      "_________________________________________________________________\n",
      "flatten_24 (Flatten)         (None, 1400)              0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 4)                 5604      \n",
      "=================================================================\n",
      "Total params: 21,875,502\n",
      "Trainable params: 21,875,502\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 238 samples, validate on 50 samples\n",
      "Epoch 1/40\n",
      "238/238 [==============================] - 35s 149ms/step - loss: 175.9552 - acc: 0.2353 - val_loss: 158.0782 - val_acc: 0.2200\n",
      "Epoch 2/40\n",
      "238/238 [==============================] - 34s 141ms/step - loss: 150.0461 - acc: 0.3697 - val_loss: 134.1815 - val_acc: 0.2200\n",
      "Epoch 3/40\n",
      "238/238 [==============================] - 33s 140ms/step - loss: 126.6920 - acc: 0.4874 - val_loss: 112.2240 - val_acc: 0.4200\n",
      "Epoch 4/40\n",
      "238/238 [==============================] - 33s 139ms/step - loss: 105.4744 - acc: 0.6555 - val_loss: 92.5393 - val_acc: 0.3200\n",
      "Epoch 5/40\n",
      "238/238 [==============================] - 33s 138ms/step - loss: 86.5802 - acc: 0.7101 - val_loss: 75.3747 - val_acc: 0.3000\n",
      "Epoch 6/40\n",
      "238/238 [==============================] - 33s 137ms/step - loss: 70.2053 - acc: 0.9538 - val_loss: 60.7301 - val_acc: 0.3800\n",
      "Epoch 7/40\n",
      "238/238 [==============================] - 33s 139ms/step - loss: 56.2868 - acc: 0.9664 - val_loss: 48.4669 - val_acc: 0.3800\n",
      "Epoch 8/40\n",
      "238/238 [==============================] - 33s 139ms/step - loss: 44.6485 - acc: 0.9832 - val_loss: 38.3756 - val_acc: 0.3400\n",
      "Epoch 9/40\n",
      "238/238 [==============================] - 34s 141ms/step - loss: 35.0334 - acc: 0.9790 - val_loss: 30.2213 - val_acc: 0.3400\n",
      "Epoch 10/40\n",
      "238/238 [==============================] - 34s 143ms/step - loss: 27.1681 - acc: 0.9706 - val_loss: 23.7358 - val_acc: 0.2600\n",
      "Epoch 11/40\n",
      "238/238 [==============================] - 33s 139ms/step - loss: 20.9263 - acc: 0.9664 - val_loss: 19.2763 - val_acc: 0.3200\n",
      "Epoch 12/40\n",
      "238/238 [==============================] - 34s 144ms/step - loss: 16.2009 - acc: 0.9160 - val_loss: 15.5351 - val_acc: 0.2600\n",
      "Epoch 13/40\n",
      "238/238 [==============================] - 33s 140ms/step - loss: 12.6212 - acc: 0.9370 - val_loss: 13.8370 - val_acc: 0.3200\n",
      "Epoch 14/40\n",
      "238/238 [==============================] - 33s 139ms/step - loss: 11.1866 - acc: 0.5210 - val_loss: 10.2364 - val_acc: 0.3200\n",
      "Epoch 15/40\n",
      "238/238 [==============================] - 33s 138ms/step - loss: 9.1683 - acc: 0.5168 - val_loss: 8.5651 - val_acc: 0.2600\n",
      "Epoch 16/40\n",
      "238/238 [==============================] - 33s 138ms/step - loss: 8.2276 - acc: 0.2899 - val_loss: 7.5000 - val_acc: 0.2600\n",
      "Epoch 17/40\n",
      "238/238 [==============================] - 33s 138ms/step - loss: 6.9904 - acc: 0.4958 - val_loss: 6.6343 - val_acc: 0.2600\n",
      "Epoch 18/40\n",
      "238/238 [==============================] - 33s 138ms/step - loss: 6.0110 - acc: 0.6765 - val_loss: 6.1181 - val_acc: 0.2800\n",
      "Epoch 19/40\n",
      "238/238 [==============================] - 33s 140ms/step - loss: 5.2228 - acc: 0.6849 - val_loss: 5.7460 - val_acc: 0.2800\n",
      "Epoch 20/40\n",
      "238/238 [==============================] - 33s 138ms/step - loss: 4.5142 - acc: 0.7731 - val_loss: 5.3950 - val_acc: 0.3200\n",
      "Epoch 21/40\n",
      "238/238 [==============================] - 33s 137ms/step - loss: 3.8346 - acc: 0.8571 - val_loss: 5.1012 - val_acc: 0.3200\n",
      "Epoch 22/40\n",
      "238/238 [==============================] - 34s 141ms/step - loss: 3.3425 - acc: 0.9118 - val_loss: 4.9436 - val_acc: 0.4000\n",
      "Epoch 23/40\n",
      "238/238 [==============================] - 33s 140ms/step - loss: 2.8711 - acc: 0.9706 - val_loss: 4.3281 - val_acc: 0.3600\n",
      "Epoch 24/40\n",
      "238/238 [==============================] - 33s 137ms/step - loss: 2.5281 - acc: 0.9958 - val_loss: 4.2469 - val_acc: 0.2800\n",
      "Epoch 25/40\n",
      "238/238 [==============================] - 33s 139ms/step - loss: 2.2355 - acc: 0.9958 - val_loss: 4.2496 - val_acc: 0.3400\n",
      "Epoch 26/40\n",
      "238/238 [==============================] - 34s 142ms/step - loss: 1.9805 - acc: 1.0000 - val_loss: 4.0030 - val_acc: 0.3800\n",
      "Epoch 27/40\n",
      "238/238 [==============================] - 34s 143ms/step - loss: 1.7743 - acc: 1.0000 - val_loss: 3.9611 - val_acc: 0.2400\n",
      "Epoch 28/40\n",
      "238/238 [==============================] - 33s 139ms/step - loss: 1.5883 - acc: 1.0000 - val_loss: 3.8002 - val_acc: 0.3400\n",
      "Epoch 29/40\n",
      "238/238 [==============================] - 33s 139ms/step - loss: 1.4304 - acc: 1.0000 - val_loss: 3.5191 - val_acc: 0.3200\n",
      "Epoch 30/40\n",
      "238/238 [==============================] - 33s 139ms/step - loss: 1.2896 - acc: 1.0000 - val_loss: 3.3812 - val_acc: 0.3000\n",
      "Epoch 31/40\n",
      "238/238 [==============================] - 33s 139ms/step - loss: 1.1769 - acc: 1.0000 - val_loss: 3.1247 - val_acc: 0.3600\n",
      "Epoch 32/40\n",
      "238/238 [==============================] - 33s 139ms/step - loss: 1.0817 - acc: 1.0000 - val_loss: 3.0224 - val_acc: 0.3600\n",
      "Epoch 33/40\n",
      "238/238 [==============================] - 33s 138ms/step - loss: 1.0035 - acc: 1.0000 - val_loss: 2.8217 - val_acc: 0.3600\n",
      "Epoch 34/40\n",
      "238/238 [==============================] - 34s 143ms/step - loss: 0.9408 - acc: 1.0000 - val_loss: 2.8166 - val_acc: 0.3600\n",
      "Epoch 35/40\n",
      "238/238 [==============================] - 34s 141ms/step - loss: 0.8912 - acc: 1.0000 - val_loss: 2.6993 - val_acc: 0.2800\n",
      "Epoch 36/40\n",
      "238/238 [==============================] - 34s 143ms/step - loss: 0.8499 - acc: 1.0000 - val_loss: 2.6843 - val_acc: 0.3000\n",
      "Epoch 37/40\n",
      "238/238 [==============================] - 33s 141ms/step - loss: 0.8171 - acc: 1.0000 - val_loss: 2.7269 - val_acc: 0.3200\n",
      "Epoch 38/40\n",
      "238/238 [==============================] - 33s 138ms/step - loss: 0.7916 - acc: 1.0000 - val_loss: 2.6773 - val_acc: 0.3600\n",
      "Epoch 39/40\n",
      "238/238 [==============================] - 33s 139ms/step - loss: 0.7652 - acc: 1.0000 - val_loss: 2.6654 - val_acc: 0.3800\n",
      "Epoch 40/40\n",
      "238/238 [==============================] - 33s 138ms/step - loss: 0.7544 - acc: 1.0000 - val_loss: 3.6350 - val_acc: 0.3200\n",
      "50/50 [==============================] - 1s 22ms/step\n",
      "Test loss: 3.63500696182251\n",
      "Test accuracy: 0.32000000059604644\n"
     ]
    }
   ],
   "source": [
    "epochs = 40\n",
    "batch_size = 80\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 1st cnn block:\n",
    "\n",
    "# 1000*22*1\n",
    "model.add(Conv2D(filters=25, kernel_size=(10,1), strides=(1,1),activation='relu', padding='valid',kernel_regularizer = regularizers.l2(0.01),kernel_initializer='uniform',\n",
    "                 input_shape=(1000,22,1)))\n",
    "# 991*22*25\n",
    "model.add(Reshape((22,25,991), input_shape=(991,22,25)))\n",
    "# 22*25*991\n",
    "model.add(Conv2D(filters=991, kernel_size=(22,1), strides=1,activation='relu',padding='valid',kernel_regularizer = regularizers.l2(0.01),kernel_initializer='uniform'))\n",
    "# 1*25*991\n",
    "model.add(Reshape((991,25,1), input_shape=(1,25,991)))\n",
    "# 991*25*1\n",
    "model.add(MaxPooling2D(pool_size=(3,1),padding='valid'))\n",
    "\n",
    "# 2nd cnn block:\n",
    "# 331*25*1     \n",
    "model.add(Conv2D(filters=50, kernel_size=(10,25), strides=1,activation='relu',kernel_regularizer = regularizers.l2(0.01),kernel_initializer='uniform'))\n",
    "# 321*1*50\n",
    "model.add(Reshape((321,50,1), input_shape=(321,1,50)))\n",
    "# 321*50*1\n",
    "model.add(MaxPooling2D(pool_size=(3,1),padding='valid'))\n",
    "\n",
    "# 3rd cnn block:\n",
    "# 107*50*1\n",
    "model.add(Conv2D(filters=100, kernel_size=(10,50), strides=1,activation='relu',kernel_regularizer = regularizers.l2(0.01),padding='valid',kernel_initializer='uniform'))\n",
    "# 98*1*100\n",
    "model.add(Reshape((98,100,1), input_shape=(98,1,100)))\n",
    "# 98*100*1\n",
    "model.add(MaxPooling2D(pool_size=(3,1),padding='valid'))\n",
    "\n",
    "# 4th cnn block:\n",
    "# 32*100*1\n",
    "model.add(Conv2D(filters=200, kernel_size=(10,100), strides=1,activation='relu',kernel_regularizer = regularizers.l2(0.01),padding='valid',kernel_initializer='uniform'))\n",
    "# 23*1*200\n",
    "model.add(Reshape((23,200,1), input_shape=(23,1,200)))\n",
    "# 23*200*1\n",
    "model.add(MaxPooling2D(pool_size=(3,1),padding='valid'))\n",
    "# 8*200*1\n",
    "\n",
    "\n",
    "# Linear Classification(dense Layer)\n",
    "model.add(Flatten())  \n",
    "\n",
    "#model.add(Dense(4096,activation='relu'))  \n",
    "#model.add(Dropout(0.5)) \n",
    "model.add(Dense(num_classes,activation='softmax'))  \n",
    "\n",
    "\n",
    "opt = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.0005, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])  \n",
    "model.summary()  \n",
    "\n",
    "model.fit(X_train1, y_train, batch_size=batch_size,validation_data=(X_test1,y_test),epochs=epochs,verbose=1)\n",
    "scores = model.evaluate(X_test1, y_test, verbose=1)\n",
    "\n",
    "#print('Test loss:', scores[0])\n",
    "#print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alexnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. A single trial of AlexNet: the model is bad for the EEG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_24 (Conv1D)           (None, 248, 96)           23328     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 123, 96)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 123, 256)          123136    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 61, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 61, 384)           295296    \n",
      "_________________________________________________________________\n",
      "conv1d_27 (Conv1D)           (None, 61, 384)           442752    \n",
      "_________________________________________________________________\n",
      "conv1d_28 (Conv1D)           (None, 61, 256)           295168    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 7680)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 4096)              31461376  \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 4)                 16388     \n",
      "=================================================================\n",
      "Total params: 49,438,756\n",
      "Trainable params: 49,438,756\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 238 samples, validate on 50 samples\n",
      "Epoch 1/10\n",
      "238/238 [==============================] - 10s 44ms/step - loss: 9.8985 - acc: 0.2059 - val_loss: 11.9274 - val_acc: 0.2600\n",
      "Epoch 2/10\n",
      "238/238 [==============================] - 8s 36ms/step - loss: 12.1224 - acc: 0.2479 - val_loss: 11.9274 - val_acc: 0.2600\n",
      "Epoch 3/10\n",
      "238/238 [==============================] - 8s 35ms/step - loss: 12.1224 - acc: 0.2479 - val_loss: 11.9274 - val_acc: 0.2600\n",
      "Epoch 4/10\n",
      "238/238 [==============================] - 8s 36ms/step - loss: 12.1224 - acc: 0.2479 - val_loss: 11.9274 - val_acc: 0.2600\n",
      "Epoch 5/10\n",
      "238/238 [==============================] - 9s 37ms/step - loss: 12.1224 - acc: 0.2479 - val_loss: 11.9274 - val_acc: 0.2600\n",
      "Epoch 6/10\n",
      "238/238 [==============================] - 9s 37ms/step - loss: 12.1224 - acc: 0.2479 - val_loss: 11.9274 - val_acc: 0.2600\n",
      "Epoch 7/10\n",
      "238/238 [==============================] - 9s 37ms/step - loss: 12.1224 - acc: 0.2479 - val_loss: 11.9274 - val_acc: 0.2600\n",
      "Epoch 8/10\n",
      "238/238 [==============================] - 9s 38ms/step - loss: 12.1224 - acc: 0.2479 - val_loss: 11.9274 - val_acc: 0.2600\n",
      "Epoch 9/10\n",
      "238/238 [==============================] - 9s 36ms/step - loss: 12.1224 - acc: 0.2479 - val_loss: 11.9274 - val_acc: 0.2600\n",
      "Epoch 10/10\n",
      "238/238 [==============================] - 8s 36ms/step - loss: 12.1224 - acc: 0.2479 - val_loss: 11.9274 - val_acc: 0.2600\n",
      "50/50 [==============================] - 0s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "epochs = 10  \n",
    "batch_size = 32\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 1st cnn layer: 96 filters of size 11*11 with stride 4\n",
    "#                POOL: 3*3 filters with stride 2\n",
    "model.add(Conv1D(filters=96, kernel_size=11, strides=4, padding='valid',activation='relu',kernel_initializer='uniform',input_shape=X_train.shape[1:]))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=5, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=384, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=384, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Flatten())  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes,activation='softmax'))  \n",
    "\n",
    "\n",
    "opt = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.0005, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])  \n",
    "model.summary()  \n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, validation_data=(X_test,y_test),epochs=epochs)\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. change the some filter size of the AlexNet layers: It's more suitable for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_29 (Conv1D)           (None, 248, 64)           15552     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling (None, 123, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_30 (Conv1D)           (None, 123, 192)          61632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling (None, 61, 192)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_31 (Conv1D)           (None, 61, 384)           221568    \n",
      "_________________________________________________________________\n",
      "conv1d_32 (Conv1D)           (None, 61, 256)           295168    \n",
      "_________________________________________________________________\n",
      "conv1d_33 (Conv1D)           (None, 61, 256)           196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_18 (MaxPooling (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 7680)              0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 4096)              31461376  \n",
      "_________________________________________________________________\n",
      "dropout_11 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 4)                 16388     \n",
      "=================================================================\n",
      "Total params: 49,049,860\n",
      "Trainable params: 49,049,860\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 238 samples, validate on 50 samples\n",
      "Epoch 1/30\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.6876 - acc: 0.2563 - val_loss: 1.4464 - val_acc: 0.2600\n",
      "Epoch 2/30\n",
      "238/238 [==============================] - 6s 26ms/step - loss: 1.5021 - acc: 0.2143 - val_loss: 1.4193 - val_acc: 0.2600\n",
      "Epoch 3/30\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.3912 - acc: 0.3235 - val_loss: 2.3216 - val_acc: 0.2400\n",
      "Epoch 4/30\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.5374 - acc: 0.2815 - val_loss: 1.4007 - val_acc: 0.2600\n",
      "Epoch 5/30\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.4009 - acc: 0.2521 - val_loss: 1.3858 - val_acc: 0.3400\n",
      "Epoch 6/30\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.3732 - acc: 0.2731 - val_loss: 1.3686 - val_acc: 0.2800\n",
      "Epoch 7/30\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.3591 - acc: 0.3109 - val_loss: 1.3693 - val_acc: 0.2800\n",
      "Epoch 8/30\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.3519 - acc: 0.3613 - val_loss: 1.3618 - val_acc: 0.3600\n",
      "Epoch 9/30\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.2896 - acc: 0.4328 - val_loss: 1.4298 - val_acc: 0.2800\n",
      "Epoch 10/30\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.3581 - acc: 0.3235 - val_loss: 1.3618 - val_acc: 0.3000\n",
      "Epoch 11/30\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.2865 - acc: 0.3361 - val_loss: 1.3787 - val_acc: 0.2800\n",
      "Epoch 12/30\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.2266 - acc: 0.4328 - val_loss: 1.4489 - val_acc: 0.3600\n",
      "Epoch 13/30\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.2449 - acc: 0.4370 - val_loss: 1.3636 - val_acc: 0.3200\n",
      "Epoch 14/30\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.1631 - acc: 0.4958 - val_loss: 1.6755 - val_acc: 0.2800\n",
      "Epoch 15/30\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 1.2879 - acc: 0.5168 - val_loss: 1.2822 - val_acc: 0.4000\n",
      "Epoch 16/30\n",
      "238/238 [==============================] - 9s 37ms/step - loss: 1.2285 - acc: 0.4580 - val_loss: 1.4317 - val_acc: 0.3800\n",
      "Epoch 17/30\n",
      "238/238 [==============================] - 8s 34ms/step - loss: 1.1742 - acc: 0.4538 - val_loss: 1.2794 - val_acc: 0.3200\n",
      "Epoch 18/30\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.0742 - acc: 0.5126 - val_loss: 1.3534 - val_acc: 0.3800\n",
      "Epoch 19/30\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.9853 - acc: 0.5798 - val_loss: 1.3505 - val_acc: 0.4000\n",
      "Epoch 20/30\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.9997 - acc: 0.5714 - val_loss: 1.3593 - val_acc: 0.4400\n",
      "Epoch 21/30\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.8545 - acc: 0.6681 - val_loss: 1.3001 - val_acc: 0.3200\n",
      "Epoch 22/30\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.8375 - acc: 0.6513 - val_loss: 1.6963 - val_acc: 0.3200\n",
      "Epoch 23/30\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.9540 - acc: 0.6429 - val_loss: 1.2420 - val_acc: 0.4000\n",
      "Epoch 24/30\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.7547 - acc: 0.7395 - val_loss: 1.7476 - val_acc: 0.4000\n",
      "Epoch 25/30\n",
      "238/238 [==============================] - 8s 35ms/step - loss: 0.6223 - acc: 0.7605 - val_loss: 1.1917 - val_acc: 0.5000\n",
      "Epoch 26/30\n",
      "238/238 [==============================] - 8s 34ms/step - loss: 0.6256 - acc: 0.7605 - val_loss: 1.7896 - val_acc: 0.4200\n",
      "Epoch 27/30\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.6004 - acc: 0.7605 - val_loss: 2.0336 - val_acc: 0.4000\n",
      "Epoch 28/30\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.7105 - acc: 0.7143 - val_loss: 1.4176 - val_acc: 0.4600\n",
      "Epoch 29/30\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.4428 - acc: 0.8487 - val_loss: 2.7291 - val_acc: 0.4200\n",
      "Epoch 30/30\n",
      "238/238 [==============================] - 8s 34ms/step - loss: 0.5266 - acc: 0.8277 - val_loss: 2.1349 - val_acc: 0.4400\n",
      "50/50 [==============================] - 0s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "epochs = 30  \n",
    "batch_size = 32\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 1st cnn layer: 64 filters of size 11*11 with stride 4\n",
    "#                POOL: 3*3 filters with stride 2\n",
    "model.add(Conv1D(filters=64, kernel_size=11, strides=4, padding='valid',activation='relu',kernel_initializer='uniform',input_shape=X_train.shape[1:]))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=192, kernel_size=5, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=384, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Flatten())  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes,activation='softmax'))  \n",
    "\n",
    "\n",
    "opt = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.0005, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])  \n",
    "model.summary()  \n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, validation_data=(X_test,y_test),epochs=epochs)\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3(1). Add one convolutional layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_21 (Conv1D)           (None, 248, 64)           15552     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 123, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 123, 192)          61632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 61, 192)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 61, 384)           221568    \n",
      "_________________________________________________________________\n",
      "conv1d_24 (Conv1D)           (None, 61, 256)           295168    \n",
      "_________________________________________________________________\n",
      "conv1d_25 (Conv1D)           (None, 61, 256)           196864    \n",
      "_________________________________________________________________\n",
      "conv1d_26 (Conv1D)           (None, 61, 256)           196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_25 (Flatten)         (None, 7680)              0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 4096)              31461376  \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 4)                 16388     \n",
      "=================================================================\n",
      "Total params: 49,246,724\n",
      "Trainable params: 49,246,724\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 238 samples, validate on 50 samples\n",
      "Epoch 1/30\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 1.4188 - acc: 0.2437 - val_loss: 1.3857 - val_acc: 0.2400\n",
      "Epoch 2/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 1.4150 - acc: 0.2899 - val_loss: 1.3840 - val_acc: 0.2600\n",
      "Epoch 3/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 1.4090 - acc: 0.2731 - val_loss: 1.3695 - val_acc: 0.2800\n",
      "Epoch 4/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 1.3741 - acc: 0.3025 - val_loss: 1.3760 - val_acc: 0.2600\n",
      "Epoch 5/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 1.3303 - acc: 0.3235 - val_loss: 1.3608 - val_acc: 0.3200\n",
      "Epoch 6/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 1.3063 - acc: 0.3403 - val_loss: 1.3417 - val_acc: 0.3800\n",
      "Epoch 7/30\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 1.3174 - acc: 0.3782 - val_loss: 1.3192 - val_acc: 0.3800\n",
      "Epoch 8/30\n",
      "238/238 [==============================] - 4s 19ms/step - loss: 1.2945 - acc: 0.4034 - val_loss: 1.2897 - val_acc: 0.4000\n",
      "Epoch 9/30\n",
      "238/238 [==============================] - 5s 19ms/step - loss: 1.2548 - acc: 0.4118 - val_loss: 1.2604 - val_acc: 0.3800\n",
      "Epoch 10/30\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 1.2283 - acc: 0.4538 - val_loss: 1.2679 - val_acc: 0.4200\n",
      "Epoch 11/30\n",
      "238/238 [==============================] - 5s 19ms/step - loss: 1.1925 - acc: 0.4664 - val_loss: 1.2411 - val_acc: 0.4200\n",
      "Epoch 12/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 1.1741 - acc: 0.4832 - val_loss: 1.2843 - val_acc: 0.4200\n",
      "Epoch 13/30\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 1.1087 - acc: 0.5420 - val_loss: 1.2645 - val_acc: 0.4400\n",
      "Epoch 14/30\n",
      "238/238 [==============================] - 5s 19ms/step - loss: 1.0404 - acc: 0.5252 - val_loss: 1.3342 - val_acc: 0.4400\n",
      "Epoch 15/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 1.0848 - acc: 0.5420 - val_loss: 1.2980 - val_acc: 0.3800\n",
      "Epoch 16/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 1.0776 - acc: 0.5714 - val_loss: 1.2130 - val_acc: 0.5000\n",
      "Epoch 17/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.9598 - acc: 0.6345 - val_loss: 1.1828 - val_acc: 0.4800\n",
      "Epoch 18/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.9061 - acc: 0.6134 - val_loss: 1.3234 - val_acc: 0.5000\n",
      "Epoch 19/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.8028 - acc: 0.6639 - val_loss: 1.2783 - val_acc: 0.4600\n",
      "Epoch 20/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.7394 - acc: 0.7101 - val_loss: 1.2045 - val_acc: 0.5400\n",
      "Epoch 21/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.6368 - acc: 0.7353 - val_loss: 1.5038 - val_acc: 0.3800\n",
      "Epoch 22/30\n",
      "238/238 [==============================] - 5s 19ms/step - loss: 0.6684 - acc: 0.7269 - val_loss: 1.2819 - val_acc: 0.5200\n",
      "Epoch 23/30\n",
      "238/238 [==============================] - 5s 19ms/step - loss: 0.5123 - acc: 0.7773 - val_loss: 1.7117 - val_acc: 0.5000\n",
      "Epoch 24/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.7012 - acc: 0.7605 - val_loss: 1.0400 - val_acc: 0.5600\n",
      "Epoch 25/30\n",
      "238/238 [==============================] - 5s 19ms/step - loss: 0.5598 - acc: 0.8193 - val_loss: 1.3111 - val_acc: 0.5600\n",
      "Epoch 26/30\n",
      "238/238 [==============================] - 5s 19ms/step - loss: 0.4736 - acc: 0.8277 - val_loss: 1.2820 - val_acc: 0.5000\n",
      "Epoch 27/30\n",
      "238/238 [==============================] - 4s 19ms/step - loss: 0.3720 - acc: 0.8739 - val_loss: 1.7012 - val_acc: 0.4600\n",
      "Epoch 28/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.3240 - acc: 0.8866 - val_loss: 1.8914 - val_acc: 0.5200\n",
      "Epoch 29/30\n",
      "238/238 [==============================] - 4s 19ms/step - loss: 0.3040 - acc: 0.8739 - val_loss: 1.9450 - val_acc: 0.5000\n",
      "Epoch 30/30\n",
      "238/238 [==============================] - 5s 19ms/step - loss: 0.3627 - acc: 0.8571 - val_loss: 1.3466 - val_acc: 0.6200\n",
      "50/50 [==============================] - 0s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "epochs = 30  \n",
    "batch_size = 80\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 1st cnn layer: 64 filters of size 11*11 with stride 4\n",
    "#                POOL: 3*3 filters with stride 2\n",
    "model.add(Conv1D(filters=64, kernel_size=11, strides=4, padding='valid',activation='relu',kernel_initializer='uniform',input_shape=X_train.shape[1:]))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=192, kernel_size=5, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=384, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "# add the layer here\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Flatten())  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(num_classes,activation='softmax'))  \n",
    "\n",
    "\n",
    "opt = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.0005, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])  \n",
    "model.summary()  \n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, validation_data=(X_test,y_test),epochs=epochs)\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "#print('Test loss:', scores[0])\n",
    "#print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3(2). Increase epoch to see the trend of validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_51 (Conv1D)           (None, 248, 64)           15552     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling (None, 123, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 123, 192)          61632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling (None, 61, 192)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 61, 384)           221568    \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 61, 256)           295168    \n",
      "_________________________________________________________________\n",
      "conv1d_55 (Conv1D)           (None, 61, 256)           196864    \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 61, 256)           196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_27 (MaxPooling (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_30 (Flatten)         (None, 7680)              0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 4096)              31461376  \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 4)                 16388     \n",
      "=================================================================\n",
      "Total params: 49,246,724\n",
      "Trainable params: 49,246,724\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 238 samples, validate on 50 samples\n",
      "Epoch 1/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.4732 - acc: 0.2605 - val_loss: 1.4300 - val_acc: 0.2400\n",
      "Epoch 2/100\n",
      "238/238 [==============================] - 4s 17ms/step - loss: 1.4152 - acc: 0.2563 - val_loss: 1.3922 - val_acc: 0.2400\n",
      "Epoch 3/100\n",
      "238/238 [==============================] - 4s 17ms/step - loss: 1.3879 - acc: 0.2815 - val_loss: 1.3748 - val_acc: 0.2800\n",
      "Epoch 4/100\n",
      "238/238 [==============================] - 4s 17ms/step - loss: 1.3700 - acc: 0.3067 - val_loss: 1.3756 - val_acc: 0.3200\n",
      "Epoch 5/100\n",
      "238/238 [==============================] - 4s 17ms/step - loss: 1.3814 - acc: 0.2899 - val_loss: 1.3733 - val_acc: 0.2600\n",
      "Epoch 6/100\n",
      "238/238 [==============================] - 4s 17ms/step - loss: 1.3497 - acc: 0.3193 - val_loss: 1.3583 - val_acc: 0.3200\n",
      "Epoch 7/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 1.3644 - acc: 0.3403 - val_loss: 1.3509 - val_acc: 0.3800\n",
      "Epoch 8/100\n",
      "238/238 [==============================] - 4s 17ms/step - loss: 1.3260 - acc: 0.3613 - val_loss: 1.3473 - val_acc: 0.3200\n",
      "Epoch 9/100\n",
      "238/238 [==============================] - 4s 17ms/step - loss: 1.3250 - acc: 0.3529 - val_loss: 1.3163 - val_acc: 0.3800\n",
      "Epoch 10/100\n",
      "238/238 [==============================] - 5s 19ms/step - loss: 1.2961 - acc: 0.4160 - val_loss: 1.2926 - val_acc: 0.4000\n",
      "Epoch 11/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 1.2305 - acc: 0.4202 - val_loss: 1.3009 - val_acc: 0.4000\n",
      "Epoch 12/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 1.1732 - acc: 0.4706 - val_loss: 1.2726 - val_acc: 0.4000\n",
      "Epoch 13/100\n",
      "238/238 [==============================] - 4s 19ms/step - loss: 1.2336 - acc: 0.4118 - val_loss: 1.2703 - val_acc: 0.4000\n",
      "Epoch 14/100\n",
      "238/238 [==============================] - 5s 19ms/step - loss: 1.1485 - acc: 0.5252 - val_loss: 1.2639 - val_acc: 0.4400\n",
      "Epoch 15/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 1.0704 - acc: 0.5294 - val_loss: 1.3078 - val_acc: 0.4600\n",
      "Epoch 16/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 1.0559 - acc: 0.5378 - val_loss: 1.3020 - val_acc: 0.4400\n",
      "Epoch 17/100\n",
      "238/238 [==============================] - 5s 19ms/step - loss: 1.0532 - acc: 0.5588 - val_loss: 1.2570 - val_acc: 0.4800\n",
      "Epoch 18/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.9243 - acc: 0.6429 - val_loss: 1.3522 - val_acc: 0.4000\n",
      "Epoch 19/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.9459 - acc: 0.5882 - val_loss: 1.3866 - val_acc: 0.4000\n",
      "Epoch 20/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.8846 - acc: 0.6387 - val_loss: 1.2922 - val_acc: 0.5000\n",
      "Epoch 21/100\n",
      "238/238 [==============================] - 4s 19ms/step - loss: 0.7526 - acc: 0.7143 - val_loss: 1.3221 - val_acc: 0.5000\n",
      "Epoch 22/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.7670 - acc: 0.6513 - val_loss: 1.2949 - val_acc: 0.4200\n",
      "Epoch 23/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.6652 - acc: 0.7479 - val_loss: 1.5520 - val_acc: 0.4800\n",
      "Epoch 24/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.7141 - acc: 0.7311 - val_loss: 2.0289 - val_acc: 0.3800\n",
      "Epoch 25/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.7836 - acc: 0.6723 - val_loss: 1.5128 - val_acc: 0.4600\n",
      "Epoch 26/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.7226 - acc: 0.7185 - val_loss: 1.7044 - val_acc: 0.4200\n",
      "Epoch 27/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.6988 - acc: 0.7521 - val_loss: 1.4145 - val_acc: 0.4800\n",
      "Epoch 28/100\n",
      "238/238 [==============================] - 4s 19ms/step - loss: 0.5270 - acc: 0.8319 - val_loss: 1.4945 - val_acc: 0.5600\n",
      "Epoch 29/100\n",
      "238/238 [==============================] - 5s 19ms/step - loss: 0.4950 - acc: 0.8151 - val_loss: 1.6455 - val_acc: 0.5200\n",
      "Epoch 30/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.3064 - acc: 0.8824 - val_loss: 2.0725 - val_acc: 0.4600\n",
      "Epoch 31/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.3737 - acc: 0.8445 - val_loss: 1.5429 - val_acc: 0.5000\n",
      "Epoch 32/100\n",
      "238/238 [==============================] - 5s 19ms/step - loss: 0.3042 - acc: 0.8992 - val_loss: 2.4546 - val_acc: 0.4200\n",
      "Epoch 33/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.2314 - acc: 0.9160 - val_loss: 2.6779 - val_acc: 0.3400\n",
      "Epoch 34/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 0.3274 - acc: 0.8992 - val_loss: 2.3118 - val_acc: 0.4600\n",
      "Epoch 35/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.1957 - acc: 0.9370 - val_loss: 2.0238 - val_acc: 0.5400\n",
      "Epoch 36/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 0.1548 - acc: 0.9412 - val_loss: 2.1038 - val_acc: 0.5400\n",
      "Epoch 37/100\n",
      "238/238 [==============================] - 5s 19ms/step - loss: 0.0834 - acc: 0.9748 - val_loss: 2.4523 - val_acc: 0.5400\n",
      "Epoch 38/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.0509 - acc: 0.9832 - val_loss: 2.7740 - val_acc: 0.4600\n",
      "Epoch 39/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.1157 - acc: 0.9664 - val_loss: 3.2062 - val_acc: 0.5200\n",
      "Epoch 40/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.4155 - acc: 0.8782 - val_loss: 2.7978 - val_acc: 0.4800\n",
      "Epoch 41/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.3026 - acc: 0.8866 - val_loss: 2.7834 - val_acc: 0.4400\n",
      "Epoch 42/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.3622 - acc: 0.8487 - val_loss: 2.3802 - val_acc: 0.4000\n",
      "Epoch 43/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.2185 - acc: 0.9244 - val_loss: 2.2160 - val_acc: 0.4600\n",
      "Epoch 44/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.1862 - acc: 0.9370 - val_loss: 2.3778 - val_acc: 0.4600\n",
      "Epoch 45/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.1913 - acc: 0.9412 - val_loss: 2.1657 - val_acc: 0.4600\n",
      "Epoch 46/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.1092 - acc: 0.9580 - val_loss: 2.4220 - val_acc: 0.5200\n",
      "Epoch 47/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.0596 - acc: 0.9748 - val_loss: 2.5869 - val_acc: 0.5200\n",
      "Epoch 48/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.0687 - acc: 0.9748 - val_loss: 2.3355 - val_acc: 0.5600\n",
      "Epoch 49/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.0483 - acc: 0.9874 - val_loss: 2.1998 - val_acc: 0.5600\n",
      "Epoch 50/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.0542 - acc: 0.9748 - val_loss: 2.3941 - val_acc: 0.6000\n",
      "Epoch 51/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.0668 - acc: 0.9832 - val_loss: 2.5903 - val_acc: 0.5400\n",
      "Epoch 52/100\n",
      "238/238 [==============================] - 5s 23ms/step - loss: 0.0266 - acc: 0.9958 - val_loss: 2.7200 - val_acc: 0.5200\n",
      "Epoch 53/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.0197 - acc: 0.9958 - val_loss: 2.7569 - val_acc: 0.5200\n",
      "Epoch 54/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.0208 - acc: 0.9958 - val_loss: 3.0230 - val_acc: 0.5600\n",
      "Epoch 55/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.0061 - acc: 1.0000 - val_loss: 4.1011 - val_acc: 0.4800\n",
      "Epoch 56/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.0126 - acc: 0.9958 - val_loss: 4.0115 - val_acc: 0.4800\n",
      "Epoch 57/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.0141 - acc: 0.9916 - val_loss: 3.4643 - val_acc: 0.5000\n",
      "Epoch 58/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.0142 - acc: 0.9958 - val_loss: 3.3921 - val_acc: 0.4800\n",
      "Epoch 59/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.0168 - acc: 0.9916 - val_loss: 3.5289 - val_acc: 0.5200\n",
      "Epoch 60/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.0073 - acc: 0.9958 - val_loss: 4.4402 - val_acc: 0.5200\n",
      "Epoch 61/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.0461 - acc: 0.9832 - val_loss: 4.5486 - val_acc: 0.5600\n",
      "Epoch 62/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.0128 - acc: 0.9958 - val_loss: 4.1412 - val_acc: 0.5400\n",
      "Epoch 63/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.0526 - acc: 0.9916 - val_loss: 3.7656 - val_acc: 0.5400\n",
      "Epoch 64/100\n",
      "238/238 [==============================] - 5s 19ms/step - loss: 0.0262 - acc: 0.9916 - val_loss: 3.5620 - val_acc: 0.5000\n",
      "Epoch 65/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 0.0212 - acc: 0.9916 - val_loss: 3.4439 - val_acc: 0.4800\n",
      "Epoch 66/100\n",
      "238/238 [==============================] - 5s 19ms/step - loss: 0.0313 - acc: 0.9958 - val_loss: 3.2050 - val_acc: 0.5400\n",
      "Epoch 67/100\n",
      "238/238 [==============================] - 4s 19ms/step - loss: 0.0269 - acc: 0.9916 - val_loss: 2.8327 - val_acc: 0.5400\n",
      "Epoch 68/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.0165 - acc: 0.9958 - val_loss: 2.9240 - val_acc: 0.5400\n",
      "Epoch 69/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.0135 - acc: 0.9916 - val_loss: 2.8608 - val_acc: 0.5200\n",
      "Epoch 70/100\n",
      "238/238 [==============================] - 5s 19ms/step - loss: 0.0092 - acc: 0.9958 - val_loss: 2.8427 - val_acc: 0.5600\n",
      "Epoch 71/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 0.0024 - acc: 1.0000 - val_loss: 2.8161 - val_acc: 0.6200\n",
      "Epoch 72/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.0026 - acc: 1.0000 - val_loss: 2.8835 - val_acc: 0.6200\n",
      "Epoch 73/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.0052 - acc: 1.0000 - val_loss: 2.9101 - val_acc: 0.6200\n",
      "Epoch 74/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 9.8711e-04 - acc: 1.0000 - val_loss: 3.0118 - val_acc: 0.5800\n",
      "Epoch 75/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.0056 - acc: 0.9958 - val_loss: 2.9334 - val_acc: 0.5800\n",
      "Epoch 76/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 7.7228e-04 - acc: 1.0000 - val_loss: 2.8732 - val_acc: 0.6000\n",
      "Epoch 77/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.0025 - acc: 1.0000 - val_loss: 2.8814 - val_acc: 0.6000\n",
      "Epoch 78/100\n",
      "238/238 [==============================] - 4s 19ms/step - loss: 3.9767e-04 - acc: 1.0000 - val_loss: 2.9302 - val_acc: 0.5600\n",
      "Epoch 79/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.0050 - acc: 1.0000 - val_loss: 3.0236 - val_acc: 0.5600\n",
      "Epoch 80/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 3.3090e-04 - acc: 1.0000 - val_loss: 3.1317 - val_acc: 0.6000\n",
      "Epoch 81/100\n",
      "238/238 [==============================] - 5s 19ms/step - loss: 0.0015 - acc: 1.0000 - val_loss: 3.2537 - val_acc: 0.6000\n",
      "Epoch 82/100\n",
      "238/238 [==============================] - 4s 19ms/step - loss: 2.0289e-04 - acc: 1.0000 - val_loss: 3.3596 - val_acc: 0.5600\n",
      "Epoch 83/100\n",
      "238/238 [==============================] - 4s 19ms/step - loss: 2.2619e-04 - acc: 1.0000 - val_loss: 3.4502 - val_acc: 0.5600\n",
      "Epoch 84/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 9.2385e-04 - acc: 1.0000 - val_loss: 3.4800 - val_acc: 0.5800\n",
      "Epoch 85/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 3.4234 - val_acc: 0.5800\n",
      "Epoch 86/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.0054 - acc: 1.0000 - val_loss: 3.4473 - val_acc: 0.5800\n",
      "Epoch 87/100\n",
      "238/238 [==============================] - 5s 19ms/step - loss: 3.4701e-04 - acc: 1.0000 - val_loss: 3.7125 - val_acc: 0.5600\n",
      "Epoch 88/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 6.1999e-04 - acc: 1.0000 - val_loss: 3.9039 - val_acc: 0.5400\n",
      "Epoch 89/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.0042 - acc: 0.9958 - val_loss: 3.7945 - val_acc: 0.5600\n",
      "Epoch 90/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.0091 - acc: 0.9958 - val_loss: 3.6597 - val_acc: 0.5800\n",
      "Epoch 91/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 2.9400e-04 - acc: 1.0000 - val_loss: 3.6700 - val_acc: 0.5400\n",
      "Epoch 92/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 2.9341e-04 - acc: 1.0000 - val_loss: 3.7103 - val_acc: 0.5600\n",
      "Epoch 93/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 5.9304e-04 - acc: 1.0000 - val_loss: 3.6942 - val_acc: 0.5800\n",
      "Epoch 94/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 1.7113e-04 - acc: 1.0000 - val_loss: 3.6701 - val_acc: 0.5600\n",
      "Epoch 95/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 3.6317 - val_acc: 0.5400\n",
      "Epoch 96/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.0067 - acc: 0.9958 - val_loss: 3.3131 - val_acc: 0.5600\n",
      "Epoch 97/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.0077 - acc: 0.9958 - val_loss: 3.1663 - val_acc: 0.5800\n",
      "Epoch 98/100\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 0.0106 - acc: 0.9958 - val_loss: 3.3508 - val_acc: 0.5800\n",
      "Epoch 99/100\n",
      "238/238 [==============================] - 4s 19ms/step - loss: 0.0031 - acc: 1.0000 - val_loss: 3.6564 - val_acc: 0.5600\n",
      "Epoch 100/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 9.2836e-04 - acc: 1.0000 - val_loss: 3.9284 - val_acc: 0.5000\n",
      "50/50 [==============================] - 0s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "epochs = 100  \n",
    "batch_size = 80\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 1st cnn layer: 64 filters of size 11*11 with stride 4\n",
    "#                POOL: 3*3 filters with stride 2\n",
    "model.add(Conv1D(filters=64, kernel_size=11, strides=4, padding='valid',activation='relu',kernel_initializer='uniform',input_shape=X_train.shape[1:]))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=192, kernel_size=5, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=384, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Flatten())  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(num_classes,activation='softmax'))  \n",
    "\n",
    "\n",
    "opt = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.0005, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])  \n",
    "model.summary()  \n",
    "\n",
    "time_callback = TimeHistory()\n",
    "acc_callback = AccuracyHistory()\n",
    "model.fit(X_train, y_train, batch_size=batch_size, validation_data=(X_test,y_test),callbacks=[time_callback,acc_callback],epochs=epochs)\n",
    "times = time_callback.times\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "#print('Test loss:', scores[0])\n",
    "#print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b395779d30>]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b2dd31aef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "times[1:]\n",
    "plt.plot(times[1:],acc_callback.testaccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3(3). Change activation functions: elu is not good for the EEG data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_57 (Conv1D)           (None, 248, 64)           15552     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 123, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 123, 192)          61632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 61, 192)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 61, 384)           221568    \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 61, 256)           295168    \n",
      "_________________________________________________________________\n",
      "conv1d_61 (Conv1D)           (None, 61, 256)           196864    \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 61, 256)           196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_31 (Flatten)         (None, 7680)              0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 4096)              31461376  \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 4)                 16388     \n",
      "=================================================================\n",
      "Total params: 49,246,724\n",
      "Trainable params: 49,246,724\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 238 samples, validate on 50 samples\n",
      "Epoch 1/30\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 3.4840 - acc: 0.2143 - val_loss: 11.3269 - val_acc: 0.2600\n",
      "Epoch 2/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 11.3097 - acc: 0.2773 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 3/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 12.0493 - acc: 0.2479 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 4/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 5/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 6/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 7/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 8/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 9/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 10/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 11/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 12/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 13/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 14/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 15/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 16/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 17/30\n",
      "238/238 [==============================] - 4s 19ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 18/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 19/30\n",
      "238/238 [==============================] - 4s 19ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 20/30\n",
      "238/238 [==============================] - 5s 19ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 21/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 22/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 23/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 24/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 25/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 26/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 27/30\n",
      "238/238 [==============================] - 4s 19ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 28/30\n",
      "238/238 [==============================] - 5s 23ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 29/30\n",
      "238/238 [==============================] - 5s 19ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 30/30\n",
      "238/238 [==============================] - 4s 18ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "50/50 [==============================] - 0s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "epochs = 30  \n",
    "batch_size = 80\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 1st cnn layer: 64 filters of size 11*11 with stride 4\n",
    "#                POOL: 3*3 filters with stride 2\n",
    "model.add(Conv1D(filters=64, kernel_size=11, strides=4, padding='valid',activation='elu',kernel_initializer='uniform',input_shape=X_train.shape[1:]))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=192, kernel_size=5, strides=1, padding='same',activation='elu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=384, kernel_size=3, strides=1, padding='same',activation='elu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='elu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='elu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='elu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Flatten())  \n",
    "model.add(Dense(4096,activation='elu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(4096,activation='elu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(num_classes,activation='softmax'))  \n",
    "\n",
    "\n",
    "opt = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.0005, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])  \n",
    "model.summary()  \n",
    "\n",
    "model.fit(X_train, y_train, batch_size=batch_size, validation_data=(X_test,y_test),epochs=epochs)\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4(1). Other ways to increase validation accuracy--add regularization doesn't change the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_69 (Conv1D)           (None, 248, 64)           15552     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 123, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 123, 192)          61632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 61, 192)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 61, 384)           221568    \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 61, 256)           295168    \n",
      "_________________________________________________________________\n",
      "conv1d_73 (Conv1D)           (None, 61, 256)           196864    \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 61, 256)           196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_33 (Flatten)         (None, 7680)              0         \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 4096)              31461376  \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 4)                 16388     \n",
      "=================================================================\n",
      "Total params: 49,246,724\n",
      "Trainable params: 49,246,724\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 238 samples, validate on 50 samples\n",
      "Epoch 1/100\n",
      "238/238 [==============================] - 9s 40ms/step - loss: 103.9838 - acc: 0.1891 - val_loss: 103.7427 - val_acc: 0.2600\n",
      "Epoch 2/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 103.6247 - acc: 0.2311 - val_loss: 103.2260 - val_acc: 0.2200\n",
      "Epoch 3/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 103.0129 - acc: 0.2773 - val_loss: 102.5314 - val_acc: 0.2800\n",
      "Epoch 4/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 102.2601 - acc: 0.2815 - val_loss: 101.6983 - val_acc: 0.2400\n",
      "Epoch 5/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 101.3861 - acc: 0.2941 - val_loss: 100.7758 - val_acc: 0.2800\n",
      "Epoch 6/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 100.4402 - acc: 0.3193 - val_loss: 99.7876 - val_acc: 0.3200\n",
      "Epoch 7/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 99.4194 - acc: 0.3782 - val_loss: 98.7426 - val_acc: 0.3000\n",
      "Epoch 8/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 98.3419 - acc: 0.4034 - val_loss: 97.6682 - val_acc: 0.3800\n",
      "Epoch 9/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 97.2607 - acc: 0.4412 - val_loss: 96.5751 - val_acc: 0.3600\n",
      "Epoch 10/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 96.1108 - acc: 0.4538 - val_loss: 95.4916 - val_acc: 0.3600\n",
      "Epoch 11/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 95.0169 - acc: 0.4412 - val_loss: 94.4331 - val_acc: 0.3400\n",
      "Epoch 12/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 93.9061 - acc: 0.4790 - val_loss: 93.3380 - val_acc: 0.2600\n",
      "Epoch 13/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 92.8432 - acc: 0.4622 - val_loss: 92.3119 - val_acc: 0.3600\n",
      "Epoch 14/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 91.7429 - acc: 0.5168 - val_loss: 91.1900 - val_acc: 0.3400\n",
      "Epoch 15/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 90.6749 - acc: 0.4664 - val_loss: 90.0783 - val_acc: 0.4200\n",
      "Epoch 16/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 89.5278 - acc: 0.5756 - val_loss: 89.0116 - val_acc: 0.4600\n",
      "Epoch 17/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 88.4709 - acc: 0.5000 - val_loss: 88.0945 - val_acc: 0.4000\n",
      "Epoch 18/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 87.4052 - acc: 0.5672 - val_loss: 87.0044 - val_acc: 0.4000\n",
      "Epoch 19/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 86.3629 - acc: 0.5714 - val_loss: 85.9794 - val_acc: 0.4400\n",
      "Epoch 20/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 85.2493 - acc: 0.6345 - val_loss: 84.9794 - val_acc: 0.4600\n",
      "Epoch 21/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 84.2201 - acc: 0.6387 - val_loss: 84.0344 - val_acc: 0.4800\n",
      "Epoch 22/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 83.1846 - acc: 0.6765 - val_loss: 83.0423 - val_acc: 0.4600\n",
      "Epoch 23/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 82.1765 - acc: 0.7143 - val_loss: 82.2448 - val_acc: 0.4600\n",
      "Epoch 24/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 81.0763 - acc: 0.7731 - val_loss: 81.2642 - val_acc: 0.5200\n",
      "Epoch 25/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 80.1383 - acc: 0.7353 - val_loss: 80.4089 - val_acc: 0.4400\n",
      "Epoch 26/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 79.1869 - acc: 0.7605 - val_loss: 79.3821 - val_acc: 0.5000\n",
      "Epoch 27/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 78.1406 - acc: 0.8361 - val_loss: 78.4356 - val_acc: 0.4600\n",
      "Epoch 28/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 77.1795 - acc: 0.8445 - val_loss: 77.6192 - val_acc: 0.4800\n",
      "Epoch 29/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 76.1940 - acc: 0.8992 - val_loss: 77.0747 - val_acc: 0.4800\n",
      "Epoch 30/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 75.2639 - acc: 0.9286 - val_loss: 76.1672 - val_acc: 0.5000\n",
      "Epoch 31/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 74.3665 - acc: 0.9370 - val_loss: 75.4279 - val_acc: 0.4400\n",
      "Epoch 32/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 73.4833 - acc: 0.9622 - val_loss: 74.6623 - val_acc: 0.5200\n",
      "Epoch 33/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 72.6453 - acc: 0.9454 - val_loss: 73.7604 - val_acc: 0.4600\n",
      "Epoch 34/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 71.7896 - acc: 0.9538 - val_loss: 73.7837 - val_acc: 0.4600\n",
      "Epoch 35/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 71.0227 - acc: 0.9370 - val_loss: 72.4672 - val_acc: 0.4800\n",
      "Epoch 36/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 70.2403 - acc: 0.9202 - val_loss: 71.3276 - val_acc: 0.5400\n",
      "Epoch 37/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 69.3639 - acc: 0.9328 - val_loss: 70.7289 - val_acc: 0.4600\n",
      "Epoch 38/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 68.5667 - acc: 0.9580 - val_loss: 69.9664 - val_acc: 0.4600\n",
      "Epoch 39/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 67.7783 - acc: 0.9580 - val_loss: 68.9105 - val_acc: 0.4400\n",
      "Epoch 40/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 66.9777 - acc: 0.9748 - val_loss: 68.4323 - val_acc: 0.4200\n",
      "Epoch 41/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 66.2285 - acc: 0.9706 - val_loss: 68.5226 - val_acc: 0.4600\n",
      "Epoch 42/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 65.4868 - acc: 0.9790 - val_loss: 67.1231 - val_acc: 0.4400\n",
      "Epoch 43/100\n",
      "238/238 [==============================] - 6s 26ms/step - loss: 64.7263 - acc: 0.9748 - val_loss: 66.3629 - val_acc: 0.4600\n",
      "Epoch 44/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 63.9715 - acc: 0.9748 - val_loss: 65.8312 - val_acc: 0.5200\n",
      "Epoch 45/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 63.2326 - acc: 0.9790 - val_loss: 65.0891 - val_acc: 0.5400\n",
      "Epoch 46/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 62.4918 - acc: 0.9958 - val_loss: 64.2942 - val_acc: 0.5200\n",
      "Epoch 47/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 61.8059 - acc: 0.9832 - val_loss: 63.2733 - val_acc: 0.5400\n",
      "Epoch 48/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 61.0884 - acc: 0.9958 - val_loss: 63.2595 - val_acc: 0.4800\n",
      "Epoch 49/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 60.4333 - acc: 0.9790 - val_loss: 62.2473 - val_acc: 0.5000\n",
      "Epoch 50/100\n",
      "238/238 [==============================] - 6s 25ms/step - loss: 59.7356 - acc: 0.9790 - val_loss: 61.4193 - val_acc: 0.4200\n",
      "Epoch 51/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 59.0487 - acc: 0.9916 - val_loss: 60.8832 - val_acc: 0.5200\n",
      "Epoch 52/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 58.3917 - acc: 0.9874 - val_loss: 60.3700 - val_acc: 0.5200\n",
      "Epoch 53/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 57.7112 - acc: 0.9958 - val_loss: 59.7685 - val_acc: 0.5000\n",
      "Epoch 54/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 57.0673 - acc: 1.0000 - val_loss: 59.0105 - val_acc: 0.5400\n",
      "Epoch 55/100\n",
      "238/238 [==============================] - 5s 23ms/step - loss: 56.4229 - acc: 0.9958 - val_loss: 58.1685 - val_acc: 0.5800\n",
      "Epoch 56/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 55.7823 - acc: 1.0000 - val_loss: 57.3509 - val_acc: 0.6000\n",
      "Epoch 57/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 55.1598 - acc: 0.9958 - val_loss: 56.7127 - val_acc: 0.6000\n",
      "Epoch 58/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 54.5467 - acc: 0.9958 - val_loss: 56.3896 - val_acc: 0.5200\n",
      "Epoch 59/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 53.9279 - acc: 1.0000 - val_loss: 55.8963 - val_acc: 0.5400\n",
      "Epoch 60/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 53.3287 - acc: 1.0000 - val_loss: 54.9545 - val_acc: 0.6200\n",
      "Epoch 61/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 52.7303 - acc: 1.0000 - val_loss: 54.4372 - val_acc: 0.5600\n",
      "Epoch 62/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 52.1432 - acc: 1.0000 - val_loss: 53.8948 - val_acc: 0.6000\n",
      "Epoch 63/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 51.5564 - acc: 1.0000 - val_loss: 53.5103 - val_acc: 0.5600\n",
      "Epoch 64/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 50.9844 - acc: 1.0000 - val_loss: 53.0916 - val_acc: 0.5400\n",
      "Epoch 65/100\n",
      "238/238 [==============================] - 5s 23ms/step - loss: 50.4173 - acc: 1.0000 - val_loss: 52.4737 - val_acc: 0.5600\n",
      "Epoch 66/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 49.8590 - acc: 1.0000 - val_loss: 51.8196 - val_acc: 0.5400\n",
      "Epoch 67/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 49.3041 - acc: 1.0000 - val_loss: 51.2539 - val_acc: 0.5200\n",
      "Epoch 68/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 48.7577 - acc: 1.0000 - val_loss: 50.6931 - val_acc: 0.5000\n",
      "Epoch 69/100\n",
      "238/238 [==============================] - 6s 23ms/step - loss: 48.2208 - acc: 1.0000 - val_loss: 50.1009 - val_acc: 0.5000\n",
      "Epoch 70/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 47.6872 - acc: 1.0000 - val_loss: 49.5451 - val_acc: 0.5000\n",
      "Epoch 71/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 47.1606 - acc: 1.0000 - val_loss: 49.0025 - val_acc: 0.5200\n",
      "Epoch 72/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 46.6405 - acc: 1.0000 - val_loss: 48.4901 - val_acc: 0.5400\n",
      "Epoch 73/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 46.1278 - acc: 1.0000 - val_loss: 47.9834 - val_acc: 0.5600\n",
      "Epoch 74/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 45.6221 - acc: 1.0000 - val_loss: 47.4604 - val_acc: 0.5000\n",
      "Epoch 75/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 45.1209 - acc: 1.0000 - val_loss: 46.9287 - val_acc: 0.4800\n",
      "Epoch 76/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 44.6267 - acc: 1.0000 - val_loss: 46.3918 - val_acc: 0.5400\n",
      "Epoch 77/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 44.1398 - acc: 1.0000 - val_loss: 45.8508 - val_acc: 0.5400\n",
      "Epoch 78/100\n",
      "238/238 [==============================] - 6s 23ms/step - loss: 43.6567 - acc: 1.0000 - val_loss: 45.3592 - val_acc: 0.5200\n",
      "Epoch 79/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 43.1800 - acc: 1.0000 - val_loss: 44.8906 - val_acc: 0.5000\n",
      "Epoch 80/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 42.7096 - acc: 1.0000 - val_loss: 44.4168 - val_acc: 0.5200\n",
      "Epoch 81/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 42.2457 - acc: 1.0000 - val_loss: 43.9638 - val_acc: 0.5400\n",
      "Epoch 82/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 41.7881 - acc: 1.0000 - val_loss: 43.5051 - val_acc: 0.5400\n",
      "Epoch 83/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 41.3358 - acc: 1.0000 - val_loss: 43.0321 - val_acc: 0.5600\n",
      "Epoch 84/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 40.8879 - acc: 1.0000 - val_loss: 42.5747 - val_acc: 0.5600\n",
      "Epoch 85/100\n",
      "238/238 [==============================] - 6s 25ms/step - loss: 40.4457 - acc: 1.0000 - val_loss: 42.1305 - val_acc: 0.5200\n",
      "Epoch 86/100\n",
      "238/238 [==============================] - 6s 26ms/step - loss: 40.0074 - acc: 1.0000 - val_loss: 41.7051 - val_acc: 0.4800\n",
      "Epoch 87/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 39.5770 - acc: 1.0000 - val_loss: 41.2719 - val_acc: 0.5000\n",
      "Epoch 88/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 39.1492 - acc: 1.0000 - val_loss: 40.8447 - val_acc: 0.4800\n",
      "Epoch 89/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 38.7316 - acc: 1.0000 - val_loss: 40.4069 - val_acc: 0.5400\n",
      "Epoch 90/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 38.3132 - acc: 1.0000 - val_loss: 39.9920 - val_acc: 0.5200\n",
      "Epoch 91/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 37.9043 - acc: 1.0000 - val_loss: 39.5586 - val_acc: 0.5600\n",
      "Epoch 92/100\n",
      "238/238 [==============================] - 5s 23ms/step - loss: 37.4987 - acc: 1.0000 - val_loss: 39.1838 - val_acc: 0.5200\n",
      "Epoch 93/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 37.0963 - acc: 1.0000 - val_loss: 38.8333 - val_acc: 0.5200\n",
      "Epoch 94/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 36.7000 - acc: 1.0000 - val_loss: 38.4482 - val_acc: 0.5600\n",
      "Epoch 95/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 36.3085 - acc: 1.0000 - val_loss: 38.0769 - val_acc: 0.5600\n",
      "Epoch 96/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 35.9225 - acc: 1.0000 - val_loss: 37.7031 - val_acc: 0.5600\n",
      "Epoch 97/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 35.5408 - acc: 1.0000 - val_loss: 37.3156 - val_acc: 0.5600\n",
      "Epoch 98/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 35.1622 - acc: 1.0000 - val_loss: 36.9297 - val_acc: 0.5600\n",
      "Epoch 99/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 34.7883 - acc: 1.0000 - val_loss: 36.5758 - val_acc: 0.5200\n",
      "Epoch 100/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 34.4225 - acc: 1.0000 - val_loss: 36.2260 - val_acc: 0.5400\n",
      "50/50 [==============================] - 1s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "epochs = 100  \n",
    "batch_size = 80\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 1st cnn layer: 64 filters of size 11*11 with stride 4\n",
    "#                POOL: 3*3 filters with stride 2\n",
    "model.add(Conv1D(filters=64, kernel_size=11, strides=4, padding='valid',activation='relu',kernel_regularizer = regularizers.l2(0.01),kernel_initializer='uniform',input_shape=X_train.shape[1:]))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=192, kernel_size=5, strides=1, padding='same',activation='relu',kernel_regularizer = regularizers.l2(0.01),kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=384, kernel_size=3, strides=1, padding='same',activation='relu',kernel_regularizer = regularizers.l2(0.01),kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_regularizer = regularizers.l2(0.01),kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_regularizer = regularizers.l2(0.01),kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_regularizer = regularizers.l2(0.01),kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Flatten())  \n",
    "model.add(Dense(4096,activation='relu',kernel_regularizer = regularizers.l2(0.01)))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(4096,activation='relu',kernel_regularizer = regularizers.l2(0.01)))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(num_classes,activation='softmax'))  \n",
    "\n",
    "\n",
    "opt = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.0005, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])  \n",
    "model.summary()  \n",
    "\n",
    "time_callback = TimeHistory()\n",
    "acc_callback = AccuracyHistory()\n",
    "model.fit(X_train, y_train, batch_size=batch_size, validation_data=(X_test,y_test),callbacks=[time_callback,acc_callback],epochs=epochs)\n",
    "times = time_callback.times\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b3c34fb978>]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b39582eef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "times[1:]\n",
    "plt.plot(times[1:],acc_callback.testaccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4(2). Other ways to increase validation accuracy--change batch size to 64: the best accuracy is 62% and it's more stable than the batch size of 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_99 (Conv1D)           (None, 248, 64)           15552     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_49 (MaxPooling (None, 123, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_100 (Conv1D)          (None, 123, 192)          61632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_50 (MaxPooling (None, 61, 192)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_101 (Conv1D)          (None, 61, 384)           221568    \n",
      "_________________________________________________________________\n",
      "conv1d_102 (Conv1D)          (None, 61, 256)           295168    \n",
      "_________________________________________________________________\n",
      "conv1d_103 (Conv1D)          (None, 61, 256)           196864    \n",
      "_________________________________________________________________\n",
      "conv1d_104 (Conv1D)          (None, 61, 256)           196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_51 (MaxPooling (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_38 (Flatten)         (None, 7680)              0         \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 4096)              31461376  \n",
      "_________________________________________________________________\n",
      "dropout_63 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_64 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 4)                 16388     \n",
      "=================================================================\n",
      "Total params: 49,246,724\n",
      "Trainable params: 49,246,724\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 238 samples, validate on 50 samples\n",
      "Epoch 1/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 1.5078 - acc: 0.2101 - val_loss: 1.4156 - val_acc: 0.2600\n",
      "Epoch 2/100\n",
      "238/238 [==============================] - 5s 19ms/step - loss: 1.4272 - acc: 0.2773 - val_loss: 1.3964 - val_acc: 0.2400\n",
      "Epoch 3/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 1.4251 - acc: 0.1975 - val_loss: 1.3909 - val_acc: 0.2400\n",
      "Epoch 4/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 1.3994 - acc: 0.2605 - val_loss: 1.3854 - val_acc: 0.3400\n",
      "Epoch 5/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 1.4005 - acc: 0.2479 - val_loss: 1.3821 - val_acc: 0.2400\n",
      "Epoch 6/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 1.3737 - acc: 0.2941 - val_loss: 1.3731 - val_acc: 0.2600\n",
      "Epoch 7/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 1.3516 - acc: 0.3655 - val_loss: 1.3564 - val_acc: 0.3000\n",
      "Epoch 8/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 1.3416 - acc: 0.3529 - val_loss: 1.3510 - val_acc: 0.3600\n",
      "Epoch 9/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 1.3032 - acc: 0.3950 - val_loss: 1.3190 - val_acc: 0.4400\n",
      "Epoch 10/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 1.2826 - acc: 0.4076 - val_loss: 1.2974 - val_acc: 0.3600\n",
      "Epoch 11/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 1.2218 - acc: 0.4496 - val_loss: 1.2885 - val_acc: 0.4000\n",
      "Epoch 12/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 1.2371 - acc: 0.4580 - val_loss: 1.2789 - val_acc: 0.3800\n",
      "Epoch 13/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 1.1971 - acc: 0.4790 - val_loss: 1.2486 - val_acc: 0.3800\n",
      "Epoch 14/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 1.1193 - acc: 0.5126 - val_loss: 1.2105 - val_acc: 0.4600\n",
      "Epoch 15/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 1.0366 - acc: 0.6008 - val_loss: 1.2386 - val_acc: 0.4800\n",
      "Epoch 16/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.9851 - acc: 0.5966 - val_loss: 1.2152 - val_acc: 0.5200\n",
      "Epoch 17/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 0.9602 - acc: 0.6050 - val_loss: 1.2426 - val_acc: 0.4400\n",
      "Epoch 18/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 0.8642 - acc: 0.6639 - val_loss: 1.2710 - val_acc: 0.4800\n",
      "Epoch 19/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 0.7835 - acc: 0.6849 - val_loss: 1.2747 - val_acc: 0.4800\n",
      "Epoch 20/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.7887 - acc: 0.6639 - val_loss: 1.4066 - val_acc: 0.4200\n",
      "Epoch 21/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.7276 - acc: 0.7059 - val_loss: 1.5929 - val_acc: 0.4200\n",
      "Epoch 22/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.6888 - acc: 0.7395 - val_loss: 1.3660 - val_acc: 0.4000\n",
      "Epoch 23/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.9085 - acc: 0.6134 - val_loss: 1.4865 - val_acc: 0.4400\n",
      "Epoch 24/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.7339 - acc: 0.7185 - val_loss: 1.5702 - val_acc: 0.5000\n",
      "Epoch 25/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 0.6028 - acc: 0.8067 - val_loss: 1.7113 - val_acc: 0.3800\n",
      "Epoch 26/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 0.4818 - acc: 0.8277 - val_loss: 1.9125 - val_acc: 0.4400\n",
      "Epoch 27/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.5708 - acc: 0.7605 - val_loss: 1.4218 - val_acc: 0.5800\n",
      "Epoch 28/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 0.4834 - acc: 0.8277 - val_loss: 2.2669 - val_acc: 0.3600\n",
      "Epoch 29/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 0.6145 - acc: 0.7521 - val_loss: 1.8518 - val_acc: 0.5200\n",
      "Epoch 30/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.3477 - acc: 0.8866 - val_loss: 2.1349 - val_acc: 0.4800\n",
      "Epoch 31/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.2832 - acc: 0.9118 - val_loss: 2.0192 - val_acc: 0.5800\n",
      "Epoch 32/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.2354 - acc: 0.9244 - val_loss: 2.2694 - val_acc: 0.5000\n",
      "Epoch 33/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.2006 - acc: 0.9328 - val_loss: 3.1337 - val_acc: 0.5200\n",
      "Epoch 34/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.2818 - acc: 0.9034 - val_loss: 3.1970 - val_acc: 0.4200\n",
      "Epoch 35/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.4318 - acc: 0.8319 - val_loss: 2.9523 - val_acc: 0.4800\n",
      "Epoch 36/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.3150 - acc: 0.8697 - val_loss: 2.1212 - val_acc: 0.5000\n",
      "Epoch 37/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.2398 - acc: 0.8992 - val_loss: 2.8046 - val_acc: 0.4800\n",
      "Epoch 38/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 0.1974 - acc: 0.9328 - val_loss: 2.2871 - val_acc: 0.5400\n",
      "Epoch 39/100\n",
      "238/238 [==============================] - 5s 23ms/step - loss: 0.1964 - acc: 0.9370 - val_loss: 3.4967 - val_acc: 0.4600\n",
      "Epoch 40/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 0.1194 - acc: 0.9622 - val_loss: 3.3521 - val_acc: 0.4600\n",
      "Epoch 41/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.1699 - acc: 0.9328 - val_loss: 2.7027 - val_acc: 0.4800\n",
      "Epoch 42/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.1441 - acc: 0.9496 - val_loss: 2.6443 - val_acc: 0.5200\n",
      "Epoch 43/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.0969 - acc: 0.9622 - val_loss: 3.4381 - val_acc: 0.4000\n",
      "Epoch 44/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 0.0822 - acc: 0.9622 - val_loss: 2.8407 - val_acc: 0.5400\n",
      "Epoch 45/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.0477 - acc: 0.9790 - val_loss: 3.5350 - val_acc: 0.4800\n",
      "Epoch 46/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.0674 - acc: 0.9748 - val_loss: 3.6225 - val_acc: 0.5000\n",
      "Epoch 47/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.0586 - acc: 0.9874 - val_loss: 2.9417 - val_acc: 0.4800\n",
      "Epoch 48/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 0.0310 - acc: 0.9916 - val_loss: 3.4969 - val_acc: 0.4200\n",
      "Epoch 49/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 0.0388 - acc: 0.9790 - val_loss: 3.7216 - val_acc: 0.4800\n",
      "Epoch 50/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.0144 - acc: 1.0000 - val_loss: 3.4614 - val_acc: 0.5200\n",
      "Epoch 51/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.0109 - acc: 0.9958 - val_loss: 3.1329 - val_acc: 0.5600\n",
      "Epoch 52/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.0072 - acc: 1.0000 - val_loss: 3.3083 - val_acc: 0.5600\n",
      "Epoch 53/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.0134 - acc: 0.9916 - val_loss: 3.5178 - val_acc: 0.5200\n",
      "Epoch 54/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 0.0034 - acc: 1.0000 - val_loss: 3.8600 - val_acc: 0.5800\n",
      "Epoch 55/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 0.0159 - acc: 0.9958 - val_loss: 3.9446 - val_acc: 0.5400\n",
      "Epoch 56/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.0224 - acc: 0.9874 - val_loss: 3.3640 - val_acc: 0.6000\n",
      "Epoch 57/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 0.0039 - acc: 1.0000 - val_loss: 3.6427 - val_acc: 0.5400\n",
      "Epoch 58/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 0.0446 - acc: 0.9874 - val_loss: 3.7145 - val_acc: 0.4800\n",
      "Epoch 59/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.0406 - acc: 0.9958 - val_loss: 3.5190 - val_acc: 0.5800\n",
      "Epoch 60/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 0.0230 - acc: 0.9916 - val_loss: 3.7703 - val_acc: 0.4600\n",
      "Epoch 61/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.0166 - acc: 0.9958 - val_loss: 3.7043 - val_acc: 0.5200\n",
      "Epoch 62/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.0051 - acc: 1.0000 - val_loss: 3.7553 - val_acc: 0.5000\n",
      "Epoch 63/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.0046 - acc: 0.9958 - val_loss: 3.7182 - val_acc: 0.5400\n",
      "Epoch 64/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 0.0073 - acc: 0.9958 - val_loss: 3.6858 - val_acc: 0.5800\n",
      "Epoch 65/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 0.0042 - acc: 1.0000 - val_loss: 3.8449 - val_acc: 0.5800\n",
      "Epoch 66/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 0.0130 - acc: 0.9958 - val_loss: 3.8003 - val_acc: 0.6200\n",
      "Epoch 67/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 3.7151 - val_acc: 0.6200\n",
      "Epoch 68/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 0.0031 - acc: 1.0000 - val_loss: 3.6908 - val_acc: 0.6000\n",
      "Epoch 69/100\n",
      "238/238 [==============================] - 6s 23ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 3.7479 - val_acc: 0.5800\n",
      "Epoch 70/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 4.3825e-04 - acc: 1.0000 - val_loss: 3.9224 - val_acc: 0.5800\n",
      "Epoch 71/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.0057 - acc: 0.9958 - val_loss: 4.0543 - val_acc: 0.5800\n",
      "Epoch 72/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 3.6223e-04 - acc: 1.0000 - val_loss: 4.1910 - val_acc: 0.5600\n",
      "Epoch 73/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.0049 - acc: 0.9958 - val_loss: 4.2646 - val_acc: 0.6200\n",
      "Epoch 74/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 4.3894 - val_acc: 0.6200\n",
      "Epoch 75/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 9.4258e-04 - acc: 1.0000 - val_loss: 4.4322 - val_acc: 0.6200\n",
      "Epoch 76/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 0.0021 - acc: 1.0000 - val_loss: 4.3586 - val_acc: 0.6000\n",
      "Epoch 77/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 0.0028 - acc: 1.0000 - val_loss: 4.1400 - val_acc: 0.6000\n",
      "Epoch 78/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 0.0163 - acc: 0.9958 - val_loss: 3.8795 - val_acc: 0.5600\n",
      "Epoch 79/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.0028 - acc: 1.0000 - val_loss: 4.0693 - val_acc: 0.5000\n",
      "Epoch 80/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 0.0060 - acc: 1.0000 - val_loss: 4.0149 - val_acc: 0.5200\n",
      "Epoch 81/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 3.9492 - val_acc: 0.5600\n",
      "Epoch 82/100\n",
      "238/238 [==============================] - 5s 23ms/step - loss: 7.5629e-04 - acc: 1.0000 - val_loss: 3.9545 - val_acc: 0.5800\n",
      "Epoch 83/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 9.6482e-04 - acc: 1.0000 - val_loss: 3.9514 - val_acc: 0.5800\n",
      "Epoch 84/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 5.2556e-04 - acc: 1.0000 - val_loss: 3.9354 - val_acc: 0.5800\n",
      "Epoch 85/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 2.0821e-04 - acc: 1.0000 - val_loss: 3.9293 - val_acc: 0.5800\n",
      "Epoch 86/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 9.0839e-05 - acc: 1.0000 - val_loss: 3.9315 - val_acc: 0.5800\n",
      "Epoch 87/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 6.0721e-04 - acc: 1.0000 - val_loss: 3.9671 - val_acc: 0.5800\n",
      "Epoch 88/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 1.6734e-04 - acc: 1.0000 - val_loss: 4.0843 - val_acc: 0.5800\n",
      "Epoch 89/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 8.5326e-05 - acc: 1.0000 - val_loss: 4.1657 - val_acc: 0.5600\n",
      "Epoch 90/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 1.6274e-04 - acc: 1.0000 - val_loss: 4.2142 - val_acc: 0.5600\n",
      "Epoch 91/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 4.3083e-04 - acc: 1.0000 - val_loss: 4.2636 - val_acc: 0.5600\n",
      "Epoch 92/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 5.8166e-05 - acc: 1.0000 - val_loss: 4.2933 - val_acc: 0.5600\n",
      "Epoch 93/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 8.2523e-05 - acc: 1.0000 - val_loss: 4.3032 - val_acc: 0.5600\n",
      "Epoch 94/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 9.9330e-05 - acc: 1.0000 - val_loss: 4.3001 - val_acc: 0.5600\n",
      "Epoch 95/100\n",
      "238/238 [==============================] - 5s 22ms/step - loss: 0.0019 - acc: 1.0000 - val_loss: 4.2451 - val_acc: 0.5400\n",
      "Epoch 96/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 1.2853e-04 - acc: 1.0000 - val_loss: 4.2265 - val_acc: 0.5800\n",
      "Epoch 97/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 5.3029e-05 - acc: 1.0000 - val_loss: 4.2568 - val_acc: 0.5800\n",
      "Epoch 98/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 5.0346e-04 - acc: 1.0000 - val_loss: 4.2781 - val_acc: 0.5600\n",
      "Epoch 99/100\n",
      "238/238 [==============================] - 5s 20ms/step - loss: 2.8210e-04 - acc: 1.0000 - val_loss: 4.2854 - val_acc: 0.5600\n",
      "Epoch 100/100\n",
      "238/238 [==============================] - 5s 21ms/step - loss: 8.7373e-05 - acc: 1.0000 - val_loss: 4.2839 - val_acc: 0.5600\n",
      "50/50 [==============================] - 0s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "epochs = 100  \n",
    "batch_size = 64\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 1st cnn layer: 64 filters of size 11*11 with stride 4\n",
    "#                POOL: 3*3 filters with stride 2\n",
    "model.add(Conv1D(filters=64, kernel_size=11, strides=4, padding='valid',activation='relu',kernel_initializer='uniform',input_shape=X_train.shape[1:]))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=192, kernel_size=5, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=384, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Flatten())  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(num_classes,activation='softmax'))  \n",
    "\n",
    "\n",
    "opt = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.0005, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])  \n",
    "model.summary()  \n",
    "\n",
    "time_callback = TimeHistory()\n",
    "acc_callback = AccuracyHistory()\n",
    "model.fit(X_train, y_train, batch_size=batch_size, validation_data=(X_test,y_test),callbacks=[time_callback,acc_callback],epochs=epochs)\n",
    "times = time_callback.times\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b583e2c8d0>]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b502ed8128>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "times[1:]\n",
    "plt.plot(times[1:],acc_callback.testaccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4(3). Other ways to increase validation accuracy--add one deep layer: the highest accuracy reaches 66%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_105 (Conv1D)          (None, 248, 64)           15552     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_52 (MaxPooling (None, 123, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_106 (Conv1D)          (None, 123, 192)          61632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_53 (MaxPooling (None, 61, 192)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 61, 384)           221568    \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 61, 256)           295168    \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 61, 256)           196864    \n",
      "_________________________________________________________________\n",
      "conv1d_110 (Conv1D)          (None, 61, 256)           196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_54 (MaxPooling (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_39 (Flatten)         (None, 7680)              0         \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 4096)              31461376  \n",
      "_________________________________________________________________\n",
      "dropout_65 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_66 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_67 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 4)                 16388     \n",
      "=================================================================\n",
      "Total params: 66,028,036\n",
      "Trainable params: 66,028,036\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 238 samples, validate on 50 samples\n",
      "Epoch 1/100\n",
      "238/238 [==============================] - 9s 39ms/step - loss: 1.4513 - acc: 0.2395 - val_loss: 1.4247 - val_acc: 0.2600\n",
      "Epoch 2/100\n",
      "238/238 [==============================] - 5s 23ms/step - loss: 1.4404 - acc: 0.2269 - val_loss: 1.3811 - val_acc: 0.2600\n",
      "Epoch 3/100\n",
      "238/238 [==============================] - 5s 23ms/step - loss: 1.3991 - acc: 0.2647 - val_loss: 1.3836 - val_acc: 0.2600\n",
      "Epoch 4/100\n",
      "238/238 [==============================] - 6s 23ms/step - loss: 1.4193 - acc: 0.2185 - val_loss: 1.3815 - val_acc: 0.2600\n",
      "Epoch 5/100\n",
      "238/238 [==============================] - 6s 23ms/step - loss: 1.3808 - acc: 0.2857 - val_loss: 1.3768 - val_acc: 0.3000\n",
      "Epoch 6/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 1.3789 - acc: 0.2983 - val_loss: 1.3755 - val_acc: 0.2800\n",
      "Epoch 7/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 1.4050 - acc: 0.2563 - val_loss: 1.3807 - val_acc: 0.3200\n",
      "Epoch 8/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 1.4078 - acc: 0.2647 - val_loss: 1.3810 - val_acc: 0.2600\n",
      "Epoch 9/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 1.3771 - acc: 0.2647 - val_loss: 1.3802 - val_acc: 0.2600\n",
      "Epoch 10/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 1.3676 - acc: 0.2857 - val_loss: 1.3773 - val_acc: 0.2800\n",
      "Epoch 11/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 1.3730 - acc: 0.2983 - val_loss: 1.3709 - val_acc: 0.3400\n",
      "Epoch 12/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 1.3756 - acc: 0.2815 - val_loss: 1.3627 - val_acc: 0.3000\n",
      "Epoch 13/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 1.3321 - acc: 0.3697 - val_loss: 1.3502 - val_acc: 0.3000\n",
      "Epoch 14/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 1.3012 - acc: 0.4286 - val_loss: 1.3372 - val_acc: 0.2200\n",
      "Epoch 15/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 1.3564 - acc: 0.3739 - val_loss: 1.3527 - val_acc: 0.3400\n",
      "Epoch 16/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 1.3129 - acc: 0.3739 - val_loss: 1.3648 - val_acc: 0.3000\n",
      "Epoch 17/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 1.2466 - acc: 0.3950 - val_loss: 1.3373 - val_acc: 0.3200\n",
      "Epoch 18/100\n",
      "238/238 [==============================] - 6s 25ms/step - loss: 1.2634 - acc: 0.4328 - val_loss: 1.3837 - val_acc: 0.3600\n",
      "Epoch 19/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 1.2354 - acc: 0.4664 - val_loss: 1.3180 - val_acc: 0.4000\n",
      "Epoch 20/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 1.2183 - acc: 0.4244 - val_loss: 1.3118 - val_acc: 0.3600\n",
      "Epoch 21/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 1.1871 - acc: 0.4664 - val_loss: 1.2930 - val_acc: 0.3200\n",
      "Epoch 22/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 1.1764 - acc: 0.4706 - val_loss: 1.2594 - val_acc: 0.4400\n",
      "Epoch 23/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 1.1034 - acc: 0.5210 - val_loss: 1.3214 - val_acc: 0.3800\n",
      "Epoch 24/100\n",
      "238/238 [==============================] - 6s 25ms/step - loss: 1.0990 - acc: 0.5084 - val_loss: 1.2281 - val_acc: 0.4400\n",
      "Epoch 25/100\n",
      "238/238 [==============================] - 6s 25ms/step - loss: 1.0201 - acc: 0.5882 - val_loss: 1.3208 - val_acc: 0.4000\n",
      "Epoch 26/100\n",
      "238/238 [==============================] - 6s 25ms/step - loss: 1.1029 - acc: 0.5084 - val_loss: 1.2135 - val_acc: 0.4200\n",
      "Epoch 27/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.9805 - acc: 0.5798 - val_loss: 1.3277 - val_acc: 0.4400\n",
      "Epoch 28/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.8141 - acc: 0.6681 - val_loss: 1.6498 - val_acc: 0.4400\n",
      "Epoch 29/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.8082 - acc: 0.6891 - val_loss: 1.1498 - val_acc: 0.5200\n",
      "Epoch 30/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.6226 - acc: 0.7647 - val_loss: 1.4417 - val_acc: 0.4800\n",
      "Epoch 31/100\n",
      "238/238 [==============================] - 6s 25ms/step - loss: 1.3204 - acc: 0.6303 - val_loss: 1.6090 - val_acc: 0.3400\n",
      "Epoch 32/100\n",
      "238/238 [==============================] - 6s 26ms/step - loss: 1.2815 - acc: 0.4202 - val_loss: 1.3584 - val_acc: 0.3400\n",
      "Epoch 33/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.3263 - acc: 0.3782 - val_loss: 1.2679 - val_acc: 0.3800\n",
      "Epoch 34/100\n",
      "238/238 [==============================] - 6s 26ms/step - loss: 1.2151 - acc: 0.4538 - val_loss: 1.5003 - val_acc: 0.3000\n",
      "Epoch 35/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 1.1961 - acc: 0.4412 - val_loss: 1.2760 - val_acc: 0.4200\n",
      "Epoch 36/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 1.1771 - acc: 0.4706 - val_loss: 1.2274 - val_acc: 0.3800\n",
      "Epoch 37/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 1.0534 - acc: 0.5210 - val_loss: 1.1799 - val_acc: 0.4200\n",
      "Epoch 38/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.9937 - acc: 0.6008 - val_loss: 1.3408 - val_acc: 0.4600\n",
      "Epoch 39/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.9544 - acc: 0.5966 - val_loss: 1.2872 - val_acc: 0.4600\n",
      "Epoch 40/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.7784 - acc: 0.6975 - val_loss: 1.6817 - val_acc: 0.4200\n",
      "Epoch 41/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 1.2228 - acc: 0.5588 - val_loss: 1.2813 - val_acc: 0.4600\n",
      "Epoch 42/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.9494 - acc: 0.6303 - val_loss: 1.2333 - val_acc: 0.4000\n",
      "Epoch 43/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.8536 - acc: 0.6429 - val_loss: 1.1810 - val_acc: 0.4600\n",
      "Epoch 44/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.8000 - acc: 0.6555 - val_loss: 1.1148 - val_acc: 0.6200\n",
      "Epoch 45/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.7503 - acc: 0.7437 - val_loss: 1.1831 - val_acc: 0.5800\n",
      "Epoch 46/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.6894 - acc: 0.7017 - val_loss: 1.1275 - val_acc: 0.5800\n",
      "Epoch 47/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.6015 - acc: 0.8067 - val_loss: 1.6473 - val_acc: 0.5200\n",
      "Epoch 48/100\n",
      "238/238 [==============================] - 6s 26ms/step - loss: 0.6066 - acc: 0.7773 - val_loss: 2.2672 - val_acc: 0.5000\n",
      "Epoch 49/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.6134 - acc: 0.7731 - val_loss: 1.2527 - val_acc: 0.5400\n",
      "Epoch 50/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.4942 - acc: 0.8193 - val_loss: 1.3946 - val_acc: 0.5800\n",
      "Epoch 51/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.4252 - acc: 0.8403 - val_loss: 1.3024 - val_acc: 0.5400\n",
      "Epoch 52/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.3156 - acc: 0.8950 - val_loss: 1.9265 - val_acc: 0.5400\n",
      "Epoch 53/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.2302 - acc: 0.9244 - val_loss: 1.7833 - val_acc: 0.6600\n",
      "Epoch 54/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.2112 - acc: 0.9454 - val_loss: 1.9657 - val_acc: 0.6400\n",
      "Epoch 55/100\n",
      "238/238 [==============================] - 6s 25ms/step - loss: 0.1774 - acc: 0.9244 - val_loss: 2.8590 - val_acc: 0.5400\n",
      "Epoch 56/100\n",
      "238/238 [==============================] - 6s 25ms/step - loss: 0.4054 - acc: 0.8739 - val_loss: 2.2661 - val_acc: 0.4800\n",
      "Epoch 57/100\n",
      "238/238 [==============================] - 6s 25ms/step - loss: 0.4024 - acc: 0.8655 - val_loss: 1.8888 - val_acc: 0.5400\n",
      "Epoch 58/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.2727 - acc: 0.9244 - val_loss: 2.0446 - val_acc: 0.4600\n",
      "Epoch 59/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.1628 - acc: 0.9454 - val_loss: 2.8032 - val_acc: 0.4800\n",
      "Epoch 60/100\n",
      "238/238 [==============================] - 6s 26ms/step - loss: 0.1401 - acc: 0.9496 - val_loss: 2.3179 - val_acc: 0.5400\n",
      "Epoch 61/100\n",
      "238/238 [==============================] - 6s 25ms/step - loss: 0.0568 - acc: 0.9916 - val_loss: 2.4965 - val_acc: 0.5400\n",
      "Epoch 62/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.0558 - acc: 0.9832 - val_loss: 2.5511 - val_acc: 0.5800\n",
      "Epoch 63/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.0318 - acc: 0.9874 - val_loss: 2.4561 - val_acc: 0.6000\n",
      "Epoch 64/100\n",
      "238/238 [==============================] - 6s 25ms/step - loss: 0.0221 - acc: 1.0000 - val_loss: 2.9160 - val_acc: 0.6200\n",
      "Epoch 65/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.0264 - acc: 0.9916 - val_loss: 3.0843 - val_acc: 0.5800\n",
      "Epoch 66/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.0306 - acc: 0.9832 - val_loss: 2.9749 - val_acc: 0.5800\n",
      "Epoch 67/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.0117 - acc: 1.0000 - val_loss: 3.1918 - val_acc: 0.6000\n",
      "Epoch 68/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.0287 - acc: 0.9874 - val_loss: 3.2644 - val_acc: 0.5400\n",
      "Epoch 69/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.0252 - acc: 0.9874 - val_loss: 2.7577 - val_acc: 0.6000\n",
      "Epoch 70/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.0162 - acc: 0.9958 - val_loss: 3.6504 - val_acc: 0.5000\n",
      "Epoch 71/100\n",
      "238/238 [==============================] - 6s 25ms/step - loss: 0.0464 - acc: 0.9790 - val_loss: 3.6133 - val_acc: 0.6200\n",
      "Epoch 72/100\n",
      "238/238 [==============================] - 6s 25ms/step - loss: 0.1352 - acc: 0.9496 - val_loss: 3.1420 - val_acc: 0.5600\n",
      "Epoch 73/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.1555 - acc: 0.9622 - val_loss: 3.0920 - val_acc: 0.4400\n",
      "Epoch 74/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.4441 - acc: 0.8529 - val_loss: 2.7176 - val_acc: 0.5000\n",
      "Epoch 75/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.3728 - acc: 0.8697 - val_loss: 2.1506 - val_acc: 0.4800\n",
      "Epoch 76/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.2726 - acc: 0.9034 - val_loss: 2.0034 - val_acc: 0.5000\n",
      "Epoch 77/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.1549 - acc: 0.9538 - val_loss: 2.6864 - val_acc: 0.5800\n",
      "Epoch 78/100\n",
      "238/238 [==============================] - 6s 25ms/step - loss: 0.1533 - acc: 0.9412 - val_loss: 3.1183 - val_acc: 0.6000\n",
      "Epoch 79/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.1081 - acc: 0.9622 - val_loss: 2.4046 - val_acc: 0.5800\n",
      "Epoch 80/100\n",
      "238/238 [==============================] - 6s 25ms/step - loss: 0.1109 - acc: 0.9580 - val_loss: 2.9347 - val_acc: 0.6000\n",
      "Epoch 81/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.0705 - acc: 0.9748 - val_loss: 3.2917 - val_acc: 0.5800\n",
      "Epoch 82/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.0796 - acc: 0.9748 - val_loss: 3.3304 - val_acc: 0.5800\n",
      "Epoch 83/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.0612 - acc: 0.9664 - val_loss: 2.6890 - val_acc: 0.6400\n",
      "Epoch 84/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.0613 - acc: 0.9748 - val_loss: 4.2735 - val_acc: 0.5400\n",
      "Epoch 85/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.1544 - acc: 0.9454 - val_loss: 3.4596 - val_acc: 0.5000\n",
      "Epoch 86/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.0688 - acc: 0.9832 - val_loss: 4.2213 - val_acc: 0.5200\n",
      "Epoch 87/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.0915 - acc: 0.9748 - val_loss: 3.6610 - val_acc: 0.6200\n",
      "Epoch 88/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.0748 - acc: 0.9874 - val_loss: 4.2287 - val_acc: 0.5400\n",
      "Epoch 89/100\n",
      "238/238 [==============================] - 6s 25ms/step - loss: 0.0763 - acc: 0.9748 - val_loss: 3.8619 - val_acc: 0.5200\n",
      "Epoch 90/100\n",
      "238/238 [==============================] - 6s 25ms/step - loss: 0.0214 - acc: 0.9958 - val_loss: 3.8832 - val_acc: 0.4800\n",
      "Epoch 91/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.1014 - acc: 0.9790 - val_loss: 2.9202 - val_acc: 0.6000\n",
      "Epoch 92/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.0908 - acc: 0.9706 - val_loss: 3.1765 - val_acc: 0.5400\n",
      "Epoch 93/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.0919 - acc: 0.9664 - val_loss: 2.7180 - val_acc: 0.6600\n",
      "Epoch 94/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.1329 - acc: 0.9706 - val_loss: 3.2520 - val_acc: 0.6000\n",
      "Epoch 95/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.0596 - acc: 0.9790 - val_loss: 3.3018 - val_acc: 0.4800\n",
      "Epoch 96/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.0467 - acc: 0.9874 - val_loss: 3.0786 - val_acc: 0.5200\n",
      "Epoch 97/100\n",
      "238/238 [==============================] - 6s 25ms/step - loss: 0.0475 - acc: 0.9790 - val_loss: 3.9274 - val_acc: 0.5800\n",
      "Epoch 98/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.0139 - acc: 0.9916 - val_loss: 4.5011 - val_acc: 0.5600\n",
      "Epoch 99/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 0.0243 - acc: 0.9958 - val_loss: 4.3395 - val_acc: 0.6000\n",
      "Epoch 100/100\n",
      "238/238 [==============================] - 6s 25ms/step - loss: 0.0448 - acc: 0.9874 - val_loss: 4.3404 - val_acc: 0.4800\n",
      "50/50 [==============================] - 0s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "epochs = 100  \n",
    "batch_size = 64\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 1st cnn layer: 64 filters of size 11*11 with stride 4\n",
    "#                POOL: 3*3 filters with stride 2\n",
    "model.add(Conv1D(filters=64, kernel_size=11, strides=4, padding='valid',activation='relu',kernel_initializer='uniform',input_shape=X_train.shape[1:]))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=192, kernel_size=5, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=384, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Flatten())  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(num_classes,activation='softmax'))  \n",
    "\n",
    "\n",
    "opt = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.0005, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])  \n",
    "model.summary()  \n",
    "\n",
    "time_callback = TimeHistory()\n",
    "acc_callback = AccuracyHistory()\n",
    "model.fit(X_train, y_train, batch_size=batch_size, validation_data=(X_test,y_test),callbacks=[time_callback,acc_callback],epochs=epochs)\n",
    "times = time_callback.times\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2b60c490908>]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2b502ed8358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "times[1:]\n",
    "plt.plot(times[1:],acc_callback.testaccuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4(4). Other ways to increase validation accuracy--add two deep layer: the highest accuracy drops "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_55 (Conv1D)           (None, 248, 64)           15552     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_31 (MaxPooling (None, 123, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_56 (Conv1D)           (None, 123, 192)          61632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 61, 192)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_57 (Conv1D)           (None, 61, 384)           221568    \n",
      "_________________________________________________________________\n",
      "conv1d_58 (Conv1D)           (None, 61, 256)           295168    \n",
      "_________________________________________________________________\n",
      "conv1d_59 (Conv1D)           (None, 61, 256)           196864    \n",
      "_________________________________________________________________\n",
      "conv1d_60 (Conv1D)           (None, 61, 256)           196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_17 (Flatten)         (None, 7680)              0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 4096)              31461376  \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 4)                 16388     \n",
      "=================================================================\n",
      "Total params: 82,809,348\n",
      "Trainable params: 82,809,348\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 238 samples, validate on 50 samples\n",
      "Epoch 1/100\n",
      "238/238 [==============================] - 11s 44ms/step - loss: 1.4425 - acc: 0.2899 - val_loss: 1.3913 - val_acc: 0.2600\n",
      "Epoch 2/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.4106 - acc: 0.2479 - val_loss: 1.3857 - val_acc: 0.2600\n",
      "Epoch 3/100\n",
      "238/238 [==============================] - 8s 34ms/step - loss: 1.4111 - acc: 0.2857 - val_loss: 1.3850 - val_acc: 0.2400\n",
      "Epoch 4/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 1.4201 - acc: 0.2437 - val_loss: 1.3881 - val_acc: 0.2600\n",
      "Epoch 5/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 1.4247 - acc: 0.2731 - val_loss: 1.3881 - val_acc: 0.2400\n",
      "Epoch 6/100\n",
      "238/238 [==============================] - 9s 36ms/step - loss: 1.4139 - acc: 0.2227 - val_loss: 1.3819 - val_acc: 0.2600\n",
      "Epoch 7/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.3756 - acc: 0.2815 - val_loss: 1.3870 - val_acc: 0.2400\n",
      "Epoch 8/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.4065 - acc: 0.2731 - val_loss: 1.3837 - val_acc: 0.2400\n",
      "Epoch 9/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.3924 - acc: 0.2395 - val_loss: 1.3834 - val_acc: 0.2400\n",
      "Epoch 10/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.3957 - acc: 0.2227 - val_loss: 1.3847 - val_acc: 0.3000\n",
      "Epoch 11/100\n",
      "238/238 [==============================] - 9s 37ms/step - loss: 1.3983 - acc: 0.2101 - val_loss: 1.3864 - val_acc: 0.2600\n",
      "Epoch 12/100\n",
      "238/238 [==============================] - 9s 40ms/step - loss: 1.3990 - acc: 0.2101 - val_loss: 1.3860 - val_acc: 0.3000\n",
      "Epoch 13/100\n",
      "238/238 [==============================] - 8s 35ms/step - loss: 1.3873 - acc: 0.2773 - val_loss: 1.3842 - val_acc: 0.3000\n",
      "Epoch 14/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 1.3822 - acc: 0.2731 - val_loss: 1.3837 - val_acc: 0.3200\n",
      "Epoch 15/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.3846 - acc: 0.2395 - val_loss: 1.3832 - val_acc: 0.3600\n",
      "Epoch 16/100\n",
      "238/238 [==============================] - 8s 35ms/step - loss: 1.3870 - acc: 0.2185 - val_loss: 1.3826 - val_acc: 0.2800\n",
      "Epoch 17/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.3797 - acc: 0.2815 - val_loss: 1.3818 - val_acc: 0.2800\n",
      "Epoch 18/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.3740 - acc: 0.2563 - val_loss: 1.3806 - val_acc: 0.3600\n",
      "Epoch 19/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 1.3812 - acc: 0.2815 - val_loss: 1.3779 - val_acc: 0.3000\n",
      "Epoch 20/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 1.3774 - acc: 0.2773 - val_loss: 1.3754 - val_acc: 0.3200\n",
      "Epoch 21/100\n",
      "238/238 [==============================] - 8s 34ms/step - loss: 1.3837 - acc: 0.2857 - val_loss: 1.3746 - val_acc: 0.3000\n",
      "Epoch 22/100\n",
      "238/238 [==============================] - 8s 34ms/step - loss: 1.3641 - acc: 0.3655 - val_loss: 1.3743 - val_acc: 0.3400\n",
      "Epoch 23/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 1.3639 - acc: 0.3529 - val_loss: 1.3695 - val_acc: 0.3400\n",
      "Epoch 24/100\n",
      "238/238 [==============================] - 8s 35ms/step - loss: 1.3783 - acc: 0.2647 - val_loss: 1.3643 - val_acc: 0.3400\n",
      "Epoch 25/100\n",
      "238/238 [==============================] - 8s 34ms/step - loss: 1.3758 - acc: 0.2731 - val_loss: 1.3626 - val_acc: 0.2800\n",
      "Epoch 26/100\n",
      "238/238 [==============================] - 8s 34ms/step - loss: 1.3696 - acc: 0.2941 - val_loss: 1.3687 - val_acc: 0.2200\n",
      "Epoch 27/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.3278 - acc: 0.3739 - val_loss: 1.3434 - val_acc: 0.3800\n",
      "Epoch 28/100\n",
      "238/238 [==============================] - 8s 35ms/step - loss: 1.3197 - acc: 0.3739 - val_loss: 1.3260 - val_acc: 0.3000\n",
      "Epoch 29/100\n",
      "238/238 [==============================] - 9s 37ms/step - loss: 1.3201 - acc: 0.3403 - val_loss: 1.3228 - val_acc: 0.3000\n",
      "Epoch 30/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.2999 - acc: 0.3992 - val_loss: 1.3462 - val_acc: 0.3400\n",
      "Epoch 31/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 1.3103 - acc: 0.3739 - val_loss: 1.3466 - val_acc: 0.3400\n",
      "Epoch 32/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.2657 - acc: 0.4118 - val_loss: 1.3354 - val_acc: 0.3200\n",
      "Epoch 33/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.2652 - acc: 0.4328 - val_loss: 1.3305 - val_acc: 0.2800\n",
      "Epoch 34/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.2613 - acc: 0.4034 - val_loss: 1.3035 - val_acc: 0.3800\n",
      "Epoch 35/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.2451 - acc: 0.4370 - val_loss: 1.3454 - val_acc: 0.3000\n",
      "Epoch 36/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.2162 - acc: 0.4580 - val_loss: 1.3065 - val_acc: 0.3800\n",
      "Epoch 37/100\n",
      "238/238 [==============================] - 9s 39ms/step - loss: 1.2084 - acc: 0.4244 - val_loss: 1.3316 - val_acc: 0.2600\n",
      "Epoch 38/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 1.2371 - acc: 0.4370 - val_loss: 1.1845 - val_acc: 0.4200\n",
      "Epoch 39/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.1529 - acc: 0.4496 - val_loss: 1.2868 - val_acc: 0.3800\n",
      "Epoch 40/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.1256 - acc: 0.4580 - val_loss: 1.1320 - val_acc: 0.4200\n",
      "Epoch 41/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.0479 - acc: 0.5504 - val_loss: 0.9671 - val_acc: 0.5800\n",
      "Epoch 42/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.9675 - acc: 0.5798 - val_loss: 1.1361 - val_acc: 0.4200\n",
      "Epoch 43/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.9492 - acc: 0.6176 - val_loss: 1.0803 - val_acc: 0.4000\n",
      "Epoch 44/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.9873 - acc: 0.5840 - val_loss: 1.5006 - val_acc: 0.4600\n",
      "Epoch 45/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.9535 - acc: 0.5966 - val_loss: 1.2155 - val_acc: 0.4400\n",
      "Epoch 46/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.5255 - acc: 0.3571 - val_loss: 1.5512 - val_acc: 0.3200\n",
      "Epoch 47/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.3063 - acc: 0.4034 - val_loss: 1.3972 - val_acc: 0.3000\n",
      "Epoch 48/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.2416 - acc: 0.4286 - val_loss: 1.5725 - val_acc: 0.3400\n",
      "Epoch 49/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.2181 - acc: 0.4706 - val_loss: 1.3370 - val_acc: 0.2800\n",
      "Epoch 50/100\n",
      "238/238 [==============================] - 8s 34ms/step - loss: 1.1903 - acc: 0.5042 - val_loss: 1.5072 - val_acc: 0.3800\n",
      "Epoch 51/100\n",
      "238/238 [==============================] - 9s 37ms/step - loss: 1.1499 - acc: 0.4664 - val_loss: 1.3034 - val_acc: 0.4000\n",
      "Epoch 52/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 1.1339 - acc: 0.5336 - val_loss: 1.5573 - val_acc: 0.3200\n",
      "Epoch 53/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 1.1476 - acc: 0.5420 - val_loss: 1.1926 - val_acc: 0.4800\n",
      "Epoch 54/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.0918 - acc: 0.5168 - val_loss: 1.4642 - val_acc: 0.3200\n",
      "Epoch 55/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.9874 - acc: 0.5798 - val_loss: 1.1998 - val_acc: 0.4600\n",
      "Epoch 56/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.9970 - acc: 0.5882 - val_loss: 1.3764 - val_acc: 0.3400\n",
      "Epoch 57/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.9133 - acc: 0.5966 - val_loss: 1.1338 - val_acc: 0.5200\n",
      "Epoch 58/100\n",
      "238/238 [==============================] - 8s 34ms/step - loss: 0.8240 - acc: 0.6513 - val_loss: 1.2885 - val_acc: 0.4600\n",
      "Epoch 59/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.8052 - acc: 0.6471 - val_loss: 1.4943 - val_acc: 0.5200\n",
      "Epoch 60/100\n",
      "238/238 [==============================] - 8s 34ms/step - loss: 0.6732 - acc: 0.7101 - val_loss: 1.4019 - val_acc: 0.4200\n",
      "Epoch 61/100\n",
      "238/238 [==============================] - 8s 34ms/step - loss: 1.0959 - acc: 0.5252 - val_loss: 2.2865 - val_acc: 0.4200\n",
      "Epoch 62/100\n",
      "238/238 [==============================] - 9s 36ms/step - loss: 1.0099 - acc: 0.5924 - val_loss: 1.1853 - val_acc: 0.4600\n",
      "Epoch 63/100\n",
      "238/238 [==============================] - 8s 34ms/step - loss: 0.7923 - acc: 0.6681 - val_loss: 1.1887 - val_acc: 0.5000\n",
      "Epoch 64/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.7196 - acc: 0.7353 - val_loss: 1.3363 - val_acc: 0.4800\n",
      "Epoch 65/100\n",
      "238/238 [==============================] - 8s 34ms/step - loss: 0.6123 - acc: 0.7311 - val_loss: 1.1762 - val_acc: 0.5200\n",
      "Epoch 66/100\n",
      "238/238 [==============================] - 8s 34ms/step - loss: 0.5548 - acc: 0.7689 - val_loss: 1.3499 - val_acc: 0.5200\n",
      "Epoch 67/100\n",
      "238/238 [==============================] - 8s 34ms/step - loss: 0.4082 - acc: 0.8361 - val_loss: 1.3273 - val_acc: 0.5400\n",
      "Epoch 68/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.3822 - acc: 0.8487 - val_loss: 1.8822 - val_acc: 0.5400\n",
      "Epoch 69/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.2949 - acc: 0.9118 - val_loss: 1.9677 - val_acc: 0.6200\n",
      "Epoch 70/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.2821 - acc: 0.8824 - val_loss: 1.8523 - val_acc: 0.4600\n",
      "Epoch 71/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.2389 - acc: 0.9118 - val_loss: 1.6699 - val_acc: 0.6000\n",
      "Epoch 72/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.1355 - acc: 0.9370 - val_loss: 2.7272 - val_acc: 0.5400\n",
      "Epoch 73/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.2406 - acc: 0.9202 - val_loss: 2.0509 - val_acc: 0.5600\n",
      "Epoch 74/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.2365 - acc: 0.9160 - val_loss: 2.4950 - val_acc: 0.5400\n",
      "Epoch 75/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.2812 - acc: 0.8992 - val_loss: 1.7610 - val_acc: 0.5600\n",
      "Epoch 76/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.2262 - acc: 0.9118 - val_loss: 2.6129 - val_acc: 0.4200\n",
      "Epoch 77/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.2311 - acc: 0.9034 - val_loss: 1.9095 - val_acc: 0.5200\n",
      "Epoch 78/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.1578 - acc: 0.9412 - val_loss: 2.0525 - val_acc: 0.4600\n",
      "Epoch 79/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.1190 - acc: 0.9706 - val_loss: 2.2416 - val_acc: 0.4800\n",
      "Epoch 80/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.1303 - acc: 0.9328 - val_loss: 3.2177 - val_acc: 0.5400\n",
      "Epoch 81/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.2366 - acc: 0.9118 - val_loss: 2.1795 - val_acc: 0.4600\n",
      "Epoch 82/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.2438 - acc: 0.9034 - val_loss: 2.3597 - val_acc: 0.5800\n",
      "Epoch 83/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.2673 - acc: 0.9118 - val_loss: 2.5042 - val_acc: 0.5000\n",
      "Epoch 84/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.1606 - acc: 0.9496 - val_loss: 2.2497 - val_acc: 0.4400\n",
      "Epoch 85/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.1981 - acc: 0.9454 - val_loss: 2.3532 - val_acc: 0.5200\n",
      "Epoch 86/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.2319 - acc: 0.9412 - val_loss: 2.6478 - val_acc: 0.5000\n",
      "Epoch 87/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.2279 - acc: 0.9160 - val_loss: 2.4875 - val_acc: 0.4200\n",
      "Epoch 88/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.1387 - acc: 0.9664 - val_loss: 3.3530 - val_acc: 0.4600\n",
      "Epoch 89/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.1729 - acc: 0.9244 - val_loss: 2.8432 - val_acc: 0.5000\n",
      "Epoch 90/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0676 - acc: 0.9706 - val_loss: 2.6536 - val_acc: 0.4000\n",
      "Epoch 91/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0789 - acc: 0.9664 - val_loss: 3.5532 - val_acc: 0.4800\n",
      "Epoch 92/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.0147 - acc: 0.9958 - val_loss: 3.4540 - val_acc: 0.5000\n",
      "Epoch 93/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0662 - acc: 0.9748 - val_loss: 3.4276 - val_acc: 0.5400\n",
      "Epoch 94/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0863 - acc: 0.9706 - val_loss: 3.3758 - val_acc: 0.5000\n",
      "Epoch 95/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.0823 - acc: 0.9664 - val_loss: 2.8967 - val_acc: 0.4800\n",
      "Epoch 96/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.2083 - acc: 0.9370 - val_loss: 5.9287 - val_acc: 0.4400\n",
      "Epoch 97/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.2686 - acc: 0.9118 - val_loss: 2.2227 - val_acc: 0.4400\n",
      "Epoch 98/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.1851 - acc: 0.9328 - val_loss: 3.1091 - val_acc: 0.4400\n",
      "Epoch 99/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0894 - acc: 0.9664 - val_loss: 3.6209 - val_acc: 0.4600\n",
      "Epoch 100/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.2751 - acc: 0.9286 - val_loss: 2.6350 - val_acc: 0.4600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "epochs = 100  \n",
    "batch_size = 64\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 1st cnn layer: 64 filters of size 11*11 with stride 4\n",
    "#                POOL: 3*3 filters with stride 2\n",
    "model.add(Conv1D(filters=64, kernel_size=11, strides=4, padding='valid',activation='relu',kernel_initializer='uniform',input_shape=X_train.shape[1:]))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=192, kernel_size=5, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=384, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Flatten())  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5)) \n",
    "model.add(Dense(num_classes,activation='softmax'))  \n",
    "\n",
    "\n",
    "opt = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.0005, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])  \n",
    "model.summary()  \n",
    "\n",
    "time_callback = TimeHistory()\n",
    "acc_callback = AccuracyHistory()\n",
    "model.fit(X_train, y_train, batch_size=batch_size, validation_data=(X_test,y_test),callbacks=[time_callback,acc_callback],epochs=epochs)\n",
    "times = time_callback.times\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4(5). Add batch normalization to the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_49 (Conv1D)           (None, 248, 64)           15552     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_28 (MaxPooling (None, 123, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_50 (Conv1D)           (None, 123, 192)          61632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_29 (MaxPooling (None, 61, 192)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_51 (Conv1D)           (None, 61, 384)           221568    \n",
      "_________________________________________________________________\n",
      "conv1d_52 (Conv1D)           (None, 61, 256)           295168    \n",
      "_________________________________________________________________\n",
      "conv1d_53 (Conv1D)           (None, 61, 256)           196864    \n",
      "_________________________________________________________________\n",
      "conv1d_54 (Conv1D)           (None, 61, 256)           196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_30 (MaxPooling (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 7680)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 7680)              30720     \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 4096)              31461376  \n",
      "_________________________________________________________________\n",
      "dropout_19 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_20 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 4096)              16384     \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 4)                 16388     \n",
      "=================================================================\n",
      "Total params: 66,091,524\n",
      "Trainable params: 66,059,780\n",
      "Non-trainable params: 31,744\n",
      "_________________________________________________________________\n",
      "Train on 238 samples, validate on 50 samples\n",
      "Epoch 1/100\n",
      "238/238 [==============================] - 8s 36ms/step - loss: 2.0357 - acc: 0.2479 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 2/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 1.8362 - acc: 0.2647 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 3/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 2.0755 - acc: 0.2563 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 4/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 2.0169 - acc: 0.3739 - val_loss: 10.9114 - val_acc: 0.2600\n",
      "Epoch 5/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.9192 - acc: 0.4454 - val_loss: 11.9274 - val_acc: 0.2600\n",
      "Epoch 6/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 2.0198 - acc: 0.4580 - val_loss: 11.8381 - val_acc: 0.2400\n",
      "Epoch 7/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 2.2859 - acc: 0.3992 - val_loss: 11.5025 - val_acc: 0.2400\n",
      "Epoch 8/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.9288 - acc: 0.4790 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 9/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 2.1891 - acc: 0.4664 - val_loss: 11.4946 - val_acc: 0.2400\n",
      "Epoch 10/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 2.1870 - acc: 0.5000 - val_loss: 10.3354 - val_acc: 0.2000\n",
      "Epoch 11/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 2.0765 - acc: 0.4832 - val_loss: 11.4042 - val_acc: 0.2800\n",
      "Epoch 12/100\n",
      "238/238 [==============================] - 7s 27ms/step - loss: 2.3132 - acc: 0.4874 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 13/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 2.2228 - acc: 0.5420 - val_loss: 9.8873 - val_acc: 0.1600\n",
      "Epoch 14/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 2.5632 - acc: 0.5168 - val_loss: 10.7553 - val_acc: 0.2200\n",
      "Epoch 15/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 2.8281 - acc: 0.5336 - val_loss: 8.0933 - val_acc: 0.3200\n",
      "Epoch 16/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 2.9145 - acc: 0.5420 - val_loss: 10.9289 - val_acc: 0.2200\n",
      "Epoch 17/100\n",
      "238/238 [==============================] - 7s 27ms/step - loss: 2.8235 - acc: 0.5798 - val_loss: 11.6895 - val_acc: 0.2600\n",
      "Epoch 18/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 3.1739 - acc: 0.5294 - val_loss: 9.7889 - val_acc: 0.3400\n",
      "Epoch 19/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 2.7519 - acc: 0.5924 - val_loss: 9.6783 - val_acc: 0.2800\n",
      "Epoch 20/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 3.3820 - acc: 0.5546 - val_loss: 9.7967 - val_acc: 0.2400\n",
      "Epoch 21/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 2.3795 - acc: 0.6345 - val_loss: 9.8139 - val_acc: 0.2000\n",
      "Epoch 22/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 2.4583 - acc: 0.6176 - val_loss: 7.7300 - val_acc: 0.2800\n",
      "Epoch 23/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 2.5106 - acc: 0.6555 - val_loss: 13.5392 - val_acc: 0.1600\n",
      "Epoch 24/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 2.3499 - acc: 0.6345 - val_loss: 7.9320 - val_acc: 0.2800\n",
      "Epoch 25/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 2.3325 - acc: 0.6345 - val_loss: 7.7786 - val_acc: 0.3600\n",
      "Epoch 26/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 2.3746 - acc: 0.6975 - val_loss: 8.9852 - val_acc: 0.3000\n",
      "Epoch 27/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 2.1777 - acc: 0.6807 - val_loss: 9.0809 - val_acc: 0.2800\n",
      "Epoch 28/100\n",
      "238/238 [==============================] - 7s 27ms/step - loss: 2.2962 - acc: 0.6765 - val_loss: 9.7746 - val_acc: 0.2200\n",
      "Epoch 29/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 2.6547 - acc: 0.6933 - val_loss: 8.1969 - val_acc: 0.3800\n",
      "Epoch 30/100\n",
      "238/238 [==============================] - 7s 27ms/step - loss: 2.4732 - acc: 0.6723 - val_loss: 9.2129 - val_acc: 0.2400\n",
      "Epoch 31/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 2.5179 - acc: 0.6975 - val_loss: 9.6211 - val_acc: 0.2400\n",
      "Epoch 32/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 2.7265 - acc: 0.6681 - val_loss: 8.8451 - val_acc: 0.3000\n",
      "Epoch 33/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 2.6684 - acc: 0.6849 - val_loss: 9.9381 - val_acc: 0.2400\n",
      "Epoch 34/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 2.6625 - acc: 0.7017 - val_loss: 9.4636 - val_acc: 0.3000\n",
      "Epoch 35/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 2.4097 - acc: 0.7353 - val_loss: 9.3870 - val_acc: 0.2400\n",
      "Epoch 36/100\n",
      "238/238 [==============================] - 7s 27ms/step - loss: 2.5244 - acc: 0.7059 - val_loss: 8.0966 - val_acc: 0.3400\n",
      "Epoch 37/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 2.6528 - acc: 0.7017 - val_loss: 8.7086 - val_acc: 0.3200\n",
      "Epoch 38/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 2.2712 - acc: 0.7311 - val_loss: 10.2948 - val_acc: 0.3000\n",
      "Epoch 39/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 2.7411 - acc: 0.7017 - val_loss: 9.1219 - val_acc: 0.3600\n",
      "Epoch 40/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 2.7471 - acc: 0.7269 - val_loss: 9.1016 - val_acc: 0.3600\n",
      "Epoch 41/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 2.2141 - acc: 0.7395 - val_loss: 9.7707 - val_acc: 0.3000\n",
      "Epoch 42/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 2.1962 - acc: 0.7563 - val_loss: 10.0669 - val_acc: 0.2600\n",
      "Epoch 43/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 3.4906 - acc: 0.6807 - val_loss: 10.1278 - val_acc: 0.2600\n",
      "Epoch 44/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 2.8573 - acc: 0.7185 - val_loss: 10.0339 - val_acc: 0.2800\n",
      "Epoch 45/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 2.5724 - acc: 0.7353 - val_loss: 9.9584 - val_acc: 0.2800\n",
      "Epoch 46/100\n",
      "238/238 [==============================] - 7s 27ms/step - loss: 3.2454 - acc: 0.6723 - val_loss: 10.5694 - val_acc: 0.2800\n",
      "Epoch 47/100\n",
      "238/238 [==============================] - 7s 27ms/step - loss: 2.5945 - acc: 0.7521 - val_loss: 9.3728 - val_acc: 0.3400\n",
      "Epoch 48/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 2.5684 - acc: 0.7731 - val_loss: 9.4500 - val_acc: 0.3000\n",
      "Epoch 49/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 2.0432 - acc: 0.7731 - val_loss: 10.1837 - val_acc: 0.3000\n",
      "Epoch 50/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 2.3986 - acc: 0.7605 - val_loss: 11.5869 - val_acc: 0.2400\n",
      "Epoch 51/100\n",
      "238/238 [==============================] - 7s 27ms/step - loss: 2.1568 - acc: 0.8235 - val_loss: 11.0117 - val_acc: 0.2400\n",
      "Epoch 52/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 2.2671 - acc: 0.7899 - val_loss: 11.2527 - val_acc: 0.2600\n",
      "Epoch 53/100\n",
      "238/238 [==============================] - 7s 27ms/step - loss: 2.3305 - acc: 0.7983 - val_loss: 11.2451 - val_acc: 0.1600\n",
      "Epoch 54/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 1.9149 - acc: 0.8277 - val_loss: 11.3299 - val_acc: 0.1800\n",
      "Epoch 55/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 2.1204 - acc: 0.8151 - val_loss: 10.8314 - val_acc: 0.2400\n",
      "Epoch 56/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 1.9278 - acc: 0.8403 - val_loss: 10.6554 - val_acc: 0.2800\n",
      "Epoch 57/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 2.4002 - acc: 0.7983 - val_loss: 10.7983 - val_acc: 0.2600\n",
      "Epoch 58/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 2.3991 - acc: 0.7563 - val_loss: 12.0787 - val_acc: 0.2000\n",
      "Epoch 59/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 2.3182 - acc: 0.7983 - val_loss: 12.4947 - val_acc: 0.1800\n",
      "Epoch 60/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 2.6164 - acc: 0.7605 - val_loss: 11.7068 - val_acc: 0.2200\n",
      "Epoch 61/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 2.2417 - acc: 0.7941 - val_loss: 11.1044 - val_acc: 0.2600\n",
      "Epoch 62/100\n",
      "238/238 [==============================] - 7s 27ms/step - loss: 1.8346 - acc: 0.8025 - val_loss: 10.4892 - val_acc: 0.2800\n",
      "Epoch 63/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.4681 - acc: 0.8529 - val_loss: 10.7365 - val_acc: 0.2400\n",
      "Epoch 64/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 1.9767 - acc: 0.8235 - val_loss: 10.8560 - val_acc: 0.2200\n",
      "Epoch 65/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 1.7627 - acc: 0.8529 - val_loss: 10.4955 - val_acc: 0.2600\n",
      "Epoch 66/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.5154 - acc: 0.8613 - val_loss: 10.9856 - val_acc: 0.2200\n",
      "Epoch 67/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 2.0615 - acc: 0.8361 - val_loss: 10.6131 - val_acc: 0.2800\n",
      "Epoch 68/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 1.9333 - acc: 0.8319 - val_loss: 11.3298 - val_acc: 0.2400\n",
      "Epoch 69/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 1.7925 - acc: 0.8487 - val_loss: 10.9021 - val_acc: 0.2400\n",
      "Epoch 70/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 1.6893 - acc: 0.8529 - val_loss: 10.7168 - val_acc: 0.2400\n",
      "Epoch 71/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.7572 - acc: 0.8571 - val_loss: 10.8656 - val_acc: 0.3000\n",
      "Epoch 72/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.9195 - acc: 0.8445 - val_loss: 9.7268 - val_acc: 0.2800\n",
      "Epoch 73/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.2589 - acc: 0.8824 - val_loss: 9.8188 - val_acc: 0.1800\n",
      "Epoch 74/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.8536 - acc: 0.8361 - val_loss: 10.1460 - val_acc: 0.2600\n",
      "Epoch 75/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.4564 - acc: 0.8529 - val_loss: 10.1684 - val_acc: 0.3000\n",
      "Epoch 76/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.7301 - acc: 0.8361 - val_loss: 10.2172 - val_acc: 0.3000\n",
      "Epoch 77/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.5749 - acc: 0.8613 - val_loss: 10.8728 - val_acc: 0.2400\n",
      "Epoch 78/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.8654 - acc: 0.8445 - val_loss: 11.8720 - val_acc: 0.2000\n",
      "Epoch 79/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 2.2892 - acc: 0.8151 - val_loss: 12.2627 - val_acc: 0.1800\n",
      "Epoch 80/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.9429 - acc: 0.8235 - val_loss: 11.6460 - val_acc: 0.2200\n",
      "Epoch 81/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.9060 - acc: 0.8571 - val_loss: 11.7675 - val_acc: 0.2000\n",
      "Epoch 82/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.4717 - acc: 0.8655 - val_loss: 11.1499 - val_acc: 0.2400\n",
      "Epoch 83/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.5893 - acc: 0.8655 - val_loss: 11.0191 - val_acc: 0.2400\n",
      "Epoch 84/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.6522 - acc: 0.8697 - val_loss: 11.9388 - val_acc: 0.2000\n",
      "Epoch 85/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.4177 - acc: 0.8697 - val_loss: 11.8063 - val_acc: 0.2400\n",
      "Epoch 86/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.4895 - acc: 0.8655 - val_loss: 11.7672 - val_acc: 0.2000\n",
      "Epoch 87/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.4481 - acc: 0.8908 - val_loss: 12.1216 - val_acc: 0.1800\n",
      "Epoch 88/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.3288 - acc: 0.8782 - val_loss: 12.4429 - val_acc: 0.1800\n",
      "Epoch 89/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.2186 - acc: 0.9034 - val_loss: 11.9490 - val_acc: 0.2000\n",
      "Epoch 90/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.5150 - acc: 0.8908 - val_loss: 12.1477 - val_acc: 0.1800\n",
      "Epoch 91/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.4934 - acc: 0.8571 - val_loss: 11.6140 - val_acc: 0.2400\n",
      "Epoch 92/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.0997 - acc: 0.9034 - val_loss: 11.8533 - val_acc: 0.2200\n",
      "Epoch 93/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.4445 - acc: 0.8782 - val_loss: 12.1114 - val_acc: 0.2200\n",
      "Epoch 94/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.1799 - acc: 0.9118 - val_loss: 12.1982 - val_acc: 0.2200\n",
      "Epoch 95/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.4469 - acc: 0.8866 - val_loss: 12.6814 - val_acc: 0.1800\n",
      "Epoch 96/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.2352 - acc: 0.8866 - val_loss: 12.1549 - val_acc: 0.1600\n",
      "Epoch 97/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.5669 - acc: 0.8782 - val_loss: 12.4403 - val_acc: 0.1400\n",
      "Epoch 98/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.9077 - acc: 0.8277 - val_loss: 12.2390 - val_acc: 0.2000\n",
      "Epoch 99/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238/238 [==============================] - 7s 30ms/step - loss: 1.4916 - acc: 0.8824 - val_loss: 11.9417 - val_acc: 0.2200\n",
      "Epoch 100/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.4547 - acc: 0.8866 - val_loss: 11.0280 - val_acc: 0.2400\n",
      "50/50 [==============================] - 0s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "epochs = 100  \n",
    "batch_size = 64\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 1st cnn layer: 64 filters of size 11*11 with stride 4\n",
    "#                POOL: 3*3 filters with stride 2\n",
    "model.add(Conv1D(filters=64, kernel_size=11, strides=4, padding='valid',activation='relu',kernel_initializer='uniform',input_shape=X_train.shape[1:]))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=192, kernel_size=5, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=384, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Flatten())  \n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5)) \n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(num_classes,activation='softmax'))  \n",
    "\n",
    "\n",
    "opt = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.0005, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])  \n",
    "model.summary()  \n",
    "\n",
    "time_callback = TimeHistory()\n",
    "acc_callback = AccuracyHistory()\n",
    "model.fit(X_train, y_train, batch_size=batch_size, validation_data=(X_test,y_test),callbacks=[time_callback,acc_callback],epochs=epochs)\n",
    "times = time_callback.times\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subject 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(288, 25, 1000)\n",
      "(288,)\n",
      "(288, 22, 1000)\n",
      "(288, 1000, 22)\n",
      "(288, 1)\n"
     ]
    }
   ],
   "source": [
    "A01T = h5py.File('A02T_slice.mat', 'r') \n",
    "X = np.copy(A01T['image']) \n",
    "y = np.copy(A01T['type']) \n",
    "y = y[0,0:X.shape[0]:1] \n",
    "y = np.asarray(y, dtype=np.int32)\n",
    "\n",
    "# 288 trials per session\n",
    "# 25 channels (the Ô¨Årst 22 are EEG and the last 3 are EOG signals)\n",
    "# The data is sampled at 250Hz, and so 1000 corresponds to 4s of data for each trial.\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# get rid of the EOG signal\n",
    "X = X[:,:22,:]\n",
    "print(X.shape)\n",
    "\n",
    "# get rif of NaN\n",
    "y = np.asarray(y, dtype=np.float32)-769\n",
    "y = y[:,None]\n",
    "X = X.transpose(0,2,1)\n",
    "where_are_NaNs = np.isnan(X)\n",
    "X[where_are_NaNs] = 0\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (238, 1000, 22) \n",
      "y_train: (238, 1) \n",
      "X_test: (50, 1000, 22) \n",
      "y_test: (50, 1) \n"
     ]
    }
   ],
   "source": [
    "# get train, validation, test data\n",
    "data = get_train_test(X,y)\n",
    "for k in data.keys():\n",
    "    print('{}: {} '.format(k, data[k].shape))\n",
    "# X[n_trials, n_steps, n_inputs]\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 4 classes  \n",
    "num_classes = 4  \n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "#y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(238, 1000, 22, 1)\n",
      "(50, 1000, 22, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train1 = np.expand_dims(X_train,-1)\n",
    "#X_val1 = np.expand_dims(X_val,-1)\n",
    "X_test1 = np.expand_dims(X_test,-1)\n",
    "\n",
    "print(X_train1.shape)\n",
    "#print(X_train1.shape[1:])\n",
    "print(X_test1.shape)\n",
    "#print(X_val1.shape[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_61 (Conv1D)           (None, 248, 64)           15552     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 123, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_62 (Conv1D)           (None, 123, 192)          61632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 61, 192)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_63 (Conv1D)           (None, 61, 384)           221568    \n",
      "_________________________________________________________________\n",
      "conv1d_64 (Conv1D)           (None, 61, 256)           295168    \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 61, 256)           196864    \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 61, 256)           196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_36 (MaxPooling (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 7680)              0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 4096)              31461376  \n",
      "_________________________________________________________________\n",
      "dropout_26 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_27 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 4)                 16388     \n",
      "=================================================================\n",
      "Total params: 66,028,036\n",
      "Trainable params: 66,028,036\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 238 samples, validate on 50 samples\n",
      "Epoch 1/100\n",
      "238/238 [==============================] - 8s 35ms/step - loss: 1.4347 - acc: 0.2311 - val_loss: 1.3984 - val_acc: 0.2400\n",
      "Epoch 2/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.4101 - acc: 0.2815 - val_loss: 1.3905 - val_acc: 0.2400\n",
      "Epoch 3/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.4278 - acc: 0.2311 - val_loss: 1.3873 - val_acc: 0.2600\n",
      "Epoch 4/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.4068 - acc: 0.2437 - val_loss: 1.3853 - val_acc: 0.3200\n",
      "Epoch 5/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.4070 - acc: 0.2647 - val_loss: 1.3829 - val_acc: 0.2400\n",
      "Epoch 6/100\n",
      "238/238 [==============================] - 8s 35ms/step - loss: 1.3981 - acc: 0.2185 - val_loss: 1.3855 - val_acc: 0.2800\n",
      "Epoch 7/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.3895 - acc: 0.2521 - val_loss: 1.3822 - val_acc: 0.2800\n",
      "Epoch 8/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.4073 - acc: 0.2353 - val_loss: 1.3809 - val_acc: 0.2600\n",
      "Epoch 9/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.3826 - acc: 0.2815 - val_loss: 1.3851 - val_acc: 0.2800\n",
      "Epoch 10/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.3763 - acc: 0.2773 - val_loss: 1.3873 - val_acc: 0.2600\n",
      "Epoch 11/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.3814 - acc: 0.2941 - val_loss: 1.3841 - val_acc: 0.2600\n",
      "Epoch 12/100\n",
      "238/238 [==============================] - 9s 38ms/step - loss: 1.3777 - acc: 0.2563 - val_loss: 1.3838 - val_acc: 0.2000\n",
      "Epoch 13/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.3824 - acc: 0.3109 - val_loss: 1.3802 - val_acc: 0.2600\n",
      "Epoch 14/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.3663 - acc: 0.3571 - val_loss: 1.3754 - val_acc: 0.2400\n",
      "Epoch 15/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.3481 - acc: 0.3571 - val_loss: 1.3698 - val_acc: 0.2400\n",
      "Epoch 16/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.3585 - acc: 0.2899 - val_loss: 1.3575 - val_acc: 0.2800\n",
      "Epoch 17/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.3031 - acc: 0.4160 - val_loss: 1.3653 - val_acc: 0.2400\n",
      "Epoch 18/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.2707 - acc: 0.4034 - val_loss: 1.3947 - val_acc: 0.2400\n",
      "Epoch 19/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.2267 - acc: 0.4496 - val_loss: 1.4020 - val_acc: 0.3000\n",
      "Epoch 20/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.2367 - acc: 0.4496 - val_loss: 1.3469 - val_acc: 0.2400\n",
      "Epoch 21/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.2156 - acc: 0.4496 - val_loss: 1.3749 - val_acc: 0.3400\n",
      "Epoch 22/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.2009 - acc: 0.4328 - val_loss: 1.3672 - val_acc: 0.2400\n",
      "Epoch 23/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.2157 - acc: 0.4202 - val_loss: 1.3640 - val_acc: 0.3200\n",
      "Epoch 24/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 1.1630 - acc: 0.4958 - val_loss: 1.4461 - val_acc: 0.3000\n",
      "Epoch 25/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 1.1719 - acc: 0.5000 - val_loss: 1.3456 - val_acc: 0.3400\n",
      "Epoch 26/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 1.0592 - acc: 0.5042 - val_loss: 1.3615 - val_acc: 0.2800\n",
      "Epoch 27/100\n",
      "238/238 [==============================] - 6s 26ms/step - loss: 1.0146 - acc: 0.5546 - val_loss: 1.4645 - val_acc: 0.3800\n",
      "Epoch 28/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.9561 - acc: 0.5798 - val_loss: 1.5067 - val_acc: 0.3800\n",
      "Epoch 29/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.8913 - acc: 0.5714 - val_loss: 1.3714 - val_acc: 0.3600\n",
      "Epoch 30/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.8032 - acc: 0.6597 - val_loss: 1.6719 - val_acc: 0.4000\n",
      "Epoch 31/100\n",
      "238/238 [==============================] - 6s 26ms/step - loss: 0.8873 - acc: 0.6303 - val_loss: 1.4817 - val_acc: 0.4000\n",
      "Epoch 32/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.8604 - acc: 0.6008 - val_loss: 1.9065 - val_acc: 0.3400\n",
      "Epoch 33/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.7576 - acc: 0.6723 - val_loss: 1.9374 - val_acc: 0.3400\n",
      "Epoch 34/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.6982 - acc: 0.6933 - val_loss: 1.6918 - val_acc: 0.3200\n",
      "Epoch 35/100\n",
      "238/238 [==============================] - 7s 27ms/step - loss: 1.4264 - acc: 0.4748 - val_loss: 1.5981 - val_acc: 0.3600\n",
      "Epoch 36/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 1.1006 - acc: 0.5084 - val_loss: 1.4344 - val_acc: 0.3000\n",
      "Epoch 37/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.0788 - acc: 0.5294 - val_loss: 1.4466 - val_acc: 0.4000\n",
      "Epoch 38/100\n",
      "238/238 [==============================] - 9s 36ms/step - loss: 0.9506 - acc: 0.5714 - val_loss: 1.3356 - val_acc: 0.3200\n",
      "Epoch 39/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.8831 - acc: 0.5924 - val_loss: 1.7332 - val_acc: 0.4400\n",
      "Epoch 40/100\n",
      "238/238 [==============================] - 9s 37ms/step - loss: 0.8265 - acc: 0.6345 - val_loss: 1.9327 - val_acc: 0.3600\n",
      "Epoch 41/100\n",
      "238/238 [==============================] - 8s 34ms/step - loss: 0.8451 - acc: 0.6807 - val_loss: 1.3309 - val_acc: 0.3600\n",
      "Epoch 42/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.8170 - acc: 0.6471 - val_loss: 1.8462 - val_acc: 0.3600\n",
      "Epoch 43/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.7624 - acc: 0.6597 - val_loss: 2.1612 - val_acc: 0.3600\n",
      "Epoch 44/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.6696 - acc: 0.6975 - val_loss: 1.6750 - val_acc: 0.4400\n",
      "Epoch 45/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.5125 - acc: 0.7941 - val_loss: 2.3141 - val_acc: 0.3000\n",
      "Epoch 46/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.4319 - acc: 0.8109 - val_loss: 3.1267 - val_acc: 0.3400\n",
      "Epoch 47/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.3118 - acc: 0.8824 - val_loss: 2.7604 - val_acc: 0.3800\n",
      "Epoch 48/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.4654 - acc: 0.8277 - val_loss: 4.4419 - val_acc: 0.3800\n",
      "Epoch 49/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.6214 - acc: 0.7731 - val_loss: 2.4615 - val_acc: 0.3600\n",
      "Epoch 50/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.6613 - acc: 0.7353 - val_loss: 2.9188 - val_acc: 0.3400\n",
      "Epoch 51/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.6050 - acc: 0.7563 - val_loss: 2.5375 - val_acc: 0.4400\n",
      "Epoch 52/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.5552 - acc: 0.7731 - val_loss: 3.0178 - val_acc: 0.3600\n",
      "Epoch 53/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.3968 - acc: 0.8529 - val_loss: 3.1226 - val_acc: 0.4200\n",
      "Epoch 54/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.1956 - acc: 0.9244 - val_loss: 3.6036 - val_acc: 0.3600\n",
      "Epoch 55/100\n",
      "238/238 [==============================] - 9s 38ms/step - loss: 0.2074 - acc: 0.9202 - val_loss: 3.8982 - val_acc: 0.4000\n",
      "Epoch 56/100\n",
      "238/238 [==============================] - 8s 35ms/step - loss: 0.1231 - acc: 0.9496 - val_loss: 4.6737 - val_acc: 0.4000\n",
      "Epoch 57/100\n",
      "238/238 [==============================] - 8s 34ms/step - loss: 0.2102 - acc: 0.9412 - val_loss: 4.3470 - val_acc: 0.3800\n",
      "Epoch 58/100\n",
      "238/238 [==============================] - 8s 34ms/step - loss: 0.1855 - acc: 0.9328 - val_loss: 3.4672 - val_acc: 0.4400\n",
      "Epoch 59/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.2161 - acc: 0.9202 - val_loss: 4.7658 - val_acc: 0.3600\n",
      "Epoch 60/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.2762 - acc: 0.8824 - val_loss: 2.2805 - val_acc: 0.4400\n",
      "Epoch 61/100\n",
      "238/238 [==============================] - 7s 27ms/step - loss: 0.4221 - acc: 0.8445 - val_loss: 1.8772 - val_acc: 0.3200\n",
      "Epoch 62/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.1873 - acc: 0.9370 - val_loss: 5.4555 - val_acc: 0.3600\n",
      "Epoch 63/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.2166 - acc: 0.9370 - val_loss: 3.1152 - val_acc: 0.4400\n",
      "Epoch 64/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.1987 - acc: 0.9286 - val_loss: 3.4352 - val_acc: 0.5000\n",
      "Epoch 65/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.1438 - acc: 0.9622 - val_loss: 4.6894 - val_acc: 0.3400\n",
      "Epoch 66/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.2698 - acc: 0.8992 - val_loss: 3.9651 - val_acc: 0.4600\n",
      "Epoch 67/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.2387 - acc: 0.9202 - val_loss: 2.7628 - val_acc: 0.3600\n",
      "Epoch 68/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.1989 - acc: 0.9202 - val_loss: 3.9784 - val_acc: 0.3600\n",
      "Epoch 69/100\n",
      "238/238 [==============================] - 8s 35ms/step - loss: 0.2130 - acc: 0.9370 - val_loss: 3.6998 - val_acc: 0.3400\n",
      "Epoch 70/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.3169 - acc: 0.8866 - val_loss: 4.8941 - val_acc: 0.3800\n",
      "Epoch 71/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.4248 - acc: 0.9034 - val_loss: 3.0102 - val_acc: 0.4200\n",
      "Epoch 72/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.4893 - acc: 0.7101 - val_loss: 2.4984 - val_acc: 0.3400\n",
      "Epoch 73/100\n",
      "238/238 [==============================] - 8s 34ms/step - loss: 1.9162 - acc: 0.3319 - val_loss: 2.4885 - val_acc: 0.3200\n",
      "Epoch 74/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 2.7445 - acc: 0.2731 - val_loss: 1.6046 - val_acc: 0.2400\n",
      "Epoch 75/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 3.2729 - acc: 0.2773 - val_loss: 3.3485 - val_acc: 0.2800\n",
      "Epoch 76/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 3.5188 - acc: 0.2437 - val_loss: 1.5466 - val_acc: 0.2600\n",
      "Epoch 77/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.8331 - acc: 0.2311 - val_loss: 1.3826 - val_acc: 0.2600\n",
      "Epoch 78/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.3920 - acc: 0.2017 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 79/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 80/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 81/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 82/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 83/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 84/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 85/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 86/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 87/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 88/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 89/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 90/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 91/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 92/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 93/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 94/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 95/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 96/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 97/100\n",
      "238/238 [==============================] - 6s 26ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 98/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 99/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 100/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "50/50 [==============================] - 0s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "epochs = 100  \n",
    "batch_size = 64\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 1st cnn layer: 64 filters of size 11*11 with stride 4\n",
    "#                POOL: 3*3 filters with stride 2\n",
    "model.add(Conv1D(filters=64, kernel_size=11, strides=4, padding='valid',activation='relu',kernel_initializer='uniform',input_shape=X_train.shape[1:]))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=192, kernel_size=5, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=384, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Flatten())  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(num_classes,activation='softmax'))  \n",
    "\n",
    "\n",
    "opt = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.0005, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])  \n",
    "model.summary()  \n",
    "\n",
    "time_callback = TimeHistory()\n",
    "acc_callback = AccuracyHistory()\n",
    "model.fit(X_train, y_train, batch_size=batch_size, validation_data=(X_test,y_test),callbacks=[time_callback,acc_callback],epochs=epochs)\n",
    "times = time_callback.times\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subject 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(288, 25, 1000)\n",
      "(288,)\n",
      "(288, 22, 1000)\n",
      "(288, 1000, 22)\n",
      "(288, 1)\n",
      "X_train: (238, 1000, 22) \n",
      "y_train: (238, 1) \n",
      "X_test: (50, 1000, 22) \n",
      "y_test: (50, 1) \n"
     ]
    }
   ],
   "source": [
    "A01T = h5py.File('A03T_slice.mat', 'r') \n",
    "X = np.copy(A01T['image']) \n",
    "y = np.copy(A01T['type']) \n",
    "y = y[0,0:X.shape[0]:1] \n",
    "y = np.asarray(y, dtype=np.int32)\n",
    "\n",
    "# 288 trials per session\n",
    "# 25 channels (the Ô¨Årst 22 are EEG and the last 3 are EOG signals)\n",
    "# The data is sampled at 250Hz, and so 1000 corresponds to 4s of data for each trial.\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# get rid of the EOG signal\n",
    "X = X[:,:22,:]\n",
    "print(X.shape)\n",
    "\n",
    "# get rif of NaN\n",
    "y = np.asarray(y, dtype=np.float32)-769\n",
    "y = y[:,None]\n",
    "X = X.transpose(0,2,1)\n",
    "where_are_NaNs = np.isnan(X)\n",
    "X[where_are_NaNs] = 0\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# get train, validation, test data\n",
    "data = get_train_test(X,y)\n",
    "for k in data.keys():\n",
    "    print('{}: {} '.format(k, data[k].shape))\n",
    "# X[n_trials, n_steps, n_inputs]\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']\n",
    "\n",
    "# There are 4 classes  \n",
    "num_classes = 4  \n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "#y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_67 (Conv1D)           (None, 248, 64)           15552     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_37 (MaxPooling (None, 123, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 123, 192)          61632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_38 (MaxPooling (None, 61, 192)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 61, 384)           221568    \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 61, 256)           295168    \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 61, 256)           196864    \n",
      "_________________________________________________________________\n",
      "conv1d_72 (Conv1D)           (None, 61, 256)           196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_39 (MaxPooling (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_19 (Flatten)         (None, 7680)              0         \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 4096)              31461376  \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_50 (Dense)             (None, 4)                 16388     \n",
      "=================================================================\n",
      "Total params: 66,028,036\n",
      "Trainable params: 66,028,036\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 238 samples, validate on 50 samples\n",
      "Epoch 1/100\n",
      "238/238 [==============================] - 9s 36ms/step - loss: 1.4839 - acc: 0.2773 - val_loss: 1.4771 - val_acc: 0.2600\n",
      "Epoch 2/100\n",
      "238/238 [==============================] - 6s 26ms/step - loss: 1.4314 - acc: 0.2857 - val_loss: 1.4000 - val_acc: 0.2800\n",
      "Epoch 3/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.4238 - acc: 0.2353 - val_loss: 1.3845 - val_acc: 0.2600\n",
      "Epoch 4/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.4384 - acc: 0.2311 - val_loss: 1.3792 - val_acc: 0.2600\n",
      "Epoch 5/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.4040 - acc: 0.2605 - val_loss: 1.3864 - val_acc: 0.2400\n",
      "Epoch 6/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.4078 - acc: 0.2353 - val_loss: 1.3854 - val_acc: 0.2400\n",
      "Epoch 7/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.3872 - acc: 0.2521 - val_loss: 1.3827 - val_acc: 0.2400\n",
      "Epoch 8/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.3950 - acc: 0.2479 - val_loss: 1.3831 - val_acc: 0.2600\n",
      "Epoch 9/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.3954 - acc: 0.2185 - val_loss: 1.3824 - val_acc: 0.2600\n",
      "Epoch 10/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.3738 - acc: 0.2815 - val_loss: 1.3801 - val_acc: 0.2400\n",
      "Epoch 11/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.3847 - acc: 0.2815 - val_loss: 1.3807 - val_acc: 0.2600\n",
      "Epoch 12/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.3989 - acc: 0.2353 - val_loss: 1.3829 - val_acc: 0.4200\n",
      "Epoch 13/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.3899 - acc: 0.2563 - val_loss: 1.3859 - val_acc: 0.2600\n",
      "Epoch 14/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.3784 - acc: 0.2983 - val_loss: 1.3808 - val_acc: 0.2600\n",
      "Epoch 15/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.3775 - acc: 0.2815 - val_loss: 1.3778 - val_acc: 0.2800\n",
      "Epoch 16/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.3593 - acc: 0.3487 - val_loss: 1.3790 - val_acc: 0.2800\n",
      "Epoch 17/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.3652 - acc: 0.2941 - val_loss: 1.3714 - val_acc: 0.2800\n",
      "Epoch 18/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.3480 - acc: 0.3529 - val_loss: 1.3625 - val_acc: 0.3000\n",
      "Epoch 19/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.3253 - acc: 0.3824 - val_loss: 1.3464 - val_acc: 0.3000\n",
      "Epoch 20/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.3205 - acc: 0.3655 - val_loss: 1.3482 - val_acc: 0.3200\n",
      "Epoch 21/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.3088 - acc: 0.4244 - val_loss: 1.3542 - val_acc: 0.2800\n",
      "Epoch 22/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.2758 - acc: 0.3866 - val_loss: 1.3395 - val_acc: 0.2600\n",
      "Epoch 23/100\n",
      "238/238 [==============================] - 7s 27ms/step - loss: 1.2621 - acc: 0.3655 - val_loss: 1.3956 - val_acc: 0.2600\n",
      "Epoch 24/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.2509 - acc: 0.4412 - val_loss: 1.2871 - val_acc: 0.3200\n",
      "Epoch 25/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.1885 - acc: 0.4454 - val_loss: 1.3237 - val_acc: 0.3200\n",
      "Epoch 26/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.1096 - acc: 0.4664 - val_loss: 1.4024 - val_acc: 0.2200\n",
      "Epoch 27/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 1.1973 - acc: 0.4916 - val_loss: 1.3845 - val_acc: 0.1600\n",
      "Epoch 28/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 1.3442 - acc: 0.3193 - val_loss: 1.4288 - val_acc: 0.2600\n",
      "Epoch 29/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 1.3801 - acc: 0.3109 - val_loss: 1.3781 - val_acc: 0.4000\n",
      "Epoch 30/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 1.3104 - acc: 0.3403 - val_loss: 1.3305 - val_acc: 0.3800\n",
      "Epoch 31/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.1985 - acc: 0.4118 - val_loss: 1.2552 - val_acc: 0.3800\n",
      "Epoch 32/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.0324 - acc: 0.5336 - val_loss: 1.3134 - val_acc: 0.3800\n",
      "Epoch 33/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.9962 - acc: 0.5294 - val_loss: 1.4321 - val_acc: 0.3800\n",
      "Epoch 34/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.9177 - acc: 0.5630 - val_loss: 1.2362 - val_acc: 0.4000\n",
      "Epoch 35/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.7827 - acc: 0.6513 - val_loss: 1.2949 - val_acc: 0.3400\n",
      "Epoch 36/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.7128 - acc: 0.6807 - val_loss: 1.4313 - val_acc: 0.3800\n",
      "Epoch 37/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.6854 - acc: 0.7437 - val_loss: 1.3714 - val_acc: 0.3800\n",
      "Epoch 38/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.7819 - acc: 0.7017 - val_loss: 1.9507 - val_acc: 0.4000\n",
      "Epoch 39/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.1502 - acc: 0.5336 - val_loss: 1.5224 - val_acc: 0.3200\n",
      "Epoch 40/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.8461 - acc: 0.6134 - val_loss: 1.6642 - val_acc: 0.3200\n",
      "Epoch 41/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.6825 - acc: 0.7017 - val_loss: 1.4597 - val_acc: 0.3800\n",
      "Epoch 42/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.5746 - acc: 0.7899 - val_loss: 2.2875 - val_acc: 0.3600\n",
      "Epoch 43/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.5232 - acc: 0.7773 - val_loss: 2.6859 - val_acc: 0.2800\n",
      "Epoch 44/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.5005 - acc: 0.8067 - val_loss: 3.7901 - val_acc: 0.3400\n",
      "Epoch 45/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.7305 - acc: 0.7521 - val_loss: 1.6890 - val_acc: 0.2800\n",
      "Epoch 46/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.8173 - acc: 0.6807 - val_loss: 1.3645 - val_acc: 0.3600\n",
      "Epoch 47/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.6441 - acc: 0.7437 - val_loss: 1.8569 - val_acc: 0.3800\n",
      "Epoch 48/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.3308 - acc: 0.8655 - val_loss: 4.7051 - val_acc: 0.4400\n",
      "Epoch 49/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.3329 - acc: 0.8655 - val_loss: 2.6165 - val_acc: 0.4400\n",
      "Epoch 50/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.3059 - acc: 0.8992 - val_loss: 4.4960 - val_acc: 0.4200\n",
      "Epoch 51/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.2818 - acc: 0.9118 - val_loss: 4.7581 - val_acc: 0.3400\n",
      "Epoch 52/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.2713 - acc: 0.8992 - val_loss: 3.9638 - val_acc: 0.4200\n",
      "Epoch 53/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.2811 - acc: 0.8992 - val_loss: 3.5593 - val_acc: 0.4200\n",
      "Epoch 54/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.1372 - acc: 0.9580 - val_loss: 5.2118 - val_acc: 0.3600\n",
      "Epoch 55/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.1612 - acc: 0.9412 - val_loss: 4.0371 - val_acc: 0.3600\n",
      "Epoch 56/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.3528 - acc: 0.8992 - val_loss: 6.3500 - val_acc: 0.3200\n",
      "Epoch 57/100\n",
      "238/238 [==============================] - 7s 27ms/step - loss: 0.3398 - acc: 0.8992 - val_loss: 3.0075 - val_acc: 0.3600\n",
      "Epoch 58/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.2753 - acc: 0.9034 - val_loss: 3.1636 - val_acc: 0.4000\n",
      "Epoch 59/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.2452 - acc: 0.9118 - val_loss: 3.4968 - val_acc: 0.4000\n",
      "Epoch 60/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.1762 - acc: 0.9496 - val_loss: 3.6752 - val_acc: 0.3800\n",
      "Epoch 61/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.1023 - acc: 0.9622 - val_loss: 5.3039 - val_acc: 0.3600\n",
      "Epoch 62/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.0541 - acc: 0.9748 - val_loss: 4.7067 - val_acc: 0.3800\n",
      "Epoch 63/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.0314 - acc: 0.9874 - val_loss: 6.4731 - val_acc: 0.3600\n",
      "Epoch 64/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.0561 - acc: 0.9790 - val_loss: 5.7216 - val_acc: 0.4200\n",
      "Epoch 65/100\n",
      "238/238 [==============================] - 6s 26ms/step - loss: 0.1451 - acc: 0.9538 - val_loss: 3.9005 - val_acc: 0.3400\n",
      "Epoch 66/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 1.0239 - acc: 0.8151 - val_loss: 2.4209 - val_acc: 0.4200\n",
      "Epoch 67/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.3714 - acc: 0.5882 - val_loss: 6.1972 - val_acc: 0.3400\n",
      "Epoch 68/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 4.3373 - acc: 0.4874 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 69/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 70/100\n",
      "238/238 [==============================] - 8s 34ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 71/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 72/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 73/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 74/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 75/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 76/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 77/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 78/100\n",
      "238/238 [==============================] - 6s 26ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 79/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 80/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 81/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 82/100\n",
      "238/238 [==============================] - 6s 26ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 83/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 84/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 85/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 86/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 87/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 88/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 89/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 90/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 91/100\n",
      "238/238 [==============================] - 6s 26ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 92/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 93/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 94/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 95/100\n",
      "238/238 [==============================] - 6s 26ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 96/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 97/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 98/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 99/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "Epoch 100/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 12.0547 - acc: 0.2521 - val_loss: 12.2498 - val_acc: 0.2400\n",
      "50/50 [==============================] - 0s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "epochs = 100  \n",
    "batch_size = 64\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 1st cnn layer: 64 filters of size 11*11 with stride 4\n",
    "#                POOL: 3*3 filters with stride 2\n",
    "model.add(Conv1D(filters=64, kernel_size=11, strides=4, padding='valid',activation='relu',kernel_initializer='uniform',input_shape=X_train.shape[1:]))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=192, kernel_size=5, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=384, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Flatten())  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(num_classes,activation='softmax'))  \n",
    "\n",
    "\n",
    "opt = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.0005, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])  \n",
    "model.summary()  \n",
    "\n",
    "time_callback = TimeHistory()\n",
    "acc_callback = AccuracyHistory()\n",
    "model.fit(X_train, y_train, batch_size=batch_size, validation_data=(X_test,y_test),callbacks=[time_callback,acc_callback],epochs=epochs)\n",
    "times = time_callback.times\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subject 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(288, 25, 1000)\n",
      "(288,)\n",
      "(288, 22, 1000)\n",
      "(288, 1000, 22)\n",
      "(288, 1)\n",
      "X_train: (238, 1000, 22) \n",
      "y_train: (238, 1) \n",
      "X_test: (50, 1000, 22) \n",
      "y_test: (50, 1) \n"
     ]
    }
   ],
   "source": [
    "A01T = h5py.File('A04T_slice.mat', 'r') \n",
    "X = np.copy(A01T['image']) \n",
    "y = np.copy(A01T['type']) \n",
    "y = y[0,0:X.shape[0]:1] \n",
    "y = np.asarray(y, dtype=np.int32)\n",
    "\n",
    "# 288 trials per session\n",
    "# 25 channels (the Ô¨Årst 22 are EEG and the last 3 are EOG signals)\n",
    "# The data is sampled at 250Hz, and so 1000 corresponds to 4s of data for each trial.\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# get rid of the EOG signal\n",
    "X = X[:,:22,:]\n",
    "print(X.shape)\n",
    "\n",
    "# get rif of NaN\n",
    "y = np.asarray(y, dtype=np.float32)-769\n",
    "y = y[:,None]\n",
    "X = X.transpose(0,2,1)\n",
    "where_are_NaNs = np.isnan(X)\n",
    "X[where_are_NaNs] = 0\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# get train, validation, test data\n",
    "data = get_train_test(X,y)\n",
    "for k in data.keys():\n",
    "    print('{}: {} '.format(k, data[k].shape))\n",
    "# X[n_trials, n_steps, n_inputs]\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']\n",
    "\n",
    "# There are 4 classes  \n",
    "num_classes = 4  \n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "#y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_73 (Conv1D)           (None, 248, 64)           15552     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_40 (MaxPooling (None, 123, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_74 (Conv1D)           (None, 123, 192)          61632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_41 (MaxPooling (None, 61, 192)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_75 (Conv1D)           (None, 61, 384)           221568    \n",
      "_________________________________________________________________\n",
      "conv1d_76 (Conv1D)           (None, 61, 256)           295168    \n",
      "_________________________________________________________________\n",
      "conv1d_77 (Conv1D)           (None, 61, 256)           196864    \n",
      "_________________________________________________________________\n",
      "conv1d_78 (Conv1D)           (None, 61, 256)           196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_42 (MaxPooling (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 7680)              0         \n",
      "_________________________________________________________________\n",
      "dense_51 (Dense)             (None, 4096)              31461376  \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 4)                 16388     \n",
      "=================================================================\n",
      "Total params: 66,028,036\n",
      "Trainable params: 66,028,036\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 238 samples, validate on 50 samples\n",
      "Epoch 1/100\n",
      "238/238 [==============================] - 9s 36ms/step - loss: 1.4547 - acc: 0.2269 - val_loss: 1.3925 - val_acc: 0.2600\n",
      "Epoch 2/100\n",
      "238/238 [==============================] - 7s 27ms/step - loss: 1.4510 - acc: 0.2773 - val_loss: 1.3876 - val_acc: 0.2400\n",
      "Epoch 3/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.4189 - acc: 0.2479 - val_loss: 1.3978 - val_acc: 0.2400\n",
      "Epoch 4/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.3923 - acc: 0.2563 - val_loss: 1.3789 - val_acc: 0.4000\n",
      "Epoch 5/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.3908 - acc: 0.2647 - val_loss: 1.3764 - val_acc: 0.3000\n",
      "Epoch 6/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.3879 - acc: 0.2479 - val_loss: 1.3759 - val_acc: 0.3000\n",
      "Epoch 7/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.4060 - acc: 0.2521 - val_loss: 1.3862 - val_acc: 0.2400\n",
      "Epoch 8/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.3758 - acc: 0.2857 - val_loss: 1.3704 - val_acc: 0.2600\n",
      "Epoch 9/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 1.3637 - acc: 0.3235 - val_loss: 1.3631 - val_acc: 0.3400\n",
      "Epoch 10/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.3523 - acc: 0.3319 - val_loss: 1.3480 - val_acc: 0.4000\n",
      "Epoch 11/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.3687 - acc: 0.3067 - val_loss: 1.3238 - val_acc: 0.4000\n",
      "Epoch 12/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.3708 - acc: 0.3151 - val_loss: 1.3199 - val_acc: 0.4200\n",
      "Epoch 13/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.3109 - acc: 0.3571 - val_loss: 1.3298 - val_acc: 0.3000\n",
      "Epoch 14/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.2905 - acc: 0.3571 - val_loss: 1.3115 - val_acc: 0.3600\n",
      "Epoch 15/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.2596 - acc: 0.4244 - val_loss: 1.3067 - val_acc: 0.3000\n",
      "Epoch 16/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.2380 - acc: 0.4454 - val_loss: 1.3077 - val_acc: 0.4000\n",
      "Epoch 17/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.1798 - acc: 0.4538 - val_loss: 1.3331 - val_acc: 0.4400\n",
      "Epoch 18/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.1423 - acc: 0.4958 - val_loss: 1.3260 - val_acc: 0.3600\n",
      "Epoch 19/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 1.1494 - acc: 0.5126 - val_loss: 1.4127 - val_acc: 0.4600\n",
      "Epoch 20/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.1393 - acc: 0.4706 - val_loss: 1.3015 - val_acc: 0.3400\n",
      "Epoch 21/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.0883 - acc: 0.5042 - val_loss: 1.2981 - val_acc: 0.4200\n",
      "Epoch 22/100\n",
      "238/238 [==============================] - 7s 27ms/step - loss: 1.0593 - acc: 0.5672 - val_loss: 1.2948 - val_acc: 0.3800\n",
      "Epoch 23/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 1.0587 - acc: 0.5630 - val_loss: 1.2994 - val_acc: 0.5000\n",
      "Epoch 24/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.9742 - acc: 0.6303 - val_loss: 1.3303 - val_acc: 0.4400\n",
      "Epoch 25/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.9017 - acc: 0.5798 - val_loss: 1.3016 - val_acc: 0.4400\n",
      "Epoch 26/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.9575 - acc: 0.6092 - val_loss: 1.4602 - val_acc: 0.3600\n",
      "Epoch 27/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.9515 - acc: 0.5714 - val_loss: 1.3931 - val_acc: 0.4600\n",
      "Epoch 28/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.8452 - acc: 0.6891 - val_loss: 1.7089 - val_acc: 0.5000\n",
      "Epoch 29/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.7923 - acc: 0.6681 - val_loss: 1.4464 - val_acc: 0.4400\n",
      "Epoch 30/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.7082 - acc: 0.6975 - val_loss: 2.0636 - val_acc: 0.3800\n",
      "Epoch 31/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.7363 - acc: 0.7395 - val_loss: 2.1018 - val_acc: 0.5200\n",
      "Epoch 32/100\n",
      "238/238 [==============================] - 8s 34ms/step - loss: 0.5539 - acc: 0.7899 - val_loss: 1.5704 - val_acc: 0.4400\n",
      "Epoch 33/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.6205 - acc: 0.7521 - val_loss: 2.0886 - val_acc: 0.4200\n",
      "Epoch 34/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.6207 - acc: 0.7185 - val_loss: 2.1254 - val_acc: 0.4600\n",
      "Epoch 35/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.4548 - acc: 0.8319 - val_loss: 1.7958 - val_acc: 0.4600\n",
      "Epoch 36/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.3788 - acc: 0.8739 - val_loss: 2.2839 - val_acc: 0.4800\n",
      "Epoch 37/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.2705 - acc: 0.8782 - val_loss: 2.1281 - val_acc: 0.4600\n",
      "Epoch 38/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.2285 - acc: 0.9286 - val_loss: 3.8537 - val_acc: 0.5200\n",
      "Epoch 39/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.7933 - acc: 0.7143 - val_loss: 3.2748 - val_acc: 0.4800\n",
      "Epoch 40/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.6285 - acc: 0.7857 - val_loss: 3.7187 - val_acc: 0.4600\n",
      "Epoch 41/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.5208 - acc: 0.8025 - val_loss: 1.5627 - val_acc: 0.4600\n",
      "Epoch 42/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.4661 - acc: 0.8529 - val_loss: 2.3048 - val_acc: 0.5000\n",
      "Epoch 43/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.3830 - acc: 0.8361 - val_loss: 2.2371 - val_acc: 0.4800\n",
      "Epoch 44/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.4541 - acc: 0.8487 - val_loss: 2.4998 - val_acc: 0.3600\n",
      "Epoch 45/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.2929 - acc: 0.8782 - val_loss: 3.4253 - val_acc: 0.3800\n",
      "Epoch 46/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.3719 - acc: 0.8739 - val_loss: 3.5273 - val_acc: 0.4000\n",
      "Epoch 47/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.2441 - acc: 0.9034 - val_loss: 3.2919 - val_acc: 0.4200\n",
      "Epoch 48/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.3163 - acc: 0.9076 - val_loss: 2.6255 - val_acc: 0.4200\n",
      "Epoch 49/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.1769 - acc: 0.9538 - val_loss: 3.0225 - val_acc: 0.3400\n",
      "Epoch 50/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.1041 - acc: 0.9790 - val_loss: 5.2027 - val_acc: 0.4400\n",
      "Epoch 51/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.0670 - acc: 0.9748 - val_loss: 4.7943 - val_acc: 0.4200\n",
      "Epoch 52/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.1282 - acc: 0.9622 - val_loss: 5.4117 - val_acc: 0.4400\n",
      "Epoch 53/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.4672 - acc: 0.8613 - val_loss: 3.1315 - val_acc: 0.5000\n",
      "Epoch 54/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.3179 - acc: 0.8908 - val_loss: 4.6809 - val_acc: 0.4200\n",
      "Epoch 55/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.4925 - acc: 0.8866 - val_loss: 2.2951 - val_acc: 0.4800\n",
      "Epoch 56/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.4255 - acc: 0.8824 - val_loss: 1.5781 - val_acc: 0.4000\n",
      "Epoch 57/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.5826 - acc: 0.7647 - val_loss: 2.9614 - val_acc: 0.5200\n",
      "Epoch 58/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.3681 - acc: 0.8613 - val_loss: 2.2299 - val_acc: 0.4600\n",
      "Epoch 59/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.2931 - acc: 0.9118 - val_loss: 4.5651 - val_acc: 0.4200\n",
      "Epoch 60/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.2054 - acc: 0.9370 - val_loss: 2.3412 - val_acc: 0.3800\n",
      "Epoch 61/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.1470 - acc: 0.9538 - val_loss: 4.9420 - val_acc: 0.3600\n",
      "Epoch 62/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.0918 - acc: 0.9706 - val_loss: 4.7445 - val_acc: 0.3800\n",
      "Epoch 63/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.1729 - acc: 0.9328 - val_loss: 3.2025 - val_acc: 0.4400\n",
      "Epoch 64/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.1569 - acc: 0.9412 - val_loss: 4.3676 - val_acc: 0.4200\n",
      "Epoch 65/100\n",
      "238/238 [==============================] - 8s 34ms/step - loss: 0.1699 - acc: 0.9580 - val_loss: 3.8286 - val_acc: 0.4000\n",
      "Epoch 66/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.2046 - acc: 0.9328 - val_loss: 5.0677 - val_acc: 0.4200\n",
      "Epoch 67/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.1529 - acc: 0.9706 - val_loss: 4.0092 - val_acc: 0.4200\n",
      "Epoch 68/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.2733 - acc: 0.8908 - val_loss: 2.5819 - val_acc: 0.4200\n",
      "Epoch 69/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.1952 - acc: 0.9370 - val_loss: 3.3007 - val_acc: 0.4400\n",
      "Epoch 70/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.2286 - acc: 0.9538 - val_loss: 4.3169 - val_acc: 0.4600\n",
      "Epoch 71/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.0856 - acc: 0.9790 - val_loss: 4.2744 - val_acc: 0.4000\n",
      "Epoch 72/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0898 - acc: 0.9748 - val_loss: 3.9122 - val_acc: 0.4400\n",
      "Epoch 73/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.1040 - acc: 0.9706 - val_loss: 4.9859 - val_acc: 0.4200\n",
      "Epoch 74/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0887 - acc: 0.9706 - val_loss: 3.3255 - val_acc: 0.3800\n",
      "Epoch 75/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.0918 - acc: 0.9622 - val_loss: 4.8790 - val_acc: 0.4400\n",
      "Epoch 76/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.0744 - acc: 0.9706 - val_loss: 3.2409 - val_acc: 0.4000\n",
      "Epoch 77/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.1593 - acc: 0.9328 - val_loss: 2.8559 - val_acc: 0.4400\n",
      "Epoch 78/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.0592 - acc: 0.9748 - val_loss: 6.2815 - val_acc: 0.4600\n",
      "Epoch 79/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.2350 - acc: 0.9538 - val_loss: 4.2187 - val_acc: 0.4000\n",
      "Epoch 80/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.5949 - acc: 0.8277 - val_loss: 3.5595 - val_acc: 0.4200\n",
      "Epoch 81/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.3273 - acc: 0.8992 - val_loss: 3.3191 - val_acc: 0.4000\n",
      "Epoch 82/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.1507 - acc: 0.9370 - val_loss: 5.1785 - val_acc: 0.3800\n",
      "Epoch 83/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.1373 - acc: 0.9664 - val_loss: 3.7217 - val_acc: 0.3800\n",
      "Epoch 84/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.1630 - acc: 0.9412 - val_loss: 4.7656 - val_acc: 0.4200\n",
      "Epoch 85/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.0693 - acc: 0.9748 - val_loss: 3.2086 - val_acc: 0.5400\n",
      "Epoch 86/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.2029 - acc: 0.9202 - val_loss: 2.9891 - val_acc: 0.5000\n",
      "Epoch 87/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.2834 - acc: 0.9244 - val_loss: 2.8775 - val_acc: 0.4400\n",
      "Epoch 88/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.1118 - acc: 0.9412 - val_loss: 3.9296 - val_acc: 0.4600\n",
      "Epoch 89/100\n",
      "238/238 [==============================] - 8s 34ms/step - loss: 0.0418 - acc: 0.9916 - val_loss: 4.2798 - val_acc: 0.4200\n",
      "Epoch 90/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.0053 - acc: 1.0000 - val_loss: 4.9730 - val_acc: 0.4600\n",
      "Epoch 91/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.0596 - acc: 0.9874 - val_loss: 4.8948 - val_acc: 0.4400\n",
      "Epoch 92/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.0195 - acc: 0.9916 - val_loss: 4.3796 - val_acc: 0.4400\n",
      "Epoch 93/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.0290 - acc: 0.9916 - val_loss: 4.7860 - val_acc: 0.4600\n",
      "Epoch 94/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.0193 - acc: 0.9958 - val_loss: 5.0040 - val_acc: 0.5000\n",
      "Epoch 95/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.0409 - acc: 0.9958 - val_loss: 4.7174 - val_acc: 0.4800\n",
      "Epoch 96/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.0428 - acc: 0.9790 - val_loss: 5.1906 - val_acc: 0.4200\n",
      "Epoch 97/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.0471 - acc: 0.9832 - val_loss: 4.5085 - val_acc: 0.5000\n",
      "Epoch 98/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.0254 - acc: 0.9916 - val_loss: 4.6046 - val_acc: 0.4800\n",
      "Epoch 99/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0878 - acc: 0.9748 - val_loss: 3.4104 - val_acc: 0.5400\n",
      "Epoch 100/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.0528 - acc: 0.9874 - val_loss: 3.9135 - val_acc: 0.4800\n",
      "50/50 [==============================] - 0s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "epochs = 100  \n",
    "batch_size = 64\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 1st cnn layer: 64 filters of size 11*11 with stride 4\n",
    "#                POOL: 3*3 filters with stride 2\n",
    "model.add(Conv1D(filters=64, kernel_size=11, strides=4, padding='valid',activation='relu',kernel_initializer='uniform',input_shape=X_train.shape[1:]))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=192, kernel_size=5, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=384, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Flatten())  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(num_classes,activation='softmax'))  \n",
    "\n",
    "\n",
    "opt = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.0005, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])  \n",
    "model.summary()  \n",
    "\n",
    "time_callback = TimeHistory()\n",
    "acc_callback = AccuracyHistory()\n",
    "model.fit(X_train, y_train, batch_size=batch_size, validation_data=(X_test,y_test),callbacks=[time_callback,acc_callback],epochs=epochs)\n",
    "times = time_callback.times\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subject 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(288, 25, 1000)\n",
      "(288,)\n",
      "(288, 22, 1000)\n",
      "(288, 1000, 22)\n",
      "(288, 1)\n",
      "X_train: (238, 1000, 22) \n",
      "y_train: (238, 1) \n",
      "X_test: (50, 1000, 22) \n",
      "y_test: (50, 1) \n"
     ]
    }
   ],
   "source": [
    "A01T = h5py.File('A05T_slice.mat', 'r') \n",
    "X = np.copy(A01T['image']) \n",
    "y = np.copy(A01T['type']) \n",
    "y = y[0,0:X.shape[0]:1] \n",
    "y = np.asarray(y, dtype=np.int32)\n",
    "\n",
    "# 288 trials per session\n",
    "# 25 channels (the Ô¨Årst 22 are EEG and the last 3 are EOG signals)\n",
    "# The data is sampled at 250Hz, and so 1000 corresponds to 4s of data for each trial.\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# get rid of the EOG signal\n",
    "X = X[:,:22,:]\n",
    "print(X.shape)\n",
    "\n",
    "# get rif of NaN\n",
    "y = np.asarray(y, dtype=np.float32)-769\n",
    "y = y[:,None]\n",
    "X = X.transpose(0,2,1)\n",
    "where_are_NaNs = np.isnan(X)\n",
    "X[where_are_NaNs] = 0\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# get train, validation, test data\n",
    "data = get_train_test(X,y)\n",
    "for k in data.keys():\n",
    "    print('{}: {} '.format(k, data[k].shape))\n",
    "# X[n_trials, n_steps, n_inputs]\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']\n",
    "\n",
    "# There are 4 classes  \n",
    "num_classes = 4  \n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "#y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_79 (Conv1D)           (None, 248, 64)           15552     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_43 (MaxPooling (None, 123, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_80 (Conv1D)           (None, 123, 192)          61632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_44 (MaxPooling (None, 61, 192)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_81 (Conv1D)           (None, 61, 384)           221568    \n",
      "_________________________________________________________________\n",
      "conv1d_82 (Conv1D)           (None, 61, 256)           295168    \n",
      "_________________________________________________________________\n",
      "conv1d_83 (Conv1D)           (None, 61, 256)           196864    \n",
      "_________________________________________________________________\n",
      "conv1d_84 (Conv1D)           (None, 61, 256)           196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_45 (MaxPooling (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_21 (Flatten)         (None, 7680)              0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 4096)              31461376  \n",
      "_________________________________________________________________\n",
      "dropout_35 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_36 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_37 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 4)                 16388     \n",
      "=================================================================\n",
      "Total params: 66,028,036\n",
      "Trainable params: 66,028,036\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 238 samples, validate on 50 samples\n",
      "Epoch 1/100\n",
      "238/238 [==============================] - 9s 36ms/step - loss: 1.4092 - acc: 0.2689 - val_loss: 1.3964 - val_acc: 0.2600\n",
      "Epoch 2/100\n",
      "238/238 [==============================] - 6s 26ms/step - loss: 1.4317 - acc: 0.2269 - val_loss: 1.3830 - val_acc: 0.2400\n",
      "Epoch 3/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.4102 - acc: 0.2227 - val_loss: 1.3834 - val_acc: 0.2600\n",
      "Epoch 4/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.3883 - acc: 0.3067 - val_loss: 1.3814 - val_acc: 0.3000\n",
      "Epoch 5/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.4010 - acc: 0.2857 - val_loss: 1.3702 - val_acc: 0.3000\n",
      "Epoch 6/100\n",
      "238/238 [==============================] - 8s 35ms/step - loss: 1.3979 - acc: 0.2689 - val_loss: 1.3861 - val_acc: 0.3200\n",
      "Epoch 7/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.3936 - acc: 0.2731 - val_loss: 1.3716 - val_acc: 0.3400\n",
      "Epoch 8/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.3951 - acc: 0.2521 - val_loss: 1.3611 - val_acc: 0.4000\n",
      "Epoch 9/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.3560 - acc: 0.3361 - val_loss: 1.3559 - val_acc: 0.4200\n",
      "Epoch 10/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.3280 - acc: 0.3571 - val_loss: 1.3364 - val_acc: 0.4400\n",
      "Epoch 11/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.3163 - acc: 0.3908 - val_loss: 1.3100 - val_acc: 0.4000\n",
      "Epoch 12/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 1.3007 - acc: 0.3697 - val_loss: 1.3279 - val_acc: 0.4000\n",
      "Epoch 13/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.2207 - acc: 0.4706 - val_loss: 1.3937 - val_acc: 0.4000\n",
      "Epoch 14/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.2799 - acc: 0.4202 - val_loss: 1.2814 - val_acc: 0.4400\n",
      "Epoch 15/100\n",
      "238/238 [==============================] - 8s 34ms/step - loss: 1.2232 - acc: 0.4580 - val_loss: 1.2599 - val_acc: 0.4000\n",
      "Epoch 16/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.1194 - acc: 0.5756 - val_loss: 1.3150 - val_acc: 0.4000\n",
      "Epoch 17/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.1454 - acc: 0.5252 - val_loss: 1.3209 - val_acc: 0.4000\n",
      "Epoch 18/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 1.0877 - acc: 0.5504 - val_loss: 1.2668 - val_acc: 0.4200\n",
      "Epoch 19/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.9878 - acc: 0.5840 - val_loss: 1.4199 - val_acc: 0.4000\n",
      "Epoch 20/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.9898 - acc: 0.5924 - val_loss: 1.4447 - val_acc: 0.4600\n",
      "Epoch 21/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.0146 - acc: 0.5756 - val_loss: 1.6721 - val_acc: 0.4600\n",
      "Epoch 22/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.9959 - acc: 0.5336 - val_loss: 1.2638 - val_acc: 0.5400\n",
      "Epoch 23/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.7905 - acc: 0.6597 - val_loss: 1.3713 - val_acc: 0.4200\n",
      "Epoch 24/100\n",
      "238/238 [==============================] - 8s 35ms/step - loss: 0.7033 - acc: 0.7437 - val_loss: 2.3729 - val_acc: 0.4000\n",
      "Epoch 25/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.9992 - acc: 0.6471 - val_loss: 1.7429 - val_acc: 0.5000\n",
      "Epoch 26/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.9687 - acc: 0.6429 - val_loss: 1.1994 - val_acc: 0.5000\n",
      "Epoch 27/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.8211 - acc: 0.6765 - val_loss: 1.1114 - val_acc: 0.5800\n",
      "Epoch 28/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.6289 - acc: 0.7437 - val_loss: 1.9123 - val_acc: 0.5400\n",
      "Epoch 29/100\n",
      "238/238 [==============================] - 6s 26ms/step - loss: 0.5279 - acc: 0.8067 - val_loss: 1.4599 - val_acc: 0.4800\n",
      "Epoch 30/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.4224 - acc: 0.8571 - val_loss: 1.4254 - val_acc: 0.5000\n",
      "Epoch 31/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.3968 - acc: 0.8487 - val_loss: 3.7160 - val_acc: 0.4200\n",
      "Epoch 32/100\n",
      "238/238 [==============================] - 8s 34ms/step - loss: 1.4582 - acc: 0.5798 - val_loss: 1.3311 - val_acc: 0.2800\n",
      "Epoch 33/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 1.0843 - acc: 0.5420 - val_loss: 1.7682 - val_acc: 0.4400\n",
      "Epoch 34/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.9060 - acc: 0.6303 - val_loss: 1.5410 - val_acc: 0.4600\n",
      "Epoch 35/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.6707 - acc: 0.7353 - val_loss: 1.5822 - val_acc: 0.5000\n",
      "Epoch 36/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.5714 - acc: 0.7731 - val_loss: 1.8267 - val_acc: 0.4000\n",
      "Epoch 37/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.5635 - acc: 0.7773 - val_loss: 2.1319 - val_acc: 0.4400\n",
      "Epoch 38/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.5915 - acc: 0.7815 - val_loss: 2.6676 - val_acc: 0.4600\n",
      "Epoch 39/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.3459 - acc: 0.8908 - val_loss: 2.0950 - val_acc: 0.5400\n",
      "Epoch 40/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.3305 - acc: 0.8697 - val_loss: 2.2023 - val_acc: 0.5000\n",
      "Epoch 41/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.2685 - acc: 0.9244 - val_loss: 2.7773 - val_acc: 0.4600\n",
      "Epoch 42/100\n",
      "238/238 [==============================] - 9s 39ms/step - loss: 0.2009 - acc: 0.9076 - val_loss: 2.5957 - val_acc: 0.5400\n",
      "Epoch 43/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.1787 - acc: 0.9496 - val_loss: 3.0482 - val_acc: 0.5400\n",
      "Epoch 44/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.1098 - acc: 0.9538 - val_loss: 2.3700 - val_acc: 0.6600\n",
      "Epoch 45/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.0560 - acc: 0.9832 - val_loss: 2.4516 - val_acc: 0.6000\n",
      "Epoch 46/100\n",
      "238/238 [==============================] - 6s 26ms/step - loss: 0.0433 - acc: 0.9832 - val_loss: 2.9013 - val_acc: 0.4800\n",
      "Epoch 47/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.0621 - acc: 0.9748 - val_loss: 3.2402 - val_acc: 0.5200\n",
      "Epoch 48/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.0539 - acc: 0.9748 - val_loss: 2.8563 - val_acc: 0.5200\n",
      "Epoch 49/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.1234 - acc: 0.9664 - val_loss: 3.6353 - val_acc: 0.5000\n",
      "Epoch 50/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.2239 - acc: 0.9454 - val_loss: 2.5278 - val_acc: 0.5800\n",
      "Epoch 51/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.1665 - acc: 0.9412 - val_loss: 2.8519 - val_acc: 0.5400\n",
      "Epoch 52/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.1375 - acc: 0.9538 - val_loss: 2.5649 - val_acc: 0.5600\n",
      "Epoch 53/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.2329 - acc: 0.9244 - val_loss: 1.8820 - val_acc: 0.6200\n",
      "Epoch 54/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.1154 - acc: 0.9412 - val_loss: 3.0536 - val_acc: 0.5000\n",
      "Epoch 55/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.0472 - acc: 0.9874 - val_loss: 2.9450 - val_acc: 0.5000\n",
      "Epoch 56/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.0088 - acc: 1.0000 - val_loss: 3.2642 - val_acc: 0.6000\n",
      "Epoch 57/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.0429 - acc: 0.9748 - val_loss: 2.9070 - val_acc: 0.6200\n",
      "Epoch 58/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.0167 - acc: 1.0000 - val_loss: 3.3390 - val_acc: 0.5400\n",
      "Epoch 59/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.1247 - acc: 0.9790 - val_loss: 2.7976 - val_acc: 0.6200\n",
      "Epoch 60/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.0168 - acc: 0.9916 - val_loss: 2.8015 - val_acc: 0.6000\n",
      "Epoch 61/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.0365 - acc: 0.9832 - val_loss: 3.1384 - val_acc: 0.6200\n",
      "Epoch 62/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0190 - acc: 0.9958 - val_loss: 3.1965 - val_acc: 0.6000\n",
      "Epoch 63/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.0087 - acc: 0.9958 - val_loss: 4.0850 - val_acc: 0.5000\n",
      "Epoch 64/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0390 - acc: 0.9832 - val_loss: 3.1119 - val_acc: 0.6400\n",
      "Epoch 65/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.0647 - acc: 0.9748 - val_loss: 3.0878 - val_acc: 0.6000\n",
      "Epoch 66/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.0420 - acc: 0.9790 - val_loss: 2.9572 - val_acc: 0.6400\n",
      "Epoch 67/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.0044 - acc: 1.0000 - val_loss: 3.2519 - val_acc: 0.6600\n",
      "Epoch 68/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.0199 - acc: 0.9916 - val_loss: 3.3193 - val_acc: 0.6400\n",
      "Epoch 69/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 3.8315 - val_acc: 0.6000\n",
      "Epoch 70/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0273 - acc: 0.9916 - val_loss: 3.6505 - val_acc: 0.6200\n",
      "Epoch 71/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.0090 - acc: 0.9958 - val_loss: 3.8810 - val_acc: 0.5800\n",
      "Epoch 72/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0752 - acc: 0.9790 - val_loss: 3.9852 - val_acc: 0.5600\n",
      "Epoch 73/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.0186 - acc: 0.9916 - val_loss: 4.3042 - val_acc: 0.5600\n",
      "Epoch 74/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0208 - acc: 0.9958 - val_loss: 4.4805 - val_acc: 0.6400\n",
      "Epoch 75/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0328 - acc: 0.9790 - val_loss: 3.4825 - val_acc: 0.6000\n",
      "Epoch 76/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.1003 - acc: 0.9622 - val_loss: 3.2980 - val_acc: 0.5400\n",
      "Epoch 77/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.1044 - acc: 0.9748 - val_loss: 4.0953 - val_acc: 0.5200\n",
      "Epoch 78/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.2397 - acc: 0.9370 - val_loss: 3.7408 - val_acc: 0.5000\n",
      "Epoch 79/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.0280 - acc: 0.9958 - val_loss: 2.7300 - val_acc: 0.5400\n",
      "Epoch 80/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.0384 - acc: 0.9874 - val_loss: 2.7976 - val_acc: 0.5400\n",
      "Epoch 81/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.0285 - acc: 0.9958 - val_loss: 3.3025 - val_acc: 0.5600\n",
      "Epoch 82/100\n",
      "238/238 [==============================] - 6s 26ms/step - loss: 0.0090 - acc: 1.0000 - val_loss: 3.2640 - val_acc: 0.5600\n",
      "Epoch 83/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.0593 - acc: 0.9832 - val_loss: 3.0218 - val_acc: 0.6000\n",
      "Epoch 84/100\n",
      "238/238 [==============================] - 8s 34ms/step - loss: 0.0413 - acc: 0.9832 - val_loss: 2.9454 - val_acc: 0.5400\n",
      "Epoch 85/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0258 - acc: 0.9874 - val_loss: 2.8875 - val_acc: 0.5600\n",
      "Epoch 86/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.0518 - acc: 0.9790 - val_loss: 3.1501 - val_acc: 0.5600\n",
      "Epoch 87/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.1783 - acc: 0.9580 - val_loss: 2.4828 - val_acc: 0.5600\n",
      "Epoch 88/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.0672 - acc: 0.9874 - val_loss: 2.8884 - val_acc: 0.5000\n",
      "Epoch 89/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.0213 - acc: 0.9958 - val_loss: 3.0186 - val_acc: 0.5400\n",
      "Epoch 90/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.0484 - acc: 0.9790 - val_loss: 3.5223 - val_acc: 0.5600\n",
      "Epoch 91/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0215 - acc: 0.9958 - val_loss: 3.7582 - val_acc: 0.5600\n",
      "Epoch 92/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0062 - acc: 0.9958 - val_loss: 3.7216 - val_acc: 0.5400\n",
      "Epoch 93/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0157 - acc: 0.9916 - val_loss: 3.6473 - val_acc: 0.5600\n",
      "Epoch 94/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0395 - acc: 0.9874 - val_loss: 3.6235 - val_acc: 0.6000\n",
      "Epoch 95/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.0212 - acc: 0.9916 - val_loss: 3.0313 - val_acc: 0.6200\n",
      "Epoch 96/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0018 - acc: 1.0000 - val_loss: 3.6960 - val_acc: 0.5600\n",
      "Epoch 97/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.0109 - acc: 0.9958 - val_loss: 3.4720 - val_acc: 0.6000\n",
      "Epoch 98/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.0055 - acc: 0.9958 - val_loss: 3.1777 - val_acc: 0.5600\n",
      "Epoch 99/100\n",
      "238/238 [==============================] - 8s 35ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 3.5684 - val_acc: 0.5800\n",
      "Epoch 100/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.0034 - acc: 1.0000 - val_loss: 3.3194 - val_acc: 0.5800\n",
      "50/50 [==============================] - 0s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "epochs = 100  \n",
    "batch_size = 64\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 1st cnn layer: 64 filters of size 11*11 with stride 4\n",
    "#                POOL: 3*3 filters with stride 2\n",
    "model.add(Conv1D(filters=64, kernel_size=11, strides=4, padding='valid',activation='relu',kernel_initializer='uniform',input_shape=X_train.shape[1:]))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=192, kernel_size=5, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=384, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Flatten())  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(num_classes,activation='softmax'))  \n",
    "\n",
    "\n",
    "opt = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.0005, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])  \n",
    "model.summary()  \n",
    "\n",
    "time_callback = TimeHistory()\n",
    "acc_callback = AccuracyHistory()\n",
    "model.fit(X_train, y_train, batch_size=batch_size, validation_data=(X_test,y_test),callbacks=[time_callback,acc_callback],epochs=epochs)\n",
    "times = time_callback.times\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subject 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(288, 25, 1000)\n",
      "(288,)\n",
      "(288, 22, 1000)\n",
      "(288, 1000, 22)\n",
      "(288, 1)\n",
      "X_train: (238, 1000, 22) \n",
      "y_train: (238, 1) \n",
      "X_test: (50, 1000, 22) \n",
      "y_test: (50, 1) \n"
     ]
    }
   ],
   "source": [
    "A01T = h5py.File('A06T_slice.mat', 'r') \n",
    "X = np.copy(A01T['image']) \n",
    "y = np.copy(A01T['type']) \n",
    "y = y[0,0:X.shape[0]:1] \n",
    "y = np.asarray(y, dtype=np.int32)\n",
    "\n",
    "# 288 trials per session\n",
    "# 25 channels (the Ô¨Årst 22 are EEG and the last 3 are EOG signals)\n",
    "# The data is sampled at 250Hz, and so 1000 corresponds to 4s of data for each trial.\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# get rid of the EOG signal\n",
    "X = X[:,:22,:]\n",
    "print(X.shape)\n",
    "\n",
    "# get rif of NaN\n",
    "y = np.asarray(y, dtype=np.float32)-769\n",
    "y = y[:,None]\n",
    "X = X.transpose(0,2,1)\n",
    "where_are_NaNs = np.isnan(X)\n",
    "X[where_are_NaNs] = 0\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# get train, validation, test data\n",
    "data = get_train_test(X,y)\n",
    "for k in data.keys():\n",
    "    print('{}: {} '.format(k, data[k].shape))\n",
    "# X[n_trials, n_steps, n_inputs]\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']\n",
    "\n",
    "# There are 4 classes  \n",
    "num_classes = 4  \n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "#y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_85 (Conv1D)           (None, 248, 64)           15552     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_46 (MaxPooling (None, 123, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_86 (Conv1D)           (None, 123, 192)          61632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_47 (MaxPooling (None, 61, 192)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_87 (Conv1D)           (None, 61, 384)           221568    \n",
      "_________________________________________________________________\n",
      "conv1d_88 (Conv1D)           (None, 61, 256)           295168    \n",
      "_________________________________________________________________\n",
      "conv1d_89 (Conv1D)           (None, 61, 256)           196864    \n",
      "_________________________________________________________________\n",
      "conv1d_90 (Conv1D)           (None, 61, 256)           196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_48 (MaxPooling (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_22 (Flatten)         (None, 7680)              0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 4096)              31461376  \n",
      "_________________________________________________________________\n",
      "dropout_38 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_39 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_40 (Dropout)         (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 4)                 16388     \n",
      "=================================================================\n",
      "Total params: 66,028,036\n",
      "Trainable params: 66,028,036\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 238 samples, validate on 50 samples\n",
      "Epoch 1/100\n",
      "238/238 [==============================] - 8s 36ms/step - loss: 1.5892 - acc: 0.2185 - val_loss: 1.3937 - val_acc: 0.2400\n",
      "Epoch 2/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.4093 - acc: 0.2311 - val_loss: 1.3923 - val_acc: 0.2400\n",
      "Epoch 3/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 1.4330 - acc: 0.2059 - val_loss: 1.3974 - val_acc: 0.2400\n",
      "Epoch 4/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.4056 - acc: 0.2479 - val_loss: 1.3910 - val_acc: 0.2200\n",
      "Epoch 5/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.4016 - acc: 0.2521 - val_loss: 1.3894 - val_acc: 0.2000\n",
      "Epoch 6/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.3971 - acc: 0.2479 - val_loss: 1.3941 - val_acc: 0.2600\n",
      "Epoch 7/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.3834 - acc: 0.2857 - val_loss: 1.3874 - val_acc: 0.2200\n",
      "Epoch 8/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.3781 - acc: 0.2815 - val_loss: 1.3843 - val_acc: 0.2600\n",
      "Epoch 9/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.3947 - acc: 0.2647 - val_loss: 1.3787 - val_acc: 0.2400\n",
      "Epoch 10/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.3871 - acc: 0.2773 - val_loss: 1.3745 - val_acc: 0.3800\n",
      "Epoch 11/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.3841 - acc: 0.3235 - val_loss: 1.3761 - val_acc: 0.2400\n",
      "Epoch 12/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 1.3535 - acc: 0.3067 - val_loss: 1.3744 - val_acc: 0.2800\n",
      "Epoch 13/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.3761 - acc: 0.2731 - val_loss: 1.3654 - val_acc: 0.2400\n",
      "Epoch 14/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.3596 - acc: 0.3571 - val_loss: 1.3538 - val_acc: 0.4600\n",
      "Epoch 15/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.3356 - acc: 0.3361 - val_loss: 1.3608 - val_acc: 0.2800\n",
      "Epoch 16/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.3290 - acc: 0.3487 - val_loss: 1.3160 - val_acc: 0.4200\n",
      "Epoch 17/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.2879 - acc: 0.4076 - val_loss: 1.3230 - val_acc: 0.3400\n",
      "Epoch 18/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.3037 - acc: 0.3866 - val_loss: 1.2900 - val_acc: 0.3000\n",
      "Epoch 19/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 1.2660 - acc: 0.4244 - val_loss: 1.3019 - val_acc: 0.4400\n",
      "Epoch 20/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 1.2548 - acc: 0.4580 - val_loss: 1.3239 - val_acc: 0.3400\n",
      "Epoch 21/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.2228 - acc: 0.4538 - val_loss: 1.4089 - val_acc: 0.2800\n",
      "Epoch 22/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.2618 - acc: 0.4076 - val_loss: 1.3149 - val_acc: 0.3400\n",
      "Epoch 23/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.1736 - acc: 0.4580 - val_loss: 1.2767 - val_acc: 0.2800\n",
      "Epoch 24/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.0743 - acc: 0.5672 - val_loss: 1.3456 - val_acc: 0.4600\n",
      "Epoch 25/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.0719 - acc: 0.5420 - val_loss: 1.3612 - val_acc: 0.5000\n",
      "Epoch 26/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.9573 - acc: 0.6008 - val_loss: 1.2528 - val_acc: 0.4000\n",
      "Epoch 27/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.9264 - acc: 0.6261 - val_loss: 1.2733 - val_acc: 0.4600\n",
      "Epoch 28/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.9144 - acc: 0.6303 - val_loss: 1.7067 - val_acc: 0.4400\n",
      "Epoch 29/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.8550 - acc: 0.6303 - val_loss: 1.3328 - val_acc: 0.4600\n",
      "Epoch 30/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.6554 - acc: 0.7143 - val_loss: 1.3886 - val_acc: 0.4000\n",
      "Epoch 31/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.6303 - acc: 0.7605 - val_loss: 1.7216 - val_acc: 0.4400\n",
      "Epoch 32/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.6755 - acc: 0.7185 - val_loss: 1.3015 - val_acc: 0.4000\n",
      "Epoch 33/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.7640 - acc: 0.6681 - val_loss: 4.6010 - val_acc: 0.4000\n",
      "Epoch 34/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.2073 - acc: 0.6765 - val_loss: 1.5186 - val_acc: 0.3600\n",
      "Epoch 35/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.9910 - acc: 0.5672 - val_loss: 2.0497 - val_acc: 0.3400\n",
      "Epoch 36/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.9010 - acc: 0.6134 - val_loss: 1.4860 - val_acc: 0.3800\n",
      "Epoch 37/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.7664 - acc: 0.6849 - val_loss: 2.4376 - val_acc: 0.3600\n",
      "Epoch 38/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.9332 - acc: 0.6681 - val_loss: 1.4269 - val_acc: 0.3600\n",
      "Epoch 39/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.6814 - acc: 0.7395 - val_loss: 1.8903 - val_acc: 0.4000\n",
      "Epoch 40/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.5904 - acc: 0.7479 - val_loss: 1.3711 - val_acc: 0.4200\n",
      "Epoch 41/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.7181 - acc: 0.7143 - val_loss: 1.9608 - val_acc: 0.3600\n",
      "Epoch 42/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.9525 - acc: 0.6176 - val_loss: 1.3144 - val_acc: 0.4400\n",
      "Epoch 43/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.7177 - acc: 0.7479 - val_loss: 2.3073 - val_acc: 0.4200\n",
      "Epoch 44/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.5835 - acc: 0.7437 - val_loss: 2.3105 - val_acc: 0.3800\n",
      "Epoch 45/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.4860 - acc: 0.7857 - val_loss: 2.1403 - val_acc: 0.3600\n",
      "Epoch 46/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.3931 - acc: 0.8445 - val_loss: 2.3299 - val_acc: 0.5400\n",
      "Epoch 47/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.3737 - acc: 0.8571 - val_loss: 2.5835 - val_acc: 0.4000\n",
      "Epoch 48/100\n",
      "238/238 [==============================] - 7s 27ms/step - loss: 0.3204 - acc: 0.8950 - val_loss: 2.1781 - val_acc: 0.5200\n",
      "Epoch 49/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.2014 - acc: 0.9286 - val_loss: 2.9334 - val_acc: 0.4000\n",
      "Epoch 50/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.1755 - acc: 0.9244 - val_loss: 4.0231 - val_acc: 0.4200\n",
      "Epoch 51/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.2012 - acc: 0.9328 - val_loss: 3.0017 - val_acc: 0.4200\n",
      "Epoch 52/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.1867 - acc: 0.9538 - val_loss: 2.6336 - val_acc: 0.4400\n",
      "Epoch 53/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.1863 - acc: 0.9244 - val_loss: 3.8701 - val_acc: 0.4800\n",
      "Epoch 54/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.0861 - acc: 0.9622 - val_loss: 3.2923 - val_acc: 0.4800\n",
      "Epoch 55/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0707 - acc: 0.9790 - val_loss: 3.7819 - val_acc: 0.4200\n",
      "Epoch 56/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.1175 - acc: 0.9664 - val_loss: 3.4269 - val_acc: 0.4800\n",
      "Epoch 57/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.0627 - acc: 0.9748 - val_loss: 2.8305 - val_acc: 0.5400\n",
      "Epoch 58/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.0519 - acc: 0.9874 - val_loss: 3.7750 - val_acc: 0.5200\n",
      "Epoch 59/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.1232 - acc: 0.9622 - val_loss: 3.9134 - val_acc: 0.4800\n",
      "Epoch 60/100\n",
      "238/238 [==============================] - 7s 27ms/step - loss: 0.2010 - acc: 0.9412 - val_loss: 2.7374 - val_acc: 0.4400\n",
      "Epoch 61/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.1552 - acc: 0.9412 - val_loss: 3.1537 - val_acc: 0.4400\n",
      "Epoch 62/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.1855 - acc: 0.9370 - val_loss: 3.2229 - val_acc: 0.4400\n",
      "Epoch 63/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.0884 - acc: 0.9706 - val_loss: 3.1064 - val_acc: 0.4000\n",
      "Epoch 64/100\n",
      "238/238 [==============================] - 7s 27ms/step - loss: 0.0765 - acc: 0.9706 - val_loss: 2.7325 - val_acc: 0.4600\n",
      "Epoch 65/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.0234 - acc: 0.9916 - val_loss: 3.9712 - val_acc: 0.5000\n",
      "Epoch 66/100\n",
      "238/238 [==============================] - 7s 27ms/step - loss: 0.0261 - acc: 0.9916 - val_loss: 4.7752 - val_acc: 0.5000\n",
      "Epoch 67/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.0096 - acc: 0.9958 - val_loss: 4.2546 - val_acc: 0.4800\n",
      "Epoch 68/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.0203 - acc: 0.9874 - val_loss: 4.6858 - val_acc: 0.4800\n",
      "Epoch 69/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.0058 - acc: 1.0000 - val_loss: 4.7198 - val_acc: 0.4200\n",
      "Epoch 70/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.0355 - acc: 0.9874 - val_loss: 4.0973 - val_acc: 0.5000\n",
      "Epoch 71/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.0394 - acc: 0.9832 - val_loss: 5.5522 - val_acc: 0.4200\n",
      "Epoch 72/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.0926 - acc: 0.9706 - val_loss: 4.2128 - val_acc: 0.5600\n",
      "Epoch 73/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.1209 - acc: 0.9580 - val_loss: 4.6651 - val_acc: 0.4200\n",
      "Epoch 74/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.1084 - acc: 0.9706 - val_loss: 4.0764 - val_acc: 0.5000\n",
      "Epoch 75/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.0536 - acc: 0.9790 - val_loss: 2.2580 - val_acc: 0.6400\n",
      "Epoch 76/100\n",
      "238/238 [==============================] - 7s 27ms/step - loss: 0.1353 - acc: 0.9706 - val_loss: 3.7010 - val_acc: 0.4400\n",
      "Epoch 77/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.0929 - acc: 0.9748 - val_loss: 3.2332 - val_acc: 0.4200\n",
      "Epoch 78/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.0516 - acc: 0.9874 - val_loss: 3.0393 - val_acc: 0.5600\n",
      "Epoch 79/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.0143 - acc: 0.9916 - val_loss: 3.8970 - val_acc: 0.5200\n",
      "Epoch 80/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.0368 - acc: 0.9874 - val_loss: 3.9431 - val_acc: 0.4400\n",
      "Epoch 81/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.0414 - acc: 0.9790 - val_loss: 4.1088 - val_acc: 0.4800\n",
      "Epoch 82/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.0277 - acc: 0.9832 - val_loss: 5.0892 - val_acc: 0.4800\n",
      "Epoch 83/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.0280 - acc: 0.9916 - val_loss: 4.7785 - val_acc: 0.4200\n",
      "Epoch 84/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.0063 - acc: 0.9958 - val_loss: 3.9101 - val_acc: 0.5000\n",
      "Epoch 85/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.0482 - acc: 0.9832 - val_loss: 4.2541 - val_acc: 0.5400\n",
      "Epoch 86/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.0661 - acc: 0.9790 - val_loss: 4.1281 - val_acc: 0.5000\n",
      "Epoch 87/100\n",
      "238/238 [==============================] - 7s 27ms/step - loss: 0.0909 - acc: 0.9790 - val_loss: 3.2343 - val_acc: 0.5800\n",
      "Epoch 88/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.0739 - acc: 0.9706 - val_loss: 2.8289 - val_acc: 0.6000\n",
      "Epoch 89/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.0222 - acc: 0.9874 - val_loss: 3.1973 - val_acc: 0.5400\n",
      "Epoch 90/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.0367 - acc: 0.9874 - val_loss: 3.1707 - val_acc: 0.5600\n",
      "Epoch 91/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.1111 - acc: 0.9580 - val_loss: 3.4953 - val_acc: 0.6000\n",
      "Epoch 92/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.1832 - acc: 0.9580 - val_loss: 3.6224 - val_acc: 0.4600\n",
      "Epoch 93/100\n",
      "238/238 [==============================] - 7s 27ms/step - loss: 0.1160 - acc: 0.9538 - val_loss: 3.9253 - val_acc: 0.4800\n",
      "Epoch 94/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.2334 - acc: 0.9286 - val_loss: 3.1266 - val_acc: 0.5800\n",
      "Epoch 95/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.0996 - acc: 0.9664 - val_loss: 3.7440 - val_acc: 0.4200\n",
      "Epoch 96/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.0857 - acc: 0.9580 - val_loss: 2.8430 - val_acc: 0.5600\n",
      "Epoch 97/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.0893 - acc: 0.9664 - val_loss: 3.7049 - val_acc: 0.4600\n",
      "Epoch 98/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.0688 - acc: 0.9790 - val_loss: 3.3723 - val_acc: 0.5000\n",
      "Epoch 99/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.0239 - acc: 0.9916 - val_loss: 3.4320 - val_acc: 0.5000\n",
      "Epoch 100/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.0471 - acc: 0.9832 - val_loss: 3.6417 - val_acc: 0.5600\n",
      "50/50 [==============================] - 1s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "epochs = 100  \n",
    "batch_size = 64\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 1st cnn layer: 64 filters of size 11*11 with stride 4\n",
    "#                POOL: 3*3 filters with stride 2\n",
    "model.add(Conv1D(filters=64, kernel_size=11, strides=4, padding='valid',activation='relu',kernel_initializer='uniform',input_shape=X_train.shape[1:]))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=192, kernel_size=5, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=384, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Flatten())  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(num_classes,activation='softmax'))  \n",
    "\n",
    "\n",
    "opt = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.0005, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])  \n",
    "model.summary()  \n",
    "\n",
    "time_callback = TimeHistory()\n",
    "acc_callback = AccuracyHistory()\n",
    "model.fit(X_train, y_train, batch_size=batch_size, validation_data=(X_test,y_test),callbacks=[time_callback,acc_callback],epochs=epochs)\n",
    "times = time_callback.times\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subject 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(288, 25, 1000)\n",
      "(288,)\n",
      "(288, 22, 1000)\n",
      "(288, 1000, 22)\n",
      "(288, 1)\n",
      "X_train: (238, 1000, 22) \n",
      "y_train: (238, 1) \n",
      "X_test: (50, 1000, 22) \n",
      "y_test: (50, 1) \n"
     ]
    }
   ],
   "source": [
    "A01T = h5py.File('A07T_slice.mat', 'r') \n",
    "X = np.copy(A01T['image']) \n",
    "y = np.copy(A01T['type']) \n",
    "y = y[0,0:X.shape[0]:1] \n",
    "y = np.asarray(y, dtype=np.int32)\n",
    "\n",
    "# 288 trials per session\n",
    "# 25 channels (the Ô¨Årst 22 are EEG and the last 3 are EOG signals)\n",
    "# The data is sampled at 250Hz, and so 1000 corresponds to 4s of data for each trial.\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# get rid of the EOG signal\n",
    "X = X[:,:22,:]\n",
    "print(X.shape)\n",
    "\n",
    "# get rif of NaN\n",
    "y = np.asarray(y, dtype=np.float32)-769\n",
    "y = y[:,None]\n",
    "X = X.transpose(0,2,1)\n",
    "where_are_NaNs = np.isnan(X)\n",
    "X[where_are_NaNs] = 0\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# get train, validation, test data\n",
    "data = get_train_test(X,y)\n",
    "for k in data.keys():\n",
    "    print('{}: {} '.format(k, data[k].shape))\n",
    "# X[n_trials, n_steps, n_inputs]\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']\n",
    "\n",
    "# There are 4 classes  \n",
    "num_classes = 4  \n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "#y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_7 (Conv1D)            (None, 248, 64)           15552     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 123, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 123, 192)          61632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 61, 192)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 61, 384)           221568    \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 61, 256)           295168    \n",
      "_________________________________________________________________\n",
      "conv1d_11 (Conv1D)           (None, 61, 256)           196864    \n",
      "_________________________________________________________________\n",
      "conv1d_12 (Conv1D)           (None, 61, 256)           196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 7680)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4096)              31461376  \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 4)                 16388     \n",
      "=================================================================\n",
      "Total params: 66,028,036\n",
      "Trainable params: 66,028,036\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 238 samples, validate on 50 samples\n",
      "Epoch 1/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.4487 - acc: 0.2185 - val_loss: 1.3840 - val_acc: 0.2200\n",
      "Epoch 2/100\n",
      "238/238 [==============================] - 6s 26ms/step - loss: 1.3917 - acc: 0.2857 - val_loss: 1.3932 - val_acc: 0.2400\n",
      "Epoch 3/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.4199 - acc: 0.2395 - val_loss: 1.3862 - val_acc: 0.3400\n",
      "Epoch 4/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.4069 - acc: 0.2647 - val_loss: 1.3717 - val_acc: 0.2800\n",
      "Epoch 5/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.3604 - acc: 0.3319 - val_loss: 1.3664 - val_acc: 0.3400\n",
      "Epoch 6/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.3609 - acc: 0.3025 - val_loss: 1.3491 - val_acc: 0.3000\n",
      "Epoch 7/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.3451 - acc: 0.3193 - val_loss: 1.3185 - val_acc: 0.3400\n",
      "Epoch 8/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.2961 - acc: 0.4118 - val_loss: 1.2828 - val_acc: 0.3800\n",
      "Epoch 9/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.2533 - acc: 0.4202 - val_loss: 1.2445 - val_acc: 0.4200\n",
      "Epoch 10/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.3006 - acc: 0.3697 - val_loss: 1.2822 - val_acc: 0.3800\n",
      "Epoch 11/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.2639 - acc: 0.3908 - val_loss: 1.3170 - val_acc: 0.3200\n",
      "Epoch 12/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.1898 - acc: 0.4916 - val_loss: 1.2066 - val_acc: 0.4400\n",
      "Epoch 13/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 1.1403 - acc: 0.4958 - val_loss: 1.2048 - val_acc: 0.4600\n",
      "Epoch 14/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.0961 - acc: 0.4916 - val_loss: 1.2168 - val_acc: 0.3400\n",
      "Epoch 15/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.0439 - acc: 0.5084 - val_loss: 1.2423 - val_acc: 0.4600\n",
      "Epoch 16/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.0053 - acc: 0.5252 - val_loss: 1.2737 - val_acc: 0.4000\n",
      "Epoch 17/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.9955 - acc: 0.5294 - val_loss: 1.2126 - val_acc: 0.4200\n",
      "Epoch 18/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.9525 - acc: 0.5756 - val_loss: 1.1784 - val_acc: 0.3800\n",
      "Epoch 19/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.8703 - acc: 0.5966 - val_loss: 1.1199 - val_acc: 0.4400\n",
      "Epoch 20/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.8687 - acc: 0.6681 - val_loss: 1.4550 - val_acc: 0.4200\n",
      "Epoch 21/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.9318 - acc: 0.5966 - val_loss: 1.3843 - val_acc: 0.4200\n",
      "Epoch 22/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.8142 - acc: 0.6345 - val_loss: 1.3405 - val_acc: 0.4000\n",
      "Epoch 23/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.7292 - acc: 0.7185 - val_loss: 1.2382 - val_acc: 0.4400\n",
      "Epoch 24/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.6392 - acc: 0.7101 - val_loss: 1.4172 - val_acc: 0.4600\n",
      "Epoch 25/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.6080 - acc: 0.7563 - val_loss: 1.8568 - val_acc: 0.4400\n",
      "Epoch 26/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.5504 - acc: 0.7563 - val_loss: 1.2361 - val_acc: 0.5000\n",
      "Epoch 27/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.6698 - acc: 0.7017 - val_loss: 1.3762 - val_acc: 0.4400\n",
      "Epoch 28/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.6003 - acc: 0.7521 - val_loss: 1.2963 - val_acc: 0.4400\n",
      "Epoch 29/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.6876 - acc: 0.7059 - val_loss: 1.4795 - val_acc: 0.4200\n",
      "Epoch 30/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.5220 - acc: 0.7899 - val_loss: 1.3551 - val_acc: 0.5200\n",
      "Epoch 31/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.5254 - acc: 0.7689 - val_loss: 1.6540 - val_acc: 0.4800\n",
      "Epoch 32/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.4216 - acc: 0.7899 - val_loss: 2.0634 - val_acc: 0.4800\n",
      "Epoch 33/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.3809 - acc: 0.8193 - val_loss: 1.9521 - val_acc: 0.4600\n",
      "Epoch 34/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.3297 - acc: 0.8613 - val_loss: 1.7951 - val_acc: 0.5600\n",
      "Epoch 35/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.3963 - acc: 0.8824 - val_loss: 1.9546 - val_acc: 0.4800\n",
      "Epoch 36/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.3986 - acc: 0.8235 - val_loss: 1.8630 - val_acc: 0.4600\n",
      "Epoch 37/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.3068 - acc: 0.8866 - val_loss: 1.5231 - val_acc: 0.5800\n",
      "Epoch 38/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.2197 - acc: 0.9328 - val_loss: 2.1043 - val_acc: 0.5200\n",
      "Epoch 39/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.1517 - acc: 0.9454 - val_loss: 2.4029 - val_acc: 0.5400\n",
      "Epoch 40/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.1335 - acc: 0.9454 - val_loss: 2.0746 - val_acc: 0.5600\n",
      "Epoch 41/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0809 - acc: 0.9748 - val_loss: 2.3882 - val_acc: 0.5600\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238/238 [==============================] - 8s 32ms/step - loss: 0.0718 - acc: 0.9748 - val_loss: 2.1074 - val_acc: 0.5800\n",
      "Epoch 43/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.1079 - acc: 0.9580 - val_loss: 2.5555 - val_acc: 0.6000\n",
      "Epoch 44/100\n",
      "238/238 [==============================] - 8s 34ms/step - loss: 0.1400 - acc: 0.9454 - val_loss: 2.9327 - val_acc: 0.5000\n",
      "Epoch 45/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.1471 - acc: 0.9370 - val_loss: 2.7958 - val_acc: 0.5400\n",
      "Epoch 46/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.1147 - acc: 0.9538 - val_loss: 2.2834 - val_acc: 0.4600\n",
      "Epoch 47/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.1202 - acc: 0.9664 - val_loss: 2.5251 - val_acc: 0.5000\n",
      "Epoch 48/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.0647 - acc: 0.9832 - val_loss: 2.7998 - val_acc: 0.4800\n",
      "Epoch 49/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.0969 - acc: 0.9538 - val_loss: 2.2869 - val_acc: 0.5000\n",
      "Epoch 50/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.1363 - acc: 0.9580 - val_loss: 3.6465 - val_acc: 0.4800\n",
      "Epoch 51/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.1692 - acc: 0.9580 - val_loss: 2.4362 - val_acc: 0.5000\n",
      "Epoch 52/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.3057 - acc: 0.9034 - val_loss: 1.8117 - val_acc: 0.5400\n",
      "Epoch 53/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.1890 - acc: 0.9412 - val_loss: 1.6657 - val_acc: 0.5200\n",
      "Epoch 54/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.0861 - acc: 0.9790 - val_loss: 2.6147 - val_acc: 0.5800\n",
      "Epoch 55/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.0679 - acc: 0.9748 - val_loss: 2.2487 - val_acc: 0.5400\n",
      "Epoch 56/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.1025 - acc: 0.9622 - val_loss: 2.9506 - val_acc: 0.5800\n",
      "Epoch 57/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.1462 - acc: 0.9454 - val_loss: 2.8653 - val_acc: 0.4800\n",
      "Epoch 58/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.1618 - acc: 0.9622 - val_loss: 2.6538 - val_acc: 0.4600\n",
      "Epoch 59/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0758 - acc: 0.9748 - val_loss: 2.1230 - val_acc: 0.5800\n",
      "Epoch 60/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0625 - acc: 0.9916 - val_loss: 2.8417 - val_acc: 0.5200\n",
      "Epoch 61/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0181 - acc: 0.9958 - val_loss: 3.0390 - val_acc: 0.5600\n",
      "Epoch 62/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.0404 - acc: 0.9874 - val_loss: 2.2034 - val_acc: 0.5400\n",
      "Epoch 63/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.0479 - acc: 0.9790 - val_loss: 4.8625 - val_acc: 0.5200\n",
      "Epoch 64/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.2194 - acc: 0.9286 - val_loss: 2.0107 - val_acc: 0.5600\n",
      "Epoch 65/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.1847 - acc: 0.9412 - val_loss: 2.7962 - val_acc: 0.5800\n",
      "Epoch 66/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.1169 - acc: 0.9622 - val_loss: 3.7666 - val_acc: 0.4200\n",
      "Epoch 67/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.1216 - acc: 0.9538 - val_loss: 2.2673 - val_acc: 0.5200\n",
      "Epoch 68/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.1776 - acc: 0.9244 - val_loss: 2.9843 - val_acc: 0.5400\n",
      "Epoch 69/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.1526 - acc: 0.9580 - val_loss: 2.6173 - val_acc: 0.4800\n",
      "Epoch 70/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.2793 - acc: 0.9118 - val_loss: 3.2740 - val_acc: 0.5000\n",
      "Epoch 71/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0918 - acc: 0.9538 - val_loss: 2.3724 - val_acc: 0.5200\n",
      "Epoch 72/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.0759 - acc: 0.9664 - val_loss: 1.6725 - val_acc: 0.5800\n",
      "Epoch 73/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.0491 - acc: 0.9874 - val_loss: 3.2600 - val_acc: 0.5200\n",
      "Epoch 74/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.0720 - acc: 0.9790 - val_loss: 2.7817 - val_acc: 0.5400\n",
      "Epoch 75/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0803 - acc: 0.9790 - val_loss: 3.7281 - val_acc: 0.5600\n",
      "Epoch 76/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.1056 - acc: 0.9748 - val_loss: 3.3758 - val_acc: 0.4800\n",
      "Epoch 77/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.1679 - acc: 0.9538 - val_loss: 3.5554 - val_acc: 0.5400\n",
      "Epoch 78/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.2118 - acc: 0.9370 - val_loss: 2.8654 - val_acc: 0.6200\n",
      "Epoch 79/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.1955 - acc: 0.9370 - val_loss: 1.9548 - val_acc: 0.5400\n",
      "Epoch 80/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0913 - acc: 0.9706 - val_loss: 2.5879 - val_acc: 0.4400\n",
      "Epoch 81/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.0886 - acc: 0.9706 - val_loss: 3.2257 - val_acc: 0.4800\n",
      "Epoch 82/100\n",
      "238/238 [==============================] - 6s 26ms/step - loss: 0.0500 - acc: 0.9790 - val_loss: 2.9984 - val_acc: 0.5000\n",
      "Epoch 83/100\n",
      "238/238 [==============================] - 8s 34ms/step - loss: 0.0351 - acc: 0.9874 - val_loss: 3.6380 - val_acc: 0.5200\n",
      "Epoch 84/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.0112 - acc: 1.0000 - val_loss: 3.4747 - val_acc: 0.4800\n",
      "Epoch 85/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.0225 - acc: 0.9916 - val_loss: 3.6105 - val_acc: 0.5200\n",
      "Epoch 86/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.0045 - acc: 1.0000 - val_loss: 3.7677 - val_acc: 0.5200\n",
      "Epoch 87/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.0083 - acc: 0.9958 - val_loss: 4.1977 - val_acc: 0.5600\n",
      "Epoch 88/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0271 - acc: 0.9916 - val_loss: 3.9565 - val_acc: 0.5400\n",
      "Epoch 89/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 3.7719 - val_acc: 0.5000\n",
      "Epoch 90/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.0277 - acc: 0.9916 - val_loss: 4.2475 - val_acc: 0.5000\n",
      "Epoch 91/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.0170 - acc: 0.9958 - val_loss: 4.2830 - val_acc: 0.5000\n",
      "Epoch 92/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.0136 - acc: 0.9958 - val_loss: 3.4135 - val_acc: 0.5200\n",
      "Epoch 93/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.0241 - acc: 0.9916 - val_loss: 3.4403 - val_acc: 0.5000\n",
      "Epoch 94/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.0025 - acc: 1.0000 - val_loss: 4.0054 - val_acc: 0.4800\n",
      "Epoch 95/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.0042 - acc: 1.0000 - val_loss: 4.5819 - val_acc: 0.5000\n",
      "Epoch 96/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.0251 - acc: 0.9958 - val_loss: 4.4830 - val_acc: 0.5400\n",
      "Epoch 97/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0302 - acc: 0.9874 - val_loss: 3.8674 - val_acc: 0.5400\n",
      "Epoch 98/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.0225 - acc: 0.9958 - val_loss: 4.3343 - val_acc: 0.5000\n",
      "Epoch 99/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0028 - acc: 1.0000 - val_loss: 4.6513 - val_acc: 0.4600\n",
      "Epoch 100/100\n",
      "238/238 [==============================] - 8s 34ms/step - loss: 0.0323 - acc: 0.9916 - val_loss: 4.1153 - val_acc: 0.5000\n",
      "50/50 [==============================] - 1s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "epochs = 100  \n",
    "batch_size = 64\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 1st cnn layer: 64 filters of size 11*11 with stride 4\n",
    "#                POOL: 3*3 filters with stride 2\n",
    "model.add(Conv1D(filters=64, kernel_size=11, strides=4, padding='valid',activation='relu',kernel_initializer='uniform',input_shape=X_train.shape[1:]))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=192, kernel_size=5, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=384, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Flatten())  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(num_classes,activation='softmax'))  \n",
    "\n",
    "\n",
    "opt = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.0005, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])  \n",
    "model.summary()  \n",
    "\n",
    "time_callback = TimeHistory()\n",
    "acc_callback = AccuracyHistory()\n",
    "model.fit(X_train, y_train, batch_size=batch_size, validation_data=(X_test,y_test),callbacks=[time_callback,acc_callback],epochs=epochs)\n",
    "times = time_callback.times\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subject 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(288, 25, 1000)\n",
      "(288,)\n",
      "(288, 22, 1000)\n",
      "(288, 1000, 22)\n",
      "(288, 1)\n",
      "X_train: (238, 1000, 22) \n",
      "y_train: (238, 1) \n",
      "X_test: (50, 1000, 22) \n",
      "y_test: (50, 1) \n"
     ]
    }
   ],
   "source": [
    "A01T = h5py.File('A08T_slice.mat', 'r') \n",
    "X = np.copy(A01T['image']) \n",
    "y = np.copy(A01T['type']) \n",
    "y = y[0,0:X.shape[0]:1] \n",
    "y = np.asarray(y, dtype=np.int32)\n",
    "\n",
    "# 288 trials per session\n",
    "# 25 channels (the Ô¨Årst 22 are EEG and the last 3 are EOG signals)\n",
    "# The data is sampled at 250Hz, and so 1000 corresponds to 4s of data for each trial.\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# get rid of the EOG signal\n",
    "X = X[:,:22,:]\n",
    "print(X.shape)\n",
    "\n",
    "# get rif of NaN\n",
    "y = np.asarray(y, dtype=np.float32)-769\n",
    "y = y[:,None]\n",
    "X = X.transpose(0,2,1)\n",
    "where_are_NaNs = np.isnan(X)\n",
    "X[where_are_NaNs] = 0\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# get train, validation, test data\n",
    "data = get_train_test(X,y)\n",
    "for k in data.keys():\n",
    "    print('{}: {} '.format(k, data[k].shape))\n",
    "# X[n_trials, n_steps, n_inputs]\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']\n",
    "\n",
    "# There are 4 classes  \n",
    "num_classes = 4  \n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "#y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\chenx\\desktop\\ucla\\2018wi~4\\hw2\\env~1\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_1 (Conv1D)            (None, 248, 64)           15552     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 123, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 123, 192)          61632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 61, 192)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 61, 384)           221568    \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 61, 256)           295168    \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 61, 256)           196864    \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 61, 256)           196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 7680)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 4096)              31461376  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 16388     \n",
      "=================================================================\n",
      "Total params: 66,028,036\n",
      "Trainable params: 66,028,036\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n",
      "Train on 238 samples, validate on 50 samples\n",
      "Epoch 1/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.5565 - acc: 0.2227 - val_loss: 1.3870 - val_acc: 0.2800\n",
      "Epoch 2/100\n",
      "238/238 [==============================] - 7s 27ms/step - loss: 1.4419 - acc: 0.2647 - val_loss: 1.3905 - val_acc: 0.2600\n",
      "Epoch 3/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.4543 - acc: 0.2269 - val_loss: 1.3899 - val_acc: 0.2400\n",
      "Epoch 4/100\n",
      "238/238 [==============================] - 8s 34ms/step - loss: 1.4615 - acc: 0.1975 - val_loss: 1.3892 - val_acc: 0.1600\n",
      "Epoch 5/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.4067 - acc: 0.2395 - val_loss: 1.3910 - val_acc: 0.2400\n",
      "Epoch 6/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.3981 - acc: 0.2185 - val_loss: 1.3825 - val_acc: 0.2600\n",
      "Epoch 7/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.3985 - acc: 0.2563 - val_loss: 1.3840 - val_acc: 0.2600\n",
      "Epoch 8/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 1.3929 - acc: 0.2311 - val_loss: 1.3828 - val_acc: 0.3200\n",
      "Epoch 9/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.3810 - acc: 0.2857 - val_loss: 1.3815 - val_acc: 0.2800\n",
      "Epoch 10/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.3909 - acc: 0.2563 - val_loss: 1.3820 - val_acc: 0.2400\n",
      "Epoch 11/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.3850 - acc: 0.2773 - val_loss: 1.3773 - val_acc: 0.3200\n",
      "Epoch 12/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.3736 - acc: 0.3025 - val_loss: 1.3770 - val_acc: 0.2200\n",
      "Epoch 13/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.3771 - acc: 0.2815 - val_loss: 1.3728 - val_acc: 0.3800\n",
      "Epoch 14/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.3795 - acc: 0.3151 - val_loss: 1.3690 - val_acc: 0.3600\n",
      "Epoch 15/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.3752 - acc: 0.3235 - val_loss: 1.3686 - val_acc: 0.3400\n",
      "Epoch 16/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.3655 - acc: 0.3025 - val_loss: 1.3536 - val_acc: 0.3000\n",
      "Epoch 17/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.3616 - acc: 0.3277 - val_loss: 1.3448 - val_acc: 0.3400\n",
      "Epoch 18/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.3269 - acc: 0.3655 - val_loss: 1.3176 - val_acc: 0.3600\n",
      "Epoch 19/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.2702 - acc: 0.4202 - val_loss: 1.2865 - val_acc: 0.4400\n",
      "Epoch 20/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.1946 - acc: 0.4790 - val_loss: 1.1726 - val_acc: 0.4600\n",
      "Epoch 21/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.3084 - acc: 0.4454 - val_loss: 1.7987 - val_acc: 0.2400\n",
      "Epoch 22/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.4367 - acc: 0.3403 - val_loss: 1.3561 - val_acc: 0.4000\n",
      "Epoch 23/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.4117 - acc: 0.2437 - val_loss: 1.3464 - val_acc: 0.3800\n",
      "Epoch 24/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.3896 - acc: 0.3193 - val_loss: 1.3712 - val_acc: 0.2200\n",
      "Epoch 25/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.4414 - acc: 0.3025 - val_loss: 1.4338 - val_acc: 0.3000\n",
      "Epoch 26/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.3996 - acc: 0.2773 - val_loss: 1.3601 - val_acc: 0.3000\n",
      "Epoch 27/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.3717 - acc: 0.3025 - val_loss: 1.3526 - val_acc: 0.3600\n",
      "Epoch 28/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.3105 - acc: 0.3992 - val_loss: 1.3549 - val_acc: 0.4000\n",
      "Epoch 29/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 1.2510 - acc: 0.3992 - val_loss: 1.3490 - val_acc: 0.3200\n",
      "Epoch 30/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.2271 - acc: 0.4748 - val_loss: 1.4950 - val_acc: 0.3800\n",
      "Epoch 31/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.1921 - acc: 0.4832 - val_loss: 1.4070 - val_acc: 0.3600\n",
      "Epoch 32/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 1.1780 - acc: 0.5042 - val_loss: 1.3737 - val_acc: 0.4200\n",
      "Epoch 33/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.2110 - acc: 0.4454 - val_loss: 1.3088 - val_acc: 0.4200\n",
      "Epoch 34/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.0571 - acc: 0.5588 - val_loss: 1.5740 - val_acc: 0.4600\n",
      "Epoch 35/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.9093 - acc: 0.6008 - val_loss: 1.2330 - val_acc: 0.4400\n",
      "Epoch 36/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.8585 - acc: 0.6261 - val_loss: 1.5147 - val_acc: 0.4600\n",
      "Epoch 37/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.2453 - acc: 0.5336 - val_loss: 1.3497 - val_acc: 0.2800\n",
      "Epoch 38/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238/238 [==============================] - 7s 28ms/step - loss: 1.3110 - acc: 0.3487 - val_loss: 1.3193 - val_acc: 0.4400\n",
      "Epoch 39/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.1557 - acc: 0.5420 - val_loss: 1.3536 - val_acc: 0.3800\n",
      "Epoch 40/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.0087 - acc: 0.5924 - val_loss: 1.6367 - val_acc: 0.4200\n",
      "Epoch 41/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.9136 - acc: 0.5924 - val_loss: 1.4054 - val_acc: 0.3600\n",
      "Epoch 42/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.7876 - acc: 0.6597 - val_loss: 1.6344 - val_acc: 0.4600\n",
      "Epoch 43/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.7032 - acc: 0.7143 - val_loss: 1.8003 - val_acc: 0.4000\n",
      "Epoch 44/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.6332 - acc: 0.7185 - val_loss: 2.0902 - val_acc: 0.4200\n",
      "Epoch 45/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.2257 - acc: 0.5378 - val_loss: 1.4549 - val_acc: 0.3600\n",
      "Epoch 46/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.0164 - acc: 0.5546 - val_loss: 1.3592 - val_acc: 0.3200\n",
      "Epoch 47/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.8630 - acc: 0.6261 - val_loss: 2.3155 - val_acc: 0.3400\n",
      "Epoch 48/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.8367 - acc: 0.6387 - val_loss: 1.3622 - val_acc: 0.3000\n",
      "Epoch 49/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.7845 - acc: 0.6597 - val_loss: 1.8957 - val_acc: 0.3800\n",
      "Epoch 50/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.6603 - acc: 0.7311 - val_loss: 1.9331 - val_acc: 0.3600\n",
      "Epoch 51/100\n",
      "238/238 [==============================] - 7s 27ms/step - loss: 0.5889 - acc: 0.7521 - val_loss: 2.1583 - val_acc: 0.4600\n",
      "Epoch 52/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.6058 - acc: 0.8025 - val_loss: 1.3265 - val_acc: 0.2800\n",
      "Epoch 53/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.9449 - acc: 0.6429 - val_loss: 1.6986 - val_acc: 0.2600\n",
      "Epoch 54/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 1.0244 - acc: 0.6723 - val_loss: 1.4637 - val_acc: 0.3200\n",
      "Epoch 55/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.8513 - acc: 0.7143 - val_loss: 1.6007 - val_acc: 0.3200\n",
      "Epoch 56/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.5356 - acc: 0.7983 - val_loss: 2.0663 - val_acc: 0.4800\n",
      "Epoch 57/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.3937 - acc: 0.8403 - val_loss: 3.6946 - val_acc: 0.4000\n",
      "Epoch 58/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.4845 - acc: 0.8529 - val_loss: 1.9920 - val_acc: 0.3600\n",
      "Epoch 59/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.4914 - acc: 0.7983 - val_loss: 4.1172 - val_acc: 0.3800\n",
      "Epoch 60/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.3899 - acc: 0.8361 - val_loss: 2.4361 - val_acc: 0.3200\n",
      "Epoch 61/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.3224 - acc: 0.8739 - val_loss: 4.4953 - val_acc: 0.3200\n",
      "Epoch 62/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.2085 - acc: 0.9454 - val_loss: 4.3400 - val_acc: 0.3400\n",
      "Epoch 63/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.1950 - acc: 0.9370 - val_loss: 5.2477 - val_acc: 0.4200\n",
      "Epoch 64/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.2950 - acc: 0.8866 - val_loss: 5.8055 - val_acc: 0.2600\n",
      "Epoch 65/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.8917 - acc: 0.6849 - val_loss: 2.1182 - val_acc: 0.3600\n",
      "Epoch 66/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.9709 - acc: 0.5924 - val_loss: 2.4125 - val_acc: 0.3400\n",
      "Epoch 67/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.6633 - acc: 0.7353 - val_loss: 2.3684 - val_acc: 0.3200\n",
      "Epoch 68/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.3959 - acc: 0.8319 - val_loss: 3.8297 - val_acc: 0.2400\n",
      "Epoch 69/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.2081 - acc: 0.9202 - val_loss: 5.6506 - val_acc: 0.2600\n",
      "Epoch 70/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.1826 - acc: 0.9286 - val_loss: 7.2168 - val_acc: 0.2800\n",
      "Epoch 71/100\n",
      "238/238 [==============================] - 8s 35ms/step - loss: 0.3280 - acc: 0.8739 - val_loss: 8.0269 - val_acc: 0.3000\n",
      "Epoch 72/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.0566 - acc: 0.7227 - val_loss: 1.3678 - val_acc: 0.2600\n",
      "Epoch 73/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 1.1767 - acc: 0.4706 - val_loss: 2.4711 - val_acc: 0.4200\n",
      "Epoch 74/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.9233 - acc: 0.6261 - val_loss: 3.7191 - val_acc: 0.3000\n",
      "Epoch 75/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 1.0464 - acc: 0.6807 - val_loss: 6.3509 - val_acc: 0.2800\n",
      "Epoch 76/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.3985 - acc: 0.5672 - val_loss: 3.4157 - val_acc: 0.3000\n",
      "Epoch 77/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.9889 - acc: 0.5924 - val_loss: 1.4843 - val_acc: 0.2800\n",
      "Epoch 78/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.5288 - acc: 0.3025 - val_loss: 1.3563 - val_acc: 0.3200\n",
      "Epoch 79/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 3.4559 - acc: 0.2689 - val_loss: 11.3456 - val_acc: 0.2400\n",
      "Epoch 80/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 11.5027 - acc: 0.2731 - val_loss: 11.9274 - val_acc: 0.2600\n",
      "Epoch 81/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 12.1224 - acc: 0.2479 - val_loss: 11.9274 - val_acc: 0.2600\n",
      "Epoch 82/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 12.1224 - acc: 0.2479 - val_loss: 11.9274 - val_acc: 0.2600\n",
      "Epoch 83/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 12.1224 - acc: 0.2479 - val_loss: 11.9274 - val_acc: 0.2600\n",
      "Epoch 84/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 12.1224 - acc: 0.2479 - val_loss: 11.9274 - val_acc: 0.2600\n",
      "Epoch 85/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 12.1224 - acc: 0.2479 - val_loss: 11.9274 - val_acc: 0.2600\n",
      "Epoch 86/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 12.1224 - acc: 0.2479 - val_loss: 11.9274 - val_acc: 0.2600\n",
      "Epoch 87/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 12.1224 - acc: 0.2479 - val_loss: 11.9274 - val_acc: 0.2600\n",
      "Epoch 88/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 12.1224 - acc: 0.2479 - val_loss: 11.9274 - val_acc: 0.2600\n",
      "Epoch 89/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 12.1224 - acc: 0.2479 - val_loss: 11.9274 - val_acc: 0.2600\n",
      "Epoch 90/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 12.1224 - acc: 0.2479 - val_loss: 11.9274 - val_acc: 0.2600\n",
      "Epoch 91/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 12.1224 - acc: 0.2479 - val_loss: 11.9274 - val_acc: 0.2600\n",
      "Epoch 92/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 12.1224 - acc: 0.2479 - val_loss: 11.9274 - val_acc: 0.2600\n",
      "Epoch 93/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 12.1224 - acc: 0.2479 - val_loss: 11.9274 - val_acc: 0.2600\n",
      "Epoch 94/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 12.1224 - acc: 0.2479 - val_loss: 11.9274 - val_acc: 0.2600\n",
      "Epoch 95/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 12.1224 - acc: 0.2479 - val_loss: 11.9274 - val_acc: 0.2600\n",
      "Epoch 96/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 12.1224 - acc: 0.2479 - val_loss: 11.9274 - val_acc: 0.2600\n",
      "Epoch 97/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 12.1224 - acc: 0.2479 - val_loss: 11.9274 - val_acc: 0.2600\n",
      "Epoch 98/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238/238 [==============================] - 7s 29ms/step - loss: 12.1224 - acc: 0.2479 - val_loss: 11.9274 - val_acc: 0.2600\n",
      "Epoch 99/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 12.1224 - acc: 0.2479 - val_loss: 11.9274 - val_acc: 0.2600\n",
      "Epoch 100/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 12.1224 - acc: 0.2479 - val_loss: 11.9274 - val_acc: 0.2600\n",
      "50/50 [==============================] - 0s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "epochs = 100  \n",
    "batch_size = 64\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 1st cnn layer: 64 filters of size 11*11 with stride 4\n",
    "#                POOL: 3*3 filters with stride 2\n",
    "model.add(Conv1D(filters=64, kernel_size=11, strides=4, padding='valid',activation='relu',kernel_initializer='uniform',input_shape=X_train.shape[1:]))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=192, kernel_size=5, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=384, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Flatten())  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(num_classes,activation='softmax'))  \n",
    "\n",
    "\n",
    "opt = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.0005, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])  \n",
    "model.summary()  \n",
    "\n",
    "time_callback = TimeHistory()\n",
    "acc_callback = AccuracyHistory()\n",
    "model.fit(X_train, y_train, batch_size=batch_size, validation_data=(X_test,y_test),callbacks=[time_callback,acc_callback],epochs=epochs)\n",
    "times = time_callback.times\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subject 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(288, 25, 1000)\n",
      "(288,)\n",
      "(288, 22, 1000)\n",
      "(288, 1000, 22)\n",
      "(288, 1)\n",
      "X_train: (238, 1000, 22) \n",
      "y_train: (238, 1) \n",
      "X_test: (50, 1000, 22) \n",
      "y_test: (50, 1) \n"
     ]
    }
   ],
   "source": [
    "A01T = h5py.File('A09T_slice.mat', 'r') \n",
    "X = np.copy(A01T['image']) \n",
    "y = np.copy(A01T['type']) \n",
    "y = y[0,0:X.shape[0]:1] \n",
    "y = np.asarray(y, dtype=np.int32)\n",
    "\n",
    "# 288 trials per session\n",
    "# 25 channels (the Ô¨Årst 22 are EEG and the last 3 are EOG signals)\n",
    "# The data is sampled at 250Hz, and so 1000 corresponds to 4s of data for each trial.\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# get rid of the EOG signal\n",
    "X = X[:,:22,:]\n",
    "print(X.shape)\n",
    "\n",
    "# get rif of NaN\n",
    "y = np.asarray(y, dtype=np.float32)-769\n",
    "y = y[:,None]\n",
    "X = X.transpose(0,2,1)\n",
    "where_are_NaNs = np.isnan(X)\n",
    "X[where_are_NaNs] = 0\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# get train, validation, test data\n",
    "data = get_train_test(X,y)\n",
    "for k in data.keys():\n",
    "    print('{}: {} '.format(k, data[k].shape))\n",
    "# X[n_trials, n_steps, n_inputs]\n",
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']\n",
    "\n",
    "# There are 4 classes  \n",
    "num_classes = 4  \n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "#y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_13 (Conv1D)           (None, 248, 64)           15552     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 123, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 123, 192)          61632     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 61, 192)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 61, 384)           221568    \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 61, 256)           295168    \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 61, 256)           196864    \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 61, 256)           196864    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 7680)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 4096)              31461376  \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 4)                 16388     \n",
      "=================================================================\n",
      "Total params: 66,028,036\n",
      "Trainable params: 66,028,036\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 238 samples, validate on 50 samples\n",
      "Epoch 1/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 1.5798 - acc: 0.2269 - val_loss: 1.4484 - val_acc: 0.2400\n",
      "Epoch 2/100\n",
      "238/238 [==============================] - 6s 24ms/step - loss: 1.4698 - acc: 0.2647 - val_loss: 1.3744 - val_acc: 0.2600\n",
      "Epoch 3/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 1.4364 - acc: 0.2227 - val_loss: 1.3701 - val_acc: 0.3000\n",
      "Epoch 4/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.3954 - acc: 0.2857 - val_loss: 1.3726 - val_acc: 0.3200\n",
      "Epoch 5/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.3892 - acc: 0.2689 - val_loss: 1.3602 - val_acc: 0.2600\n",
      "Epoch 6/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.3663 - acc: 0.3571 - val_loss: 1.3571 - val_acc: 0.2600\n",
      "Epoch 7/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.3795 - acc: 0.3403 - val_loss: 1.3601 - val_acc: 0.3400\n",
      "Epoch 8/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.4130 - acc: 0.2689 - val_loss: 1.3536 - val_acc: 0.4000\n",
      "Epoch 9/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.3772 - acc: 0.2899 - val_loss: 1.3546 - val_acc: 0.4600\n",
      "Epoch 10/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.3621 - acc: 0.3697 - val_loss: 1.3276 - val_acc: 0.4400\n",
      "Epoch 11/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.3194 - acc: 0.3739 - val_loss: 1.2881 - val_acc: 0.4200\n",
      "Epoch 12/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.2878 - acc: 0.3950 - val_loss: 1.2498 - val_acc: 0.4400\n",
      "Epoch 13/100\n",
      "238/238 [==============================] - 7s 27ms/step - loss: 1.1940 - acc: 0.4580 - val_loss: 1.2500 - val_acc: 0.4600\n",
      "Epoch 14/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.1974 - acc: 0.4538 - val_loss: 1.2645 - val_acc: 0.3600\n",
      "Epoch 15/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.1839 - acc: 0.4412 - val_loss: 1.2126 - val_acc: 0.4000\n",
      "Epoch 16/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.0528 - acc: 0.5252 - val_loss: 1.2394 - val_acc: 0.4600\n",
      "Epoch 17/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.9813 - acc: 0.5504 - val_loss: 1.2242 - val_acc: 0.4600\n",
      "Epoch 18/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.9808 - acc: 0.6092 - val_loss: 1.2214 - val_acc: 0.4200\n",
      "Epoch 19/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.8863 - acc: 0.6008 - val_loss: 1.3576 - val_acc: 0.5400\n",
      "Epoch 20/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 1.1562 - acc: 0.4580 - val_loss: 1.1149 - val_acc: 0.5600\n",
      "Epoch 21/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.8961 - acc: 0.6303 - val_loss: 1.0941 - val_acc: 0.4800\n",
      "Epoch 22/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.9422 - acc: 0.5882 - val_loss: 1.0577 - val_acc: 0.5400\n",
      "Epoch 23/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.7354 - acc: 0.6681 - val_loss: 1.2095 - val_acc: 0.6200\n",
      "Epoch 24/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.7844 - acc: 0.7059 - val_loss: 1.3391 - val_acc: 0.3800\n",
      "Epoch 25/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 1.1564 - acc: 0.4706 - val_loss: 1.2567 - val_acc: 0.4000\n",
      "Epoch 26/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.8918 - acc: 0.5840 - val_loss: 1.3402 - val_acc: 0.4600\n",
      "Epoch 27/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.8019 - acc: 0.6471 - val_loss: 1.5994 - val_acc: 0.4200\n",
      "Epoch 28/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.7944 - acc: 0.6555 - val_loss: 1.8643 - val_acc: 0.4000\n",
      "Epoch 29/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.8892 - acc: 0.6176 - val_loss: 1.4937 - val_acc: 0.4400\n",
      "Epoch 30/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.7947 - acc: 0.6891 - val_loss: 1.1536 - val_acc: 0.4800\n",
      "Epoch 31/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.6392 - acc: 0.7689 - val_loss: 1.1950 - val_acc: 0.5400\n",
      "Epoch 32/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.5545 - acc: 0.7899 - val_loss: 1.4970 - val_acc: 0.5000\n",
      "Epoch 33/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.4263 - acc: 0.8319 - val_loss: 1.5086 - val_acc: 0.5800\n",
      "Epoch 34/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.3837 - acc: 0.8529 - val_loss: 1.4625 - val_acc: 0.4800\n",
      "Epoch 35/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.3747 - acc: 0.8824 - val_loss: 1.9250 - val_acc: 0.5200\n",
      "Epoch 36/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.3654 - acc: 0.8571 - val_loss: 3.1306 - val_acc: 0.4200\n",
      "Epoch 37/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.4449 - acc: 0.8403 - val_loss: 2.1236 - val_acc: 0.5000\n",
      "Epoch 38/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.4206 - acc: 0.8361 - val_loss: 2.0207 - val_acc: 0.4600\n",
      "Epoch 39/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.3385 - acc: 0.8571 - val_loss: 2.0600 - val_acc: 0.5000\n",
      "Epoch 40/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.2390 - acc: 0.8950 - val_loss: 2.4801 - val_acc: 0.4000\n",
      "Epoch 41/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.4091 - acc: 0.8445 - val_loss: 3.1194 - val_acc: 0.4600\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238/238 [==============================] - 8s 34ms/step - loss: 0.4294 - acc: 0.8697 - val_loss: 2.6202 - val_acc: 0.5000\n",
      "Epoch 43/100\n",
      "238/238 [==============================] - 8s 34ms/step - loss: 0.4519 - acc: 0.8109 - val_loss: 2.4164 - val_acc: 0.4800\n",
      "Epoch 44/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.2172 - acc: 0.9160 - val_loss: 3.4641 - val_acc: 0.4600\n",
      "Epoch 45/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.1902 - acc: 0.9286 - val_loss: 2.5618 - val_acc: 0.5600\n",
      "Epoch 46/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.1952 - acc: 0.9370 - val_loss: 2.8894 - val_acc: 0.4600\n",
      "Epoch 47/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.1710 - acc: 0.9538 - val_loss: 3.0153 - val_acc: 0.4800\n",
      "Epoch 48/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.1964 - acc: 0.9328 - val_loss: 2.3042 - val_acc: 0.5600\n",
      "Epoch 49/100\n",
      "238/238 [==============================] - 6s 26ms/step - loss: 0.1758 - acc: 0.9370 - val_loss: 3.6500 - val_acc: 0.5000\n",
      "Epoch 50/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.4202 - acc: 0.8613 - val_loss: 2.5980 - val_acc: 0.5000\n",
      "Epoch 51/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.1839 - acc: 0.9286 - val_loss: 2.8550 - val_acc: 0.4400\n",
      "Epoch 52/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.2325 - acc: 0.9118 - val_loss: 1.8261 - val_acc: 0.5400\n",
      "Epoch 53/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.2063 - acc: 0.9244 - val_loss: 1.5988 - val_acc: 0.5400\n",
      "Epoch 54/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.0763 - acc: 0.9748 - val_loss: 3.0749 - val_acc: 0.6200\n",
      "Epoch 55/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.2760 - acc: 0.9286 - val_loss: 3.1047 - val_acc: 0.5200\n",
      "Epoch 56/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.3121 - acc: 0.8992 - val_loss: 3.2514 - val_acc: 0.4600\n",
      "Epoch 57/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.3807 - acc: 0.8824 - val_loss: 2.0735 - val_acc: 0.4800\n",
      "Epoch 58/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.2157 - acc: 0.9160 - val_loss: 2.4033 - val_acc: 0.5200\n",
      "Epoch 59/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.3101 - acc: 0.9076 - val_loss: 1.9081 - val_acc: 0.5800\n",
      "Epoch 60/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.1873 - acc: 0.9412 - val_loss: 2.1258 - val_acc: 0.5600\n",
      "Epoch 61/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.2169 - acc: 0.9244 - val_loss: 2.3515 - val_acc: 0.4800\n",
      "Epoch 62/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.1689 - acc: 0.9580 - val_loss: 2.8505 - val_acc: 0.4600\n",
      "Epoch 63/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.0990 - acc: 0.9496 - val_loss: 3.0334 - val_acc: 0.5400\n",
      "Epoch 64/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.0806 - acc: 0.9832 - val_loss: 2.8500 - val_acc: 0.5800\n",
      "Epoch 65/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.0491 - acc: 0.9790 - val_loss: 3.5935 - val_acc: 0.4600\n",
      "Epoch 66/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.1281 - acc: 0.9538 - val_loss: 3.8605 - val_acc: 0.5200\n",
      "Epoch 67/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.2296 - acc: 0.9538 - val_loss: 2.5681 - val_acc: 0.4800\n",
      "Epoch 68/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.1996 - acc: 0.9370 - val_loss: 3.2582 - val_acc: 0.5000\n",
      "Epoch 69/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.1781 - acc: 0.9244 - val_loss: 2.3460 - val_acc: 0.4800\n",
      "Epoch 70/100\n",
      "238/238 [==============================] - 8s 32ms/step - loss: 0.1815 - acc: 0.9328 - val_loss: 2.1910 - val_acc: 0.4200\n",
      "Epoch 71/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.0916 - acc: 0.9664 - val_loss: 2.6504 - val_acc: 0.4200\n",
      "Epoch 72/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.0664 - acc: 0.9748 - val_loss: 2.2822 - val_acc: 0.5600\n",
      "Epoch 73/100\n",
      "238/238 [==============================] - 7s 27ms/step - loss: 0.0636 - acc: 0.9790 - val_loss: 2.6276 - val_acc: 0.5800\n",
      "Epoch 74/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.0233 - acc: 0.9958 - val_loss: 2.9708 - val_acc: 0.6000\n",
      "Epoch 75/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.0302 - acc: 0.9832 - val_loss: 3.1939 - val_acc: 0.6200\n",
      "Epoch 76/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.0118 - acc: 0.9958 - val_loss: 3.7910 - val_acc: 0.5200\n",
      "Epoch 77/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0117 - acc: 0.9958 - val_loss: 4.2099 - val_acc: 0.5000\n",
      "Epoch 78/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.0060 - acc: 1.0000 - val_loss: 4.0288 - val_acc: 0.5000\n",
      "Epoch 79/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.0059 - acc: 0.9958 - val_loss: 3.7254 - val_acc: 0.4800\n",
      "Epoch 80/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.0130 - acc: 0.9958 - val_loss: 3.3774 - val_acc: 0.5600\n",
      "Epoch 81/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.0798 - acc: 0.9916 - val_loss: 3.7002 - val_acc: 0.5200\n",
      "Epoch 82/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.0066 - acc: 1.0000 - val_loss: 4.9004 - val_acc: 0.5400\n",
      "Epoch 83/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.0576 - acc: 0.9748 - val_loss: 4.3305 - val_acc: 0.4400\n",
      "Epoch 84/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.2927 - acc: 0.8992 - val_loss: 3.2487 - val_acc: 0.5200\n",
      "Epoch 85/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.1321 - acc: 0.9664 - val_loss: 3.3484 - val_acc: 0.5400\n",
      "Epoch 86/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.2763 - acc: 0.9286 - val_loss: 3.9242 - val_acc: 0.5200\n",
      "Epoch 87/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.0809 - acc: 0.9748 - val_loss: 2.1538 - val_acc: 0.5200\n",
      "Epoch 88/100\n",
      "238/238 [==============================] - 6s 27ms/step - loss: 0.0587 - acc: 0.9790 - val_loss: 2.7766 - val_acc: 0.6000\n",
      "Epoch 89/100\n",
      "238/238 [==============================] - 7s 28ms/step - loss: 0.0290 - acc: 0.9916 - val_loss: 3.4170 - val_acc: 0.4800\n",
      "Epoch 90/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 0.0391 - acc: 0.9874 - val_loss: 3.4828 - val_acc: 0.5400\n",
      "Epoch 91/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.0110 - acc: 0.9916 - val_loss: 3.6228 - val_acc: 0.5600\n",
      "Epoch 92/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0082 - acc: 0.9958 - val_loss: 3.7276 - val_acc: 0.5200\n",
      "Epoch 93/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.0039 - acc: 1.0000 - val_loss: 3.5086 - val_acc: 0.5600\n",
      "Epoch 94/100\n",
      "238/238 [==============================] - 7s 29ms/step - loss: 8.9793e-04 - acc: 1.0000 - val_loss: 3.5205 - val_acc: 0.5600\n",
      "Epoch 95/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.0070 - acc: 0.9958 - val_loss: 3.6370 - val_acc: 0.5800\n",
      "Epoch 96/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0020 - acc: 1.0000 - val_loss: 4.0580 - val_acc: 0.5400\n",
      "Epoch 97/100\n",
      "238/238 [==============================] - 7s 31ms/step - loss: 0.0073 - acc: 0.9958 - val_loss: 4.3828 - val_acc: 0.5200\n",
      "Epoch 98/100\n",
      "238/238 [==============================] - 7s 30ms/step - loss: 0.0012 - acc: 1.0000 - val_loss: 3.7933 - val_acc: 0.5600\n",
      "Epoch 99/100\n",
      "238/238 [==============================] - 8s 33ms/step - loss: 0.0451 - acc: 0.9916 - val_loss: 3.8058 - val_acc: 0.5600\n",
      "Epoch 100/100\n",
      "238/238 [==============================] - 9s 36ms/step - loss: 0.0046 - acc: 0.9958 - val_loss: 4.9293 - val_acc: 0.5200\n",
      "50/50 [==============================] - 1s 11ms/step\n"
     ]
    }
   ],
   "source": [
    "epochs = 100  \n",
    "batch_size = 64\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# 1st cnn layer: 64 filters of size 11*11 with stride 4\n",
    "#                POOL: 3*3 filters with stride 2\n",
    "model.add(Conv1D(filters=64, kernel_size=11, strides=4, padding='valid',activation='relu',kernel_initializer='uniform',input_shape=X_train.shape[1:]))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=192, kernel_size=5, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Conv1D(filters=384, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=3, strides=1, padding='same',activation='relu',kernel_initializer='uniform'))\n",
    "model.add(MaxPooling1D(pool_size=3,strides=2))\n",
    "\n",
    "model.add(Flatten())  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(4096,activation='relu'))  \n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(num_classes,activation='softmax'))  \n",
    "\n",
    "\n",
    "opt = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.0005, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])  \n",
    "model.summary()  \n",
    "\n",
    "time_callback = TimeHistory()\n",
    "acc_callback = AccuracyHistory()\n",
    "model.fit(X_train, y_train, batch_size=batch_size, validation_data=(X_test,y_test),callbacks=[time_callback,acc_callback],epochs=epochs)\n",
    "times = time_callback.times\n",
    "\n",
    "scores = model.evaluate(X_test, y_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
